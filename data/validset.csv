Id,Abstract,Task 1
D06501,"In this paper we present the Creative Invention Benchmark (CrIB), a 2000-problem benchmark for evaluating a particular facet of computational creativity.$$$Specifically, we address combinational p-creativity, the creativity at play when someone combines existing knowledge to achieve a solution novel to that individual.$$$We present generation strategies for the five problem categories of the benchmark and a set of initial baselines.",OBJECTIVES BACKGROUND METHODS
D02945,"Computer algorithms are written with the intent that when run they perform a useful function.$$$Typically any information obtained is unknown until the algorithm is run.$$$However, if the behavior of an algorithm can be fully described by precomputing just once how this algorithm will respond when executed on any input, this precomputed result provides a complete specification for all solutions in the problem domain.$$$We apply this idea to a previous anomaly detection algorithm, and in doing so transform it from one that merely detects individual anomalies when asked to discover potentially anomalous values, into an algorithm also capable of generating a complete specification for those values it would deem to be anomalous.$$$This specification is derived by examining no more than a small training data, can be obtained in very small constant time, and is inherently far more useful than results obtained by repeated execution of this tool.$$$For example, armed with such a specification one can ask how close an anomaly is to being deemed normal, and can validate this answer not by exhaustively testing the algorithm but by examining if the specification so generated is indeed correct.$$$This powerful idea can be applied to any algorithm whose runtime behavior can be recovered from its construction and so has wide applicability.",BACKGROUND BACKGROUND OBJECTIVES METHODS/RESULTS RESULTS RESULTS CONCLUSIONS
D02025,"The problem of finding the maximum number of vertex-disjoint uni-color paths in an edge-colored graph (called MaxCDP) has been recently introduced in literature, motivated by applications in social network analysis.$$$In this paper we investigate how the complexity of the problem depends on graph parameters (namely the number of vertices to remove to make the graph a collection of disjoint paths and the size of the vertex cover of the graph), which makes sense since graphs in social networks are not random and have structure.$$$The problem was known to be hard to approximate in polynomial time and not fixed-parameter tractable (FPT) for the natural parameter.$$$Here, we show that it is still hard to approximate, even in FPT-time.$$$Finally, we introduce a new variant of the problem, called MaxCDDP, whose goal is to find the maximum number of vertex-disjoint and color-disjoint uni-color paths.$$$We extend some of the results of MaxCDP to this new variant, and we prove that unlike MaxCDP, MaxCDDP is already hard on graphs at distance two from disjoint paths.",BACKGROUND OBJECTIVES BACKGROUND OBJECTIVES/METHODS OBJECTIVES/METHODS OBJECTIVES
D00264,"Real-world machine learning applications often have complex test metrics, and may have training and test data that follow different distributions.$$$We propose addressing these issues by using a weighted loss function with a standard convex loss, but with weights on the training examples that are learned to optimize the test metric of interest on the validation set.$$$These metric-optimized example weights can be learned for any test metric, including black box losses and customized metrics for specific applications.$$$We illustrate the performance of our proposal with public benchmark datasets and real-world applications with domain shift and custom loss functions that balance multiple objectives, impose fairness policies, and are non-convex and non-decomposable.",BACKGROUND METHODS METHODS RESULTS
D04351,"Typically an ontology matching technique is a combination of much different type of matchers operating at various abstraction levels such as structure, semantic, syntax, instance etc.$$$An ontology matching technique which employs matchers at all possible abstraction levels is expected to give, in general, best results in terms of precision, recall and F-measure due to improvement in matching opportunities and if we discount efficiency issues which may improve with better computing resources such as parallel processing.$$$A gold standard ontology matching model is derived from a model classification of ontology matching techniques.$$$A suitable metric is also defined based on gold standard ontology matching model.$$$A review of various ontology matching techniques specified in recent research papers in the area was undertaken to categorize an ontology matching technique as per newly proposed gold standard model and a metric value for the whole group was computed.$$$The results of the above study support proposed gold standard ontology matching model.",BACKGROUND BACKGROUND/OBJECTIVES/METHODS BACKGROUND/OBJECTIVES/METHODS/RESULTS BACKGROUND/OBJECTIVES/METHODS/RESULTS BACKGROUND/OBJECTIVES/METHODS/RESULTS BACKGROUND/OBJECTIVES/RESULTS
D03425,"This paper focuses on improved edge model based on Curvelet coefficients analysis.$$$Curvelet transform is a powerful tool for multiresolution representation of object with anisotropic edge.$$$Curvelet coefficients contributions have been analyzed using Scale Invariant Feature Transform (SIFT), commonly used to study local structure in images.$$$The permutation of Curvelet coefficients from original image and edges image obtained from gradient operator is used to improve original edges.$$$Experimental results show that this method brings out details on edges when the decomposition scale increases.",OBJECTIVES BACKGROUND METHODS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS
D06749,"The automatic parking is being massively developed by car manufacturers and providers.$$$Until now, there are two problems with the automatic parking.$$$First, there is no openly-available segmentation labels of parking slot on panoramic surround view (PSV) dataset.$$$Second, how to detect parking slot and road structure robustly.$$$Therefore, in this paper, we build up a public PSV dataset.$$$At the same time, we proposed a highly fused convolutional network (HFCN) based segmentation method for parking slot and lane markings based on the PSV dataset.$$$A surround-view image is made of four calibrated images captured from four fisheye cameras.$$$We collect and label more than 4,200 surround view images for this task, which contain various illuminated scenes of different types of parking slots.$$$A VH-HFCN network is proposed, which adopts an HFCN as the base, with an extra efficient VH-stage for better segmenting various markings.$$$The VH-stage consists of two independent linear convolution paths with vertical and horizontal convolution kernels respectively.$$$This modification enables the network to robustly and precisely extract linear features.$$$We evaluated our model on the PSV dataset and the results showed outstanding performance in ground markings segmentation.$$$Based on the segmented markings, parking slots and lanes are acquired by skeletonization, hough line transform and line arrangement.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES METHODS RESULTS METHODS METHODS RESULTS RESULTS RESULTS
D06216,"Understanding nonlinear dynamical systems (NLDSs) is challenging in a variety of engineering and scientific fields.$$$Dynamic mode decomposition (DMD), which is a numerical algorithm for the spectral analysis of Koopman operators, has been attracting attention as a way of obtaining global modal descriptions of NLDSs without requiring explicit prior knowledge.$$$However, since existing DMD algorithms are in principle formulated based on the concatenation of scalar observables, it is not directly applicable to data with dependent structures among observables, which take, for example, the form of a sequence of graphs.$$$In this paper, we formulate Koopman spectral analysis for NLDSs with structures among observables and propose an estimation algorithm for this problem.$$$This method can extract and visualize the underlying low-dimensional global dynamics of NLDSs with structures among observables from data, which can be useful in understanding the underlying dynamics of such NLDSs.$$$To this end, we first formulate the problem of estimating spectra of the Koopman operator defined in vector-valued reproducing kernel Hilbert spaces, and then develop an estimation procedure for this problem by reformulating tensor-based DMD.$$$As a special case of our method, we propose the method named as Graph DMD, which is a numerical algorithm for Koopman spectral analysis of graph dynamical systems, using a sequence of adjacency matrices.$$$We investigate the empirical performance of our method by using synthetic and real-world data.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS METHODS RESULTS
D06363,"A body of literature has demonstrated that users' mental health conditions, such as depression and anxiety, can be predicted from their social media language.$$$There is still a gap in the scientific understanding of how psychological stress is expressed on social media.$$$Stress is one of the primary underlying causes and correlates of chronic physical illnesses and mental health conditions.$$$In this paper, we explore the language of psychological stress with a dataset of 601 social media users, who answered the Perceived Stress Scale questionnaire and also consented to share their Facebook and Twitter data.$$$Firstly, we find that stressed users post about exhaustion, losing control, increased self-focus and physical pain as compared to posts about breakfast, family-time, and travel by users who are not stressed.$$$Secondly, we find that Facebook language is more predictive of stress than Twitter language.$$$Thirdly, we demonstrate how the language based models thus developed can be adapted and be scaled to measure county-level trends.$$$Since county-level language is easily available on Twitter using the Streaming API, we explore multiple domain adaptation algorithms to adapt user-level Facebook models to Twitter language.$$$We find that domain-adapted and scaled social media-based measurements of stress outperform sociodemographic variables (age, gender, race, education, and income), against ground-truth survey-based stress measurements, both at the user- and the county-level in the U.S. Twitter language that scores higher in stress is also predictive of poorer health, less access to facilities and lower socioeconomic status in counties.$$$We conclude with a discussion of the implications of using social media as a new tool for monitoring stress levels of both individuals and counties.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/METHODS RESULTS RESULTS RESULTS METHODS RESULTS CONCLUSIONS
D05590,"There are now several large scale deployments of differential privacy used to collect statistical information about users.$$$However, these deployments periodically recollect the data and recompute the statistics using algorithms designed for a single use.$$$As a result, these systems do not provide meaningful privacy guarantees over long time scales.$$$Moreover, existing techniques to mitigate this effect do not apply in the ""local model"" of differential privacy that these systems use.$$$In this paper, we introduce a new technique for local differential privacy that makes it possible to maintain up-to-date statistics over time, with privacy guarantees that degrade only in the number of changes in the underlying distribution rather than the number of collection periods.$$$We use our technique for tracking a changing statistic in the setting where users are partitioned into an unknown collection of groups, and at every time period each user draws a single bit from a common (but changing) group-specific distribution.$$$We also provide an application to frequency and heavy-hitter estimation.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES RESULTS RESULTS
D00133,"Integrating model-free and model-based approaches in reinforcement learning has the potential to achieve the high performance of model-free algorithms with low sample complexity.$$$However, this is difficult because an imperfect dynamics model can degrade the performance of the learning algorithm, and in sufficiently complex environments, the dynamics model will almost always be imperfect.$$$As a result, a key challenge is to combine model-based approaches with model-free learning in such a way that errors in the model do not degrade performance.$$$We propose stochastic ensemble value expansion (STEVE), a novel model-based technique that addresses this issue.$$$By dynamically interpolating between model rollouts of various horizon lengths for each individual example, STEVE ensures that the model is only utilized when doing so does not introduce significant errors.$$$Our approach outperforms model-free baselines on challenging continuous control benchmarks with an order-of-magnitude increase in sample efficiency, and in contrast to previous model-based approaches, performance does not degrade in complex environments.",BACKGROUND OBJECTIVES OBJECTIVES METHODS METHODS RESULTS
D06407,"A network covert channel is created that uses resource names such as addresses to convey information, and that approximates typical user behavior in order to blend in with its environment.$$$The channel correlates available resource names with a user defined code-space, and transmits its covert message by selectively accessing resources associated with the message codes.$$$In this paper we focus on an implementation of the channel using the Hypertext Transfer Protocol (HTTP) with Uniform Resource Locators (URLs) as the message names, though the system can be used in conjunction with a variety of protocols.$$$The covert channel does not modify expected protocol structure as might be detected by simple inspection, and our HTTP implementation emulates transaction level web user behavior in order to avoid detection by statistical or behavioral analysis.",OBJECTIVES METHODS METHODS METHODS
D04280,"Data diversity is critical to success when training deep learning models.$$$Medical imaging data sets are often imbalanced as pathologic findings are generally rare, which introduces significant challenges when training deep learning models.$$$In this work, we propose a method to generate synthetic abnormal MRI images with brain tumors by training a generative adversarial network using two publicly available data sets of brain MRI.$$$We demonstrate two unique benefits that the synthetic images provide.$$$First, we illustrate improved performance on tumor segmentation by leveraging the synthetic images as a form of data augmentation.$$$Second, we demonstrate the value of generative models as an anonymization tool, achieving comparable tumor segmentation results when trained on the synthetic data versus when trained on real subject data.$$$Together, these results offer a potential solution to two of the largest challenges facing machine learning in medical imaging, namely the small incidence of pathological findings, and the restrictions around sharing of patient data.",BACKGROUND BACKGROUND OBJECTIVES RESULTS RESULTS RESULTS RESULTS/CONCLUSIONS
D02399,"The problem of distributed dynamic state estimation in wireless sensor networks is studied.$$$Two important properties of local estimates, namely, the consistency and confidence, are emphasized.$$$On one hand, the consistency, which means that the approximated error covariance is lower bounded by the true unknown one, has to be guaranteed so that the estimate is not over-confident.$$$On the other hand, since the confidence indicates the accuracy of the estimate, the estimate should be as confident as possible.$$$We first analyze two different information fusion strategies used in the case of information sources with, respectively, uncorrelated errors and unknown but correlated errors.$$$Then a distributed hybrid information fusion algorithm is proposed, where each agent uses the information obtained not only by itself, but also from its neighbors through communication.$$$The proposed algorithm not only guarantees the consistency of the estimates, but also utilizes the available information sources in a more efficient manner and hence improves the confidence.$$$Besides, the proposed algorithm is fully distributed and guarantees convergence with the sufficient condition formulated.$$$The comparisons with existing algorithms are shown.",OBJECTIVES CONCLUSIONS BACKGROUND BACKGROUND METHODS METHODS RESULTS RESULTS METHODS
D05713,"Iterative decoding and linear programming decoding are guaranteed to converge to the maximum-likelihood codeword when the underlying Tanner graph is cycle-free.$$$Therefore, cycles are usually seen as the culprit of low-density parity-check (LDPC) codes.$$$In this paper, we argue in the context of graph cover pseudocodeword that, for a code that permits a cycle-free Tanner graph, cycles have no effect on error performance as long as they are a part of redundant rows.$$$Specifically, we characterize all parity-check matrices that are pseudocodeword-free for such class of codes.",BACKGROUND BACKGROUND RESULTS/CONCLUSIONS METHODS
D03255,"Attributing the culprit of a cyber-attack is widely considered one of the major technical and policy challenges of cyber-security.$$$The lack of ground truth for an individual responsible for a given attack has limited previous studies.$$$Here, we overcome this limitation by leveraging DEFCON capture-the-flag (CTF) exercise data where the actual ground-truth is known.$$$In this work, we use various classification techniques to identify the culprit in a cyberattack and find that deceptive activities account for the majority of misclassified samples.$$$We also explore several heuristics to alleviate some of the misclassification caused by deception.",BACKGROUND BACKGROUND METHODS OBJECTIVES METHODS
D06027,"High implementation complexity of multi-scroll circuit is a bottleneck problem in real chaos-based communication.$$$Especially, in multi-scroll Chua's circuit, the simplified implementation of piecewise-linear resistors with multiple segments is difficult due to their intricate irregular breakpoints and slopes.$$$To solve the challenge, this paper presents a systematic scheme for synthesizing a Chua's diode with multi-segment piecewise-linearity, which is achieved by cascading even-numbered passive nonlinear resistors with odd-numbered ones via a negative impedance converter (NIC).$$$As no extra DC bias voltage is employed and the scheme can be implemented by much simpler circuit.$$$The voltage-current characteristics of the obtained Chua's diode are analyzed theoretically and verified by numerical simulations.$$$Based on an available Chua's diode and a second-order active Sallen-Key high-pass filter, a new inductor-free Chua's circuit is then constructed to generate multi-scroll chaotic attractors.$$$The circuit simulations and hardware experiments both confirmed the feasibility of the designed system.",BACKGROUND BACKGROUND OBJECTIVES/METHODS RESULTS METHODS/RESULTS OBJECTIVES/METHODS/RESULTS CONCLUSIONS
D02265,"We introduce segmental recurrent neural networks (SRNNs) which define, given an input sequence, a joint probability distribution over segmentations of the input and labelings of the segments.$$$Representations of the input segments (i.e., contiguous subsequences of the input) are computed by encoding their constituent tokens using bidirectional recurrent neural nets, and these ""segment embeddings"" are used to define compatibility scores with output labels.$$$These local compatibility scores are integrated using a global semi-Markov conditional random field.$$$Both fully supervised training -- in which segment boundaries and labels are observed -- as well as partially supervised training -- in which segment boundaries are latent -- are straightforward.$$$Experiments on handwriting recognition and joint Chinese word segmentation/POS tagging show that, compared to models that do not explicitly represent segments such as BIO tagging schemes and connectionist temporal classification (CTC), SRNNs obtain substantially higher accuracies.",OBJECTIVES/CONCLUSIONS METHODS METHODS METHODS RESULTS
D01942,"In this paper, we derive a temporal arbitrage policy for storage via reinforcement learning.$$$Real-time price arbitrage is an important source of revenue for storage units, but designing good strategies have proven to be difficult because of the highly uncertain nature of the prices.$$$Instead of current model predictive or dynamic programming approaches, we use reinforcement learning to design an optimal arbitrage policy.$$$This policy is learned through repeated charge and discharge actions performed by the storage unit through updating a value matrix.$$$We design a reward function that does not only reflect the instant profit of charge/discharge decisions but also incorporate the history information.$$$Simulation results demonstrate that our designed reward function leads to significant performance improvement compared with existing algorithms.",OBJECTIVES/METHODS BACKGROUND/OBJECTIVES BACKGROUND/METHODS METHODS METHODS RESULTS/CONCLUSIONS
D00764,"A group of transition probability functions form a Shannon's channel whereas a group of truth functions form a semantic channel.$$$By the third kind of Bayes' theorem, we can directly convert a Shannon's channel into an optimized semantic channel.$$$When a sample is not big enough, we can use a truth function with parameters to produce the likelihood function, then train the truth function by the conditional sampling distribution.$$$The third kind of Bayes' theorem is proved.$$$A semantic information theory is simply introduced.$$$The semantic information measure reflects Popper's hypothesis-testing thought.$$$The Semantic Information Method (SIM) adheres to maximum semantic information criterion which is compatible with maximum likelihood criterion and Regularized Least Squares criterion.$$$It supports Wittgenstein's view: the meaning of a word lies in its use.$$$Letting the two channels mutually match, we obtain the Channels' Matching (CM) algorithm for machine learning.$$$The CM algorithm is used to explain the evolution of the semantic meaning of natural language, such as ""Old age"".$$$The semantic channel for medical tests and the confirmation measures of test-positive and test-negative are discussed.$$$The applications of the CM algorithm to semi-supervised learning and non-supervised learning are simply introduced.$$$As a predictive model, the semantic channel fits variable sources and hence can overcome class-imbalance problem.$$$The SIM strictly distinguishes statistical probability and logical probability and uses both at the same time.$$$This method is compatible with the thoughts of Bayes, Fisher, Shannon, Zadeh, Tarski, Davidson, Wittgenstein, and Popper.It is a competitive alternative to Bayesian inference.",METHODS METHODS METHODS METHODS BACKGROUND BACKGROUND BACKGROUND BACKGROUND METHODS METHODS METHODS RESULTS RESULTS METHODS BACKGROUND
D03177,"Bug localization in object oriented program ha s always been an important issue in softeware engineering.$$$In this paper, I propose a source level bug localization technique for object oriented embedded programs.$$$My proposed technique, presents the idea of debugging an object oriented program in class level, incorporating the object state information into the Class Dependence Graph (ClDG).$$$Given a program (having buggy statement) and an input that fails and others pass, my approach uses concrete as well as symbolic execution to synthesize the passing inputs that marginally from the failing input in their control flow behavior.$$$A comparison of the execution traces of the failing input and the passing input provides necessary clues to the root-cause of the failure.$$$A state trace difference, regarding the respective nodes of the ClDG is obtained, which leads to detect the bug in the program.",BACKGROUND OBJECTIVES METHODS RESULTS CONCLUSIONS OTHERS
D03046,"This paper proposes a new approach to a novel value network architecture for the game Go, called a multi-labelled (ML) value network.$$$In the ML value network, different values (win rates) are trained simultaneously for different settings of komi, a compensation given to balance the initiative of playing first.$$$The ML value network has three advantages, (a) it outputs values for different komi, (b) it supports dynamic komi, and (c) it lowers the mean squared error (MSE).$$$This paper also proposes a new dynamic komi method to improve game-playing strength.$$$This paper also performs experiments to demonstrate the merits of the architecture.$$$First, the MSE of the ML value network is generally lower than the value network alone.$$$Second, the program based on the ML value network wins by a rate of 67.6% against the program based on the value network alone.$$$Third, the program with the proposed dynamic komi method significantly improves the playing strength over the baseline that does not use dynamic komi, especially for handicap games.$$$To our knowledge, up to date, no handicap games have been played openly by programs using value networks.$$$This paper provides these programs with a useful approach to playing handicap games.",METHODS BACKGROUND CONCLUSIONS METHODS RESULTS RESULTS RESULTS RESULTS CONCLUSIONS RESULTS
D03065,"Phase retrieval refers to the problem of recovering real- or complex-valued vectors from magnitude measurements.$$$The best-known algorithms for this problem are iterative in nature and rely on so-called spectral initializers that provide accurate initialization vectors.$$$We propose a novel class of estimators suitable for general nonlinear measurement systems, called linear spectral estimators (LSPEs), which can be used to compute accurate initialization vectors for phase retrieval problems.$$$The proposed LSPEs not only provide accurate initialization vectors for noisy phase retrieval systems with structured or random measurement matrices, but also enable the derivation of sharp and nonasymptotic mean-squared error bounds.$$$We demonstrate the efficacy of LSPEs on synthetic and real-world phase retrieval problems, and show that our estimators significantly outperform existing methods for structured measurement systems that arise in practice.",OBJECTIVES BACKGROUND METHODS RESULTS RESULTS/CONCLUSIONS
D00102,"During the life span of large software projects, developers often apply the same code changes to different code locations in slight variations.$$$Since the application of these changes to all locations is time-consuming and error-prone, tools exist that learn change patterns from input examples, search for possible pattern applications, and generate corresponding recommendations.$$$In many cases, the generated recommendations are syntactically or semantically wrong due to code movements in the input examples.$$$Thus, they are of low accuracy and developers cannot directly copy them into their projects without adjustments.$$$We present the Accurate REcommendation System (ARES) that achieves a higher accuracy than other tools because its algorithms take care of code movements when creating patterns and recommendations.$$$On average, the recommendations by ARES have an accuracy of 96% with respect to code changes that developers have manually performed in commits of source code archives.$$$At the same time ARES achieves precision and recall values that are on par with other tools.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND/OBJECTIVES OBJECTIVES/METHODS/CONCLUSIONS RESULTS RESULTS
D06238,"Acquisition of labeled training samples for affective computing is usually costly and time-consuming, as affects are intrinsically subjective, subtle and uncertain, and hence multiple human assessors are needed to evaluate each affective sample.$$$Particularly, for affect estimation in the 3D space of valence, arousal and dominance, each assessor has to perform the evaluations in three dimensions, which makes the labeling problem even more challenging.$$$Many sophisticated machine learning approaches have been proposed to reduce the data labeling requirement in various other domains, but so far few have considered affective computing.$$$This paper proposes two multi-task active learning for regression approaches, which select the most beneficial samples to label, by considering the three affect primitives simultaneously.$$$Experimental results on the VAM corpus demonstrated that our optimal sample selection approaches can result in better estimation performance than random selection and several traditional single-task active learning approaches.$$$Thus, they can help alleviate the data labeling problem in affective computing, i.e., better estimation performance can be obtained from fewer labeling queries.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/METHODS RESULTS CONCLUSIONS
D01372,"Literature search is critical for any scientific research.$$$Different from Web or general domain search, a large portion of queries in scientific literature search are entity-set queries, that is, multiple entities of possibly different types.$$$Entity-set queries reflect user's need for finding documents that contain multiple entities and reveal inter-entity relationships and thus pose non-trivial challenges to existing search algorithms that model each entity separately.$$$However, entity-set queries are usually sparse (i.e., not so repetitive), which makes ineffective many supervised ranking models that rely heavily on associated click history.$$$To address these challenges, we introduce SetRank, an unsupervised ranking framework that models inter-entity relationships and captures entity type information.$$$Furthermore, we develop a novel unsupervised model selection algorithm, based on the technique of weighted rank aggregation, to automatically choose the parameter settings in SetRank without resorting to a labeled validation set.$$$We evaluate our proposed unsupervised approach using datasets from TREC Genomics Tracks and Semantic Scholar's query log.$$$The experiments demonstrate that SetRank significantly outperforms the baseline unsupervised models, especially on entity-set queries, and our model selection algorithm effectively chooses suitable parameter settings.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/METHODS METHODS RESULTS CONCLUSIONS
D00626,"A smart city provides its people with high standard of living through advanced technologies and transport is one of the major foci.$$$With the advent of autonomous vehicles (AVs), an AV-based public transportation system has been proposed recently, which is capable of providing new forms of transportation services with high efficiency, high flexibility, and low cost.$$$For the benefit of passengers, multitenancy can increase market competition leading to lower service charge and higher quality of service.$$$In this paper, we study the pricing issue of the multi-tenant AV public transportation system and three types of services are defined.$$$The pricing process for each service type is modeled as a combinatorial auction, in which the service providers, as bidders, compete for offering transportation services.$$$The winners of the auction are determined through an integer linear program.$$$To prevent the bidders from raising their bids for higher returns, we propose a strategy-proof Vickrey-Clarke-Groves-based charging mechanism, which can maximize the social welfare, to settle the final charges for the customers.$$$We perform extensive simulations to verify the analytical results and evaluate the performance of the charging mechanism.",BACKGROUND BACKGROUND BACKGROUND/OBJECTIVES OBJECTIVES METHODS METHODS METHODS RESULTS
D03917,"A typical problem in MOOCs is the missing opportunity for course conductors to individually support students in overcoming their problems and misconceptions.$$$This paper presents the results of automatically intervening on struggling students during programming exercises and offering peer feedback and tailored bonus exercises.$$$To improve learning success, we do not want to abolish instructionally desired trial and error but reduce extensive struggle and demotivation.$$$Therefore, we developed adaptive automatic just-in-time interventions to encourage students to ask for help if they require considerably more than average working time to solve an exercise.$$$Additionally, we offered students bonus exercises tailored for their individual weaknesses.$$$The approach was evaluated within a live course with over 5,000 active students via a survey and metrics gathered alongside.$$$Results show that we can increase the call outs for help by up to 66% and lower the dwelling time until issuing action.$$$Learnings from the experiments can further be used to pinpoint course material to be improved and tailor content to be audience specific.",BACKGROUND OBJECTIVES OBJECTIVES/METHODS METHODS METHODS METHODS RESULTS OTHERS
D01076,"Mafia (also called Werewolf) is a party game.$$$The participants are divided into two competing groups: citizens and a mafia.$$$The objective is to eliminate the opponent group.$$$The game consists of two consecutive phases (day and night) and a certain set of actions (e.g. lynching during day).$$$The mafia members have additional powers (knowing each other, killing during night) whereas the citizens are more numerous.$$$We propose a simple mathematical model of the game, which is essentially a pure death process with discrete time.$$$We find the closed-form solutions for the mafia winning-chance, w(n,m), as well as for the evolution of the game.$$$Moreover, we investigate the discrete properties of results, as well as their continuous-time approximations.$$$It turns out that a relatively small number of the mafia members, i.e. proportional to the square root of the total number of players, gives equal winning-chance for both groups.$$$Furthermore, the game strongly depends on the parity of the total number of players.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND METHODS OBJECTIVES/RESULTS OBJECTIVES/METHODS RESULTS RESULTS
D03257,"Effective data analysis ideally requires the analyst to have high expertise as well as high knowledge of the data.$$$Even with such familiarity, manually pursuing all potential hypotheses and exploring all possible views is impractical.$$$We present DataSite, a proactive visual analytics system where the burden of selecting and executing appropriate computations is shared by an automatic server-side computation engine.$$$Salient features identified by these automatic background processes are surfaced as notifications in a feed timeline.$$$DataSite effectively turns data analysis into a conversation between analyst and computer, thereby reducing the cognitive load and domain knowledge requirements.$$$We validate the system with a user study comparing it to a recent visualization recommendation system, yielding significant improvement, particularly for complex analyses that existing analytics systems do not support well.",BACKGROUND BACKGROUND OBJECTIVES METHODS OBJECTIVES METHODS/RESULTS
D02317,"Recently, increasing attention has been directed to the study of the speech emotion recognition, in which global acoustic features of an utterance are mostly used to eliminate the content differences.$$$However, the expression of speech emotion is a dynamic process, which is reflected through dynamic durations, energies, and some other prosodic information when one speaks.$$$In this paper, a novel local dynamic pitch probability distribution feature, which is obtained by drawing the histogram, is proposed to improve the accuracy of speech emotion recognition.$$$Compared with most of the previous works using global features, the proposed method takes advantage of the local dynamic information conveyed by the emotional speech.$$$Several experiments on Berlin Database of Emotional Speech are conducted to verify the effectiveness of the proposed method.$$$The experimental results demonstrate that the local dynamic information obtained with the proposed method is more effective for speech emotion recognition than the traditional global features.",BACKGROUND BACKGROUND/OBJECTIVES METHODS RESULTS METHODS CONCLUSIONS
D00562,"Binary classification rules based on covariates typically depend on simple loss functions such as zero-one misclassification.$$$Some cases may require more complex loss functions.$$$For example, individual-level monitoring of HIV-infected individuals on antiretroviral therapy (ART) requires periodic assessment of treatment failure, defined as having a viral load (VL) value above a certain threshold.$$$In some resource limited settings, VL tests may be limited by cost or technology, and diagnoses are based on other clinical markers.$$$Depending on scenario, higher premium may be placed on avoiding false-positives which brings greater cost and reduced treatment options.$$$Here, the optimal rule is determined by minimizing a weighted misclassification loss/risk.$$$We propose a method for finding and cross-validating optimal binary classification rules under weighted misclassification loss.$$$We focus on rules comprising a prediction score and an associated threshold, where the score is derived using an ensemble learner.$$$Simulations and examples show that our method, which derives the score and threshold jointly, more accurately estimates overall risk and has better operating characteristics compared with methods that derive the score first and the cutoff conditionally on the score especially for finite samples.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES METHODS RESULTS/CONCLUSIONS
D05445,"In this paper we present and start analyzing the iCub World data-set, an object recognition data-set, we acquired using a Human-Robot Interaction (HRI) scheme and the iCub humanoid robot platform.$$$Our set up allows for rapid acquisition and annotation of data with corresponding ground truth.$$$While more constrained in its scopes -- the iCub world is essentially a robotics research lab -- we demonstrate how the proposed data-set poses challenges to current recognition systems.$$$The iCubWorld data-set is publicly available.$$$The data-set can be downloaded from: http://www.iit.it/en/projects/data-sets.html.",OBJECTIVES RESULTS RESULTS CONCLUSIONS RESULTS
D05104,"Delay-coordinate reconstruction is a proven modeling strategy for building effective forecasts of nonlinear time series.$$$The first step in this process is the estimation of good values for two parameters, the time delay and the embedding dimension.$$$Many heuristics and strategies have been proposed in the literature for estimating these values.$$$Few, if any, of these methods were developed with forecasting in mind, however, and their results are not optimal for that purpose.$$$Even so, these heuristics---intended for other applications---are routinely used when building delay coordinate reconstruction-based forecast models.$$$In this paper, we propose a new strategy for choosing optimal parameter values for forecast methods that are based on delay-coordinate reconstructions.$$$The basic calculation involves maximizing the shared information between each delay vector and the future state of the system.$$$We illustrate the effectiveness of this method on several synthetic and experimental systems, showing that this metric can be calculated quickly and reliably from a relatively short time series, and that it provides a direct indication of how well a near-neighbor based forecasting method will work on a given delay reconstruction of that time series.$$$This allows a practitioner to choose reconstruction parameters that avoid any pathologies, regardless of the underlying mechanism, and maximize the predictive information contained in the reconstruction.",BACKGROUND METHODS BACKGROUND OBJECTIVES BACKGROUND OBJECTIVES METHODS RESULTS CONCLUSIONS
D03008,"Recent networking research has identified that data-driven congestion control (CC) can be more efficient than traditional CC in TCP.$$$Deep reinforcement learning (RL), in particular, has the potential to learn optimal network policies.$$$However, RL suffers from instability and over-fitting, deficiencies which so far render it unacceptable for use in datacenter networks.$$$In this paper, we analyze the requirements for RL to succeed in the datacenter context.$$$We present a new emulator, Iroko, which we developed to support different network topologies, congestion control algorithms, and deployment scenarios.$$$Iroko interfaces with the OpenAI gym toolkit, which allows for fast and fair evaluation of different RL and traditional CC algorithms under the same conditions.$$$We present initial benchmarks on three deep RL algorithms compared to TCP New Vegas and DCTCP.$$$Our results show that these algorithms are able to learn a CC policy which exceeds the performance of TCP New Vegas on a dumbbell and fat-tree topology.$$$We make our emulator open-source and publicly available: https://github.com/dcgym/iroko",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS RESULTS CONCLUSIONS
D01116,"We report simulation of nanostructured memristor device using piecewise linear and nonlinear window functions for RRAM and neuromorphic applications.$$$The linear drift model of memristor has been exploited for the simulation purpose with the linear and non-linear window function as the mathematical and scripting basis.$$$The results evidences that the piecewise linear window function can aptly simulate the memristor characteristics pertaining to RRAM application.$$$However, the nonlinear window function could exhibit the nonlinear phenomenon in simulation only at the lower magnitude of control parameter.$$$This has motivated us to propose a new nonlinear window function for emulating the simulation model of the memristor.$$$Interestingly, the proposed window function is scalable up to f(x)=1 and exhibits the nonlinear behavior at higher magnitude of control parameter.$$$Moreover, the simulation results of proposed nonlinear window function are encouraging and reveals the smooth nonlinear change from LRS to HRS and vice versa and therefore useful for the neuromorphic applications.",BACKGROUND METHODS RESULTS RESULTS RESULTS/CONCLUSIONS CONCLUSIONS CONCLUSIONS
D03909,"As robots become increasingly prevalent in human environments, there will inevitably be times when a robot needs to interrupt a human to initiate an interaction.$$$Our work introduces the first interruptibility-aware mobile robot system, and evaluates the effects of interruptibility-awareness on human task performance, robot task performance, and on human interpretation of the robot's social aptitude.$$$Our results show that our robot is effective at predicting interruptibility at high accuracy, allowing it to interrupt at more appropriate times.$$$Results of a large-scale user study show that while participants are able to maintain task performance even in the presence of interruptions, interruptibility-awareness improves the robot's task performance and improves participant social perception of the robot.",BACKGROUND/OBJECTIVES CONCLUSIONS CONCLUSIONS RESULTS
D02154,"Maximum-likelihood estimation (MLE) is widely used in sequence to sequence tasks for model training.$$$It uniformly treats the generation/prediction of each target token as multi-class classification, and yields non-smooth prediction probabilities: in a target sequence, some tokens are predicted with small probabilities while other tokens are with large probabilities.$$$According to our empirical study, we find that the non-smoothness of the probabilities results in low quality of generated sequences.$$$In this paper, we propose a sentence-wise regularization method which aims to output smooth prediction probabilities for all the tokens in the target sequence.$$$Our proposed method can automatically adjust the weights and gradients of each token in one sentence to ensure the predictions in a sequence uniformly well.$$$Experiments on three neural machine translation tasks and one text summarization task show that our method outperforms conventional MLE loss on all these tasks and achieves promising BLEU scores on WMT14 English-German and WMT17 Chinese-English translation task.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS
D06224,"Recurrent neural networks (RNNs) are the state of the art in sequence modeling for natural language.$$$However, it remains poorly understood what grammatical characteristics of natural language they implicitly learn and represent as a consequence of optimizing the language modeling objective.$$$Here we deploy the methods of controlled psycholinguistic experimentation to shed light on to what extent RNN behavior reflects incremental syntactic state and grammatical dependency representations known to characterize human linguistic behavior.$$$We broadly test two publicly available long short-term memory (LSTM) English sequence models, and learn and test a new Japanese LSTM.$$$We demonstrate that these models represent and maintain incremental syntactic state, but that they do not always generalize in the same way as humans.$$$Furthermore, none of our models learn the appropriate grammatical dependency configurations licensing reflexive pronouns or negative polarity items.",BACKGROUND OBJECTIVES METHODS METHODS CONCLUSIONS CONCLUSIONS
D05292,"Wireless Sensor Network (WSN) consists of large number of low-cost, resource-constrained sensor nodes.$$$The constraints of the wireless sensor node is their characteristics which include low memory, low computation power, they are deployed in hostile area and left unattended, small range of communication capability and low energy capabilities.$$$Base on those characteristics makes this network vulnerable to several attacks, such as sinkhole attack.$$$Sinkhole attack is a type of attack were compromised node tries to attract network traffic by advertise its fake routing update.$$$One of the impacts of sinkhole attack is that, it can be used to launch other attacks like selective forwarding attack, acknowledge spoofing attack and drops or altered routing information.$$$It can also used to send bogus information to base station.$$$This paper is focus on exploring and analyzing the existing solutions which used to detect and identify sinkhole attack in wireless sensor network.$$$The analysis is based on advantages and limitation of the proposed solutions.",BACKGROUND BACKGROUND BACKGROUND RESULTS RESULTS RESULTS OBJECTIVES METHODS
D03510,"This work presents a supervised learning based approach to the computer vision problem of frame interpolation.$$$The presented technique could also be used in the cartoon animations since drawing each individual frame consumes a noticeable amount of time.$$$The most existing solutions to this problem use unsupervised methods and focus only on real life videos with already high frame rate.$$$However, the experiments show that such methods do not work as well when the frame rate becomes low and object displacements between frames becomes large.$$$This is due to the fact that interpolation of the large displacement motion requires knowledge of the motion structure thus the simple techniques such as frame averaging start to fail.$$$In this work the deep convolutional neural network is used to solve the frame interpolation problem.$$$In addition, it is shown that incorporating the prior information such as optical flow improves the interpolation quality significantly.",BACKGROUND BACKGROUND BACKGROUND RESULTS RESULTS OBJECTIVES METHODS
D01288,"Despite of the progress achieved by deep learning in face recognition (FR), more and more people find that racial bias explicitly degrades the performance in realistic FR systems.$$$Facing the fact that existing training and testing databases consist of almost Caucasian subjects, there are still no independent testing databases to evaluate racial bias and even no training databases and methods to reduce it.$$$To facilitate the research towards conquering those unfair issues, this paper contributes a new dataset called Racial Faces in-the-Wild (RFW) database with two important uses, 1) racial bias testing: four testing subsets, namely Caucasian, Asian, Indian and African, are constructed, and each contains about 3000 individuals with 6000 image pairs for face verification, 2) racial bias reducing: one labeled training subset with Caucasians and three unlabeled training subsets with Asians, Indians and Africans are offered to encourage FR algorithms to transfer recognition knowledge from Caucasians to other races.$$$For we all know, RFW is the first database for measuring racial bias in FR algorithms.$$$After proving the existence of domain gap among different races and the existence of racial bias in FR algorithms, we further propose a deep information maximization adaptation network (IMAN) to bridge the domain gap, and comprehensive experiments show that the racial bias could be narrowed-down by our algorithm.",BACKGROUND BACKGROUND OBJECTIVES/METHODS CONCLUSIONS METHODS/RESULTS
D00246,"Rate control is widely adopted during video streaming to provide both high video qualities and low latency under various network conditions.$$$However, despite that many work have been proposed, they fail to tackle one major problem: previous methods determine a future transmission rate as a single for value which will be used in an entire time-slot, while real-world network conditions, unlike lab setup, often suffer from rapid and stochastic changes, resulting in the failures of predictions.$$$In this paper, we propose a delay-constrained rate control approach based on end-to-end deep learning.$$$The proposed model predicts future bit rate not as a single value, but as possible bit rate ranges using target delay gradient, with which the transmission delay is guaranteed.$$$We collect a large scale of real-world live streaming data to train our model, and as a result, it automatically learns the correlation between throughput and target delay gradient.$$$We build a testbed to evaluate our approach.$$$Compared with the state-of-the-art methods, our approach demonstrates a better performance in bandwidth utilization.$$$In all considered scenarios, a range based rate control approach outperforms the one without range by 19% to 35% in average QoE improvement.",BACKGROUND OBJECTIVES OBJECTIVES METHODS METHODS METHODS RESULTS/CONCLUSIONS RESULTS
D01796,"Business process models describe the way of working in an organization.$$$Typically, business process models distinguish between the normal flow of work and exceptions to that normal flow.$$$However, they often present an idealized view.$$$This means that unexpected exceptions - exceptions that are not modelled in the business process model - can also occur in practice.$$$This has an effect on the efficiency of the organization, because information systems are not developed to handle unexpected exceptions.$$$This paper studies the relation between the occurrence of exceptions and operational performance.$$$It does this by analyzing the execution logs of business processes from five organizations, classifying execution paths as normal or exceptional.$$$Subsequently, it analyzes the differences between normal and exceptional paths.$$$The results show that exceptions are related to worse operational performance in terms of a longer throughput time and that unexpected exceptions relate to a stronger increase in throughput time than expected exceptions.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS RESULTS
D05254,"We study the application of active learning techniques to the translation of unbounded data streams via interactive neural machine translation.$$$The main idea is to select, from an unbounded stream of source sentences, those worth to be supervised by a human agent.$$$The user will interactively translate those samples.$$$Once validated, these data is useful for adapting the neural machine translation model.$$$We propose two novel methods for selecting the samples to be validated.$$$We exploit the information from the attention mechanism of a neural machine translation system.$$$Our experiments show that the inclusion of active learning techniques into this pipeline allows to reduce the effort required during the process, while increasing the quality of the translation system.$$$Moreover, it enables to balance the human effort required for achieving a certain translation quality.$$$Moreover, our neural system outperforms classical approaches by a large margin.",OBJECTIVES METHODS METHODS METHODS METHODS METHODS RESULTS CONCLUSIONS CONCLUSIONS
D03033,"Strongly multiplicative linear secret sharing schemes (LSSS) have been a powerful tool for constructing secure multiparty computation protocols.$$$However, it remains open whether or not there exist efficient constructions of strongly multiplicative LSSS from general LSSS.$$$In this paper, we propose the new concept of a 3-multiplicative LSSS, and establish its relationship with strongly multiplicative LSSS.$$$More precisely, we show that any 3-multiplicative LSSS is a strongly multiplicative LSSS, but the converse is not true; and that any strongly multiplicative LSSS can be efficiently converted into a 3-multiplicative LSSS.$$$Furthermore, we apply 3-multiplicative LSSS to the computation of unbounded fan-in multiplication, which reduces its round complexity to four (from five of the previous protocol based on strongly multiplicative LSSS).$$$We also give two constructions of 3-multiplicative LSSS from Reed-Muller codes and algebraic geometric codes.$$$We believe that the construction and verification of 3-multiplicative LSSS are easier than those of strongly multiplicative LSSS.$$$This presents a step forward in settling the open problem of efficient constructions of strongly multiplicative LSSS from general LSSS.",BACKGROUND OBJECTIVES OBJECTIVES METHODS METHODS/RESULTS RESULTS CONCLUSIONS CONCLUSIONS
D02863,"Context: Visual aesthetics is increasingly seen as an essential factor in perceived usability, interaction, and overall appraisal of user interfaces especially with respect to mobile applications.$$$Yet, a question that remains is how to assess and to which extend users agree on visual aesthetics.$$$Objective: This paper analyzes the inter-rater agreement on visual aesthetics of user interfaces of Android apps as a basis for guidelines and evaluation models.$$$Method: We systematically collected ratings on the visual aesthetics of 100 user interfaces of Android apps from 10 participants and analyzed the frequency distribution, reliability and influencing design aspects.$$$Results: In general, user interfaces of Android apps are perceived more ugly than beautiful.$$$Yet, raters only moderately agree on the visual aesthetics.$$$Disagreements seem to be related to subtle differences with respect to layout, shapes, colors, typography, and background images.$$$Conclusion: Visual aesthetics is a key factor for the success of apps.$$$However, the considerable disagreement of raters on the perceived visual aesthetics indicates the need for a better understanding of this software quality with respect to mobile apps.",BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS RESULTS RESULTS CONCLUSIONS CONCLUSIONS
D02674,"Cloud Computing emerges from the global economic crisis as an option to use computing resources from a more rational point of view.$$$In other words, a cheaper way to have IT resources.$$$However, issues as security and privacy, SLA (Service Layer Agreement), resource sharing, and billing has left open questions about the real gains of that model.$$$This study aims to investigate state-of-the-art in Cloud Computing, identify gaps, challenges, synthesize available evidences both its use and development, and provides relevant information, clarifying open questions and common discussed issues about that model through literature.$$$The good practices of systematic map- ping study methodology were adopted in order to reach those objectives.$$$Al- though Cloud Computing is based on a business model with over 50 years of existence, evidences found in this study indicate that Cloud Computing still presents limitations that prevent the full use of the proposal on-demand.",BACKGROUND BACKGROUND RESULTS OBJECTIVES METHODS CONCLUSIONS
D05988,We completely determine the complexity status of the dominating set problem for hereditary graph classes defined by forbidden induced subgraphs with at most five vertices.,RESULTS
D00178,"The structure of a network dramatically affects the spreading phenomena unfolding upon it.$$$The contact distribution of the nodes has long been recognized as the key ingredient in influencing the outbreak events.$$$However, limited knowledge is currently available on the role of the weight of the edges on the persistence of a pathogen.$$$At the same time, recent works showed a strong influence of temporal network dynamics on disease spreading.$$$In this work we provide an analytical understanding, corroborated by numerical simulations, about the conditions for infected stable state in weighted networks.$$$In particular, we reveal the role of heterogeneity of edge weights and of the dynamic assignment of weights on the ties in the network in driving the spread of the epidemic.$$$In this context we show that when weights are dynamically assigned to ties in the network an heterogeneous distribution is able to hamper the diffusion of the disease, contrary to what happens when weights are fixed in time.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/METHODS RESULTS CONCLUSIONS
D06468,"Trajectory prediction (TP) is of great importance for a wide range of location-based applications in intelligent transport systems such as location-based advertising, route planning, traffic management, and early warning systems.$$$In the last few years, the widespread use of GPS navigation systems and wireless communication technology enabled vehicles has resulted in huge volumes of trajectory data.$$$The task of utilizing this data employing spatio-temporal techniques for trajectory prediction in an efficient and accurate manner is an ongoing research problem.$$$Existing TP approaches are limited to short-term predictions.$$$Moreover, they cannot handle a large volume of trajectory data for long-term prediction.$$$To address these limitations, we propose a scalable clustering and Markov chain based hybrid framework, called Traj-clusiVAT-based TP, for both short-term and long-term trajectory prediction, which can handle a large number of overlapping trajectories in a dense road network.$$$Traj-clusiVAT can also determine the number of clusters, which represent different movement behaviours in input trajectory data.$$$In our experiments, we compare our proposed approach with a mixed Markov model (MMM)-based scheme, and a trajectory clustering, NETSCAN-based TP method for both short- and long-term trajectory predictions.$$$We performed our experiments on two real, vehicle trajectory datasets, including a large-scale trajectory dataset consisting of 3.28 million trajectories obtained from 15,061 taxis in Singapore over a period of one month.$$$Experimental results on two real trajectory datasets show that our proposed approach outperforms the existing approaches in terms of both short- and long-term prediction performances, based on prediction accuracy and distance error (in km).",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES METHODS METHODS RESULTS/CONCLUSIONS
D00038,"This paper explores the idea that the universe is a virtual reality created by information processing, and relates this strange idea to the findings of modern physics about the physical world.$$$The virtual reality concept is familiar to us from online worlds, but our world as a virtual reality is usually a subject for science fiction rather than science.$$$Yet logically the world could be an information simulation running on a multi-dimensional space-time screen.$$$Indeed, if the essence of the universe is information, matter, charge, energy and movement could be aspects of information, and the many conservation laws could be a single law of information conservation.$$$If the universe were a virtual reality, its creation at the big bang would no longer be paradoxical, as every virtual system must be booted up.$$$It is suggested that whether the world is an objective reality or a virtual reality is a matter for science to resolve.$$$Modern information science can suggest how core physical properties like space, time, light, matter and movement could derive from information processing.$$$Such an approach could reconcile relativity and quantum theories, with the former being how information processing creates space-time, and the latter how it creates energy and matter.",OBJECTIVES BACKGROUND BACKGROUND CONCLUSIONS CONCLUSIONS METHODS CONCLUSIONS CONCLUSIONS
D03397,"Loss of thrust emergencies-e.g., induced by bird/drone strikes or fuel exhaustion-create the need for dynamic data-driven flight trajectory planning to advise pilots or control UAVs.$$$While total loss of thrust trajectories to nearby airports can be pre-computed for all initial points in a 3D flight plan, dynamic aspects such as partial power and airplane surface damage must be considered for accuracy.$$$In this paper, we propose a new Dynamic Data-Driven Avionics Software (DDDAS) approach which during flight updates a damaged aircraft performance model, used in turn to generate plausible flight trajectories to a safe landing site.$$$Our damaged aircraft model is parameterized on a baseline glide ratio for a clean aircraft configuration assuming best gliding airspeed on straight flight.$$$The model predicts purely geometric criteria for flight trajectory generation, namely, glide ratio and turn radius for different bank angles and drag configurations.$$$Given actual aircraft performance data, we dynamically infer the baseline glide ratio to update the damaged aircraft model.$$$Our new flight trajectory generation algorithm thus can significantly improve upon prior Dubins based trajectory generation work by considering these data-driven geometric criteria.$$$We further introduce a trajectory utility function to rank trajectories for safety.$$$As a use case, we consider the Hudson River ditching of US Airways 1549 in January 2009 using a flight simulator to evaluate our trajectories and to get sensor data.$$$In this case, a baseline glide ratio of 17.25:1 enabled us to generate trajectories up to 28 seconds after the birds strike, whereas, a 19:1 baseline glide ratio enabled us to generate trajectories up to 36 seconds after the birds strike.$$$DDDAS can significantly improve the accuracy of generated flight trajectories thereby enabling better decision support systems for pilots in emergency conditions.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS RESULTS METHODS RESULTS CONCLUSIONS CONCLUSIONS
D03810,"Kernel-based nonlinear mixing models have been applied to unmix spectral information of hyperspectral images when the type of mixing occurring in the scene is too complex or unknown.$$$Such methods, however, usually require the inversion of matrices of sizes equal to the number of spectral bands.$$$Reducing the computational load of these methods remains a challenge in large scale applications.$$$This paper proposes a centralized method for band selection (BS) in the reproducing kernel Hilbert space (RKHS).$$$It is based upon the coherence criterion, which sets the largest value allowed for correlations between the basis kernel functions characterizing the unmixing model.$$$We show that the proposed BS approach is equivalent to solving a maximum clique problem (MCP), that is, searching for the biggest complete subgraph in a graph.$$$Furthermore, we devise a strategy for selecting the coherence threshold and the Gaussian kernel bandwidth using coherence bounds for linearly independent bases.$$$Simulation results illustrate the efficiency of the proposed method.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS/RESULTS METHODS RESULTS
D04709,"Cyber-physical systems involve a network of discrete controllers that control physical processes.$$$Examples range from autonomous cars to implantable medical devices, which are highly safety critical.$$$Hybrid Automata (HA) based formal approach is gaining momentum for the specification and validation of CPS.$$$HA combines the model of the plant along with its discrete controller resulting in a piece-wise continuous system with discontinuities.$$$Accurate detection of these discontinuities, using appropriate level crossing detectors, is a key challenge to simulation of CPS based on HA.$$$Existing techniques employ time discrete numerical integration with bracketing for level crossing detection.$$$These techniques involve back-tracking and are highly non-deterministic and hence error prone.$$$As level crossings happen based on the values of continuous variables, Quantized State System (QSS)- integration may be more suitable.$$$Existing QSS integrators, based on fixed quanta, are also unsuitable for simulating HAs.$$$This is since the quantum selected is not dependent on the HA guard conditions, which are the main cause of discontinuities.$$$Considering this, we propose a new dynamic quanta based formal model called Quantized State Hybrid Automata (QSHA).$$$The developed formal model and the associated simulation framework guarantees that (1) all level crossings are accurately detected and (2) the time of the level crossing is also accurate within floating point error bounds.$$$Interestingly, benchmarking results reveal that the proposed simulation technique takes 720, 1.33 and 4.41 times fewer simulation steps compared to standard Quantized State System (QSS)-1, Runge-Kutta (RK)-45, and Differential Algebraic System Solver (DASSL) integration based techniques respectively.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS METHODS METHODS OBJECTIVES CONCLUSIONS RESULTS
D03367,"User modeling is a very important task for making relevant suggestions of venues to the users.$$$These suggestions are often based on matching the venues' features with the users' preferences, which can be collected from previously visited locations.$$$In this paper, we present a set of relevance scores for making personalized suggestions of points of interest.$$$These scores model each user by focusing on the different types of information extracted from venues that they have previously visited.$$$In particular, we focus on scores extracted from social information available on location-based social networks.$$$Our experiments, conducted on the dataset of the TREC Contextual Suggestion Track, show that social scores are more effective than scores based venues' content.",BACKGROUND BACKGROUND METHODS METHODS METHODS RESULTS
D04673,"Mammogram classification is directly related to computer-aided diagnosis of breast cancer.$$$Traditional methods requires great effort to annotate the training data by costly manual labeling and specialized computational models to detect these annotations during test.$$$Inspired by the success of using deep convolutional features for natural image analysis and multi-instance learning for labeling a set of instances/patches, we propose end-to-end trained deep multi-instance networks for mass classification based on whole mammogram without the aforementioned costly need to annotate the training data.$$$We explore three different schemes to construct deep multi-instance networks for whole mammogram classification.$$$Experimental results on the INbreast dataset demonstrate the robustness of proposed deep networks compared to previous work using segmentation and detection annotations in the training.",BACKGROUND BACKGROUND OBJECTIVES/METHODS METHODS RESULTS/CONCLUSIONS
D00586,"We present parallel algorithms for constructing and traversing sparse octrees on graphics processing units (GPUs).$$$The algorithms are based on parallel-scan and sort methods.$$$To test the performance and feasibility, we implemented them in CUDA in the form of a gravitational tree-code which completely runs on the GPU.$$$(The code is publicly available at: http://castle.strw.leidenuniv.nl/software.html) The tree construction and traverse algorithms are portable to many-core devices which have support for CUDA or OpenCL programming languages.$$$The gravitational tree-code outperforms tuned CPU code during the tree-construction and shows a performance improvement of more than a factor 20 overall, resulting in a processing rate of more than 2.8 million particles per second.",BACKGROUND/RESULTS METHODS METHODS OTHERS RESULTS/CONCLUSIONS
D01439,"This Ontologies are widely used as a means for solving the information heterogeneity problems on the web because of their capability to provide explicit meaning to the information.$$$They become an efficient tool for knowledge representation in a structured manner.$$$There is always more than one ontology for the same domain.$$$Furthermore, there is no standard method for building ontologies, and there are many ontology building tools using different ontology languages.$$$Because of these reasons, interoperability between the ontologies is very low.$$$Current ontology tools mostly use functions to build, edit and inference the ontology.$$$Methods for merging heterogeneous domain ontologies are not included in most tools.$$$This paper presents ontology merging methodology for building a single global ontology from heterogeneous eXtensible Markup Language (XML) data sources to capture and maintain all the knowledge which XML data sources can contain",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES/CONCLUSIONS
D01023,This paper investigates how secure information sharing with external vendors can be achieved in an Industrial Internet of Things (IIoT).$$$It also identifies necessary security requirements for secure information sharing based on identified security challenges stated by the industry.$$$The paper then proposes a roadmap for improving security in IIoT which investigates both short-term and long-term solutions for protecting IIoT devices.$$$The short-term solution is mainly based on integrating existing good practices.$$$The paper also outlines a long term solution for protecting IIoT devices with fine-grained access control for sharing data between external entities that would support cloud-based data storage.,OBJECTIVES BACKGROUND RESULTS CONCLUSIONS RESULTS
D00545,"Most researchers acknowledge an intrinsic hierarchy in the scholarly journals ('journal rank') that they submit their work to, and adjust not only their submission but also their reading strategies accordingly.$$$On the other hand, much has been written about the negative effects of institutionalizing journal rank as an impact measure.$$$So far, contributions to the debate concerning the limitations of journal rank as a scientific impact assessment tool have either lacked data, or relied on only a few studies.$$$In this review, we present the most recent and pertinent data on the consequences of our current scholarly communication system with respect to various measures of scientific quality (such as utility/citations, methodological soundness, expert ratings or retractions).$$$These data corroborate previous hypotheses: using journal rank as an assessment tool is bad scientific practice.$$$Moreover, the data lead us to argue that any journal rank (not only the currently-favored Impact Factor) would have this negative impact.$$$Therefore, we suggest that abandoning journals altogether, in favor of a library-based scholarly communication system, will ultimately be necessary.$$$This new system will use modern information technology to vastly improve the filter, sort and discovery functions of the current journal system.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/METHODS RESULTS CONCLUSIONS CONCLUSIONS CONCLUSIONS
D03881,"Many malware families utilize domain generation algorithms (DGAs) to establish command and control (C&C) connections.$$$While there are many methods to pseudorandomly generate domains, we focus in this paper on detecting (and generating) domains on a per-domain basis which provides a simple and flexible means to detect known DGA families.$$$Recent machine learning approaches to DGA detection have been successful on fairly simplistic DGAs, many of which produce names of fixed length.$$$However, models trained on limited datasets are somewhat blind to new DGA variants.$$$In this paper, we leverage the concept of generative adversarial networks to construct a deep learning based DGA that is designed to intentionally bypass a deep learning based detector.$$$In a series of adversarial rounds, the generator learns to generate domain names that are increasingly more difficult to detect.$$$In turn, a detector model updates its parameters to compensate for the adversarially generated domains.$$$We test the hypothesis of whether adversarially generated domains may be used to augment training sets in order to harden other machine learning models against yet-to-be-observed DGAs.$$$We detail solutions to several challenges in training this character-based generative adversarial network (GAN).$$$In particular, our deep learning architecture begins as a domain name auto-encoder (encoder + decoder) trained on domains in the Alexa one million.$$$Then the encoder and decoder are reassembled competitively in a generative adversarial network (detector + generator), with novel neural architectures and training strategies to improve convergence.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS METHODS METHODS METHODS
D01075,We define a weak compatibility condition for the Newest Vertex Bisection algorithm on simplex grids of any dimension and show that using this condition the iterative algorithm terminates successfully.$$$Additionally we provide an O(n) algorithm that renumbers any simplex grid to fulfil this condition.$$$Furthermore we conduct experiments to estimate the distance to the standard compatibility and also the geometric quality of the produced meshes.,RESULTS RESULTS RESULTS
D03967,"Motivated by applications in social network community analysis, we introduce a new clustering paradigm termed motif clustering.$$$Unlike classical clustering, motif clustering aims to minimize the number of clustering errors associated with both edges and certain higher order graph structures (motifs) that represent ""atomic units"" of social organizations.$$$Our contributions are two-fold: We first introduce motif correlation clustering, in which the goal is to agnostically partition the vertices of a weighted complete graph so that certain predetermined ""important"" social subgraphs mostly lie within the same cluster, while ""less relevant"" social subgraphs are allowed to lie across clusters.$$$We then proceed to introduce the notion of motif covers, in which the goal is to cover the vertices of motifs via the smallest number of (near) cliques in the graph.$$$Motif cover algorithms provide a natural solution for overlapping clustering and they also play an important role in latent feature inference of networks.$$$For both motif correlation clustering and its extension introduced via the covering problem, we provide hardness results, algorithmic solutions and community detection results for two well-studied social networks.",OBJECTIVES METHODS RESULTS RESULTS RESULTS RESULTS
D02550,"Predicting issue lifetime can help software developers, managers, and stakeholders effectively prioritize work, allocate development resources, and better understand project timelines.$$$Progress had been made on this prediction problem, but prior work has reported low precision and high false alarms.$$$The latest results also use complex models such as random forests that detract from their readability.$$$We solve both issues by using small, readable decision trees (under 20 lines long) and correlation feature selection to predict issue lifetime, achieving high precision and low false alarms (medians of 71% and 13% respectively).$$$We also address the problem of high class imbalance within issue datasets - when local data fails to train a good model, we show that cross-project data can be used in place of the local data.$$$In fact, cross-project data works so well that we argue it should be the default approach for learning predictors for issue lifetime.",BACKGROUND BACKGROUND BACKGROUND METHODS/RESULTS OBJECTIVES/RESULTS/CONCLUSIONS CONCLUSIONS
D04928,"Assessing the magnitude of cause-and-effect relations is one of the central challenges found throughout the empirical sciences.$$$The problem of identification of causal effects is concerned with determining whether a causal effect can be computed from a combination of observational data and substantive knowledge about the domain under investigation, which is formally expressed in the form of a causal graph.$$$In many practical settings, however, the knowledge available for the researcher is not strong enough so as to specify a unique causal graph.$$$Another line of investigation attempts to use observational data to learn a qualitative description of the domain called a Markov equivalence class, which is the collection of causal graphs that share the same set of observed features.$$$In this paper, we marry both approaches and study the problem of causal identification from an equivalence class, represented by a partial ancestral graph (PAG).$$$We start by deriving a set of graphical properties of PAGs that are carried over to its induced subgraphs.$$$We then develop an algorithm to compute the effect of an arbitrary set of variables on an arbitrary outcome set.$$$We show that the algorithm is strictly more powerful than the current state of the art found in the literature.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS/RESULTS METHODS/RESULTS RESULTS/CONCLUSIONS
D00753,"Augmenting deep neural networks with skip connections, as introduced in the so called ResNet architecture, surprised the community by enabling the training of networks of more than 1000 layers with significant performance gains.$$$It has been shown that identity skip connections eliminate singularities and improve the optimization landscape of the network.$$$This paper deciphers ResNet by analyzing the of effect of skip connections in the backward path and sets forth new theoretical results on the advantages of identity skip connections in deep neural networks.$$$We prove that the skip connections in the residual blocks facilitate preserving the norm of the gradient and lead to well-behaved and stable back-propagation, which is a desirable feature from optimization perspective.$$$We also show that, perhaps surprisingly, as more residual blocks are stacked, the network becomes more norm-preserving.$$$Traditionally, norm-preservation is enforced on the network only at beginning of the training, by using initialization techniques.$$$However, we show that identity skip connection retain norm-preservation during the training procedure.$$$Our theoretical arguments are supported by extensive empirical evidence.$$$Can we push for more norm-preservation?$$$We answer this question by proposing zero-phase whitening of the fully-connected layer and adding norm-preserving transition layers.$$$Our numerical investigations demonstrate that the learning dynamics and the performance of ResNets can be improved by making it even more norm preserving through changing only a few blocks in very deep residual networks.$$$Our results and the introduced modification for ResNet, referred to as Procrustes ResNets, can be used as a guide for studying more complex architectures such as DenseNet, training deeper networks, and inspiring new architectures.",BACKGROUND BACKGROUND OBJECTIVES/METHODS RESULTS RESULTS BACKGROUND RESULTS RESULTS OBJECTIVES METHODS RESULTS RESULTS
D04813,"The spread of ideas in the scientific community is often viewed as a competition, in which good ideas spread further because of greater intrinsic fitness, and publication venue and citation counts correlate with importance and impact.$$$However, relatively little is known about how structural factors influence the spread of ideas, and specifically how where an idea originates might influence how it spreads.$$$Here, we investigate the role of faculty hiring networks, which embody the set of researcher transitions from doctoral to faculty institutions, in shaping the spread of ideas in computer science, and the importance of where in the network an idea originates.$$$We consider comprehensive data on the hiring events of 5032 faculty at all 205 Ph.D.-granting departments of computer science in the U.S. and Canada, and on the timing and titles of 200,476 associated publications.$$$Analyzing five popular research topics, we show empirically that faculty hiring can and does facilitate the spread of ideas in science.$$$Having established such a mechanism, we then analyze its potential consequences using epidemic models to simulate the generic spread of research ideas and quantify the impact of where an idea originates on its longterm diffusion across the network.$$$We find that research from prestigious institutions spreads more quickly and completely than work of similar quality originating from less prestigious institutions.$$$Our analyses establish the theoretical trade-offs between university prestige and the quality of ideas necessary for efficient circulation.$$$Our results establish faculty hiring as an underlying mechanism that drives the persistent epistemic advantage observed for elite institutions, and provide a theoretical lower bound for the impact of structural inequality in shaping the spread of ideas in science.",BACKGROUND OBJECTIVES OBJECTIVES METHODS RESULTS METHODS RESULTS RESULTS CONCLUSIONS
D04304,"Scissor lifts, a staple of mechanical design, especially in competitive robotics, are a type of linkage that can be used to raise a load to some height, when acted upon by some force, usually exerted by an actuator.$$$The position of this actuator, however, can affect the mechanical advantage and velocity ratio of the system.$$$Hence, there needs to be a concrete way to analytically compare different actuator positions.$$$However, all current research into the analysis of scissor lifts either focusses only on the screw jack configuration, or derives separate force expressions for different actuator positions.$$$This, once again, leaves the decision between different actuator positions to trial and error, since the expression to test the potency of the position can only be derived once the position is chosen.$$$This paper proposes a derivation for a general force expression, in terms of a few carefully chosen position variables, which can be used to generate the force expression for any actuator position.$$$Hence, this expression illustrates exactly how each of the position variables (called a, b and i in this paper, as defined later) affect the force output, and hence can be used to pick an appropriate actuator position, by choosing values for the position variables that give the desired result.",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES BACKGROUND/OBJECTIVES OBJECTIVES/METHODS/RESULTS RESULTS
D04939,"This paper discusses two existing approaches to the correlation analysis between automatic evaluation metrics and human scores in the area of natural language generation.$$$Our experiments show that depending on the usage of a system- or sentence-level correlation analysis, correlation results between automatic scores and human judgments are inconsistent.",BACKGROUND RESULTS
D03434,"This thesis describes the development of fast algorithms for the computation of PERcentage CLOSure of eyes (PERCLOS) and Saccadic Ratio (SR).$$$PERCLOS and SR are two ocular parameters reported to be measures of alertness levels in human beings.$$$PERCLOS is the percentage of time in which at least 80% of the eyelid remains closed over the pupil.$$$Saccades are fast and simultaneous movement of both the eyes in the same direction.$$$SR is the ratio of peak saccadic velocity to the saccadic duration.$$$This thesis addresses the issues of image based estimation of PERCLOS and SR, prevailing in the literature such as illumination variation, poor illumination conditions, head rotations etc.$$$In this work, algorithms for real-time PERCLOS computation has been developed and implemented on an embedded platform.$$$The platform has been used as a case study for assessment of loss of attention in automotive drivers.$$$The SR estimation has been carried out offline as real-time implementation requires high frame rates of processing which is difficult to achieve due to hardware limitations.$$$The accuracy in estimation of the loss of attention using PERCLOS and SR has been validated using brain signals, which are reported to be an authentic cue for estimating the state of alertness in human beings.$$$The major contributions of this thesis include database creation, design and implementation of fast algorithms for estimating PERCLOS and SR on embedded computing platforms.",OBJECTIVES BACKGROUND BACKGROUND BACKGROUND BACKGROUND METHODS METHODS METHODS METHODS CONCLUSIONS CONCLUSIONS
D01607,"Modularisation, repetition, and symmetry are structural features shared by almost all biological neural networks.$$$These features are very unlikely to be found by the means of structural evolution of artificial neural networks.$$$This paper introduces NMODE, which is specifically designed to operate on neuro-modules.$$$NMODE addresses a second problem in the context of evolutionary robotics, which is incremental evolution of complex behaviours for complex machines, by offering a way to interface neuro-modules.$$$The scenario in mind is a complex walking machine, for which a locomotion module is evolved first, that is then extended by other modules in later stages.$$$We show that NMODE is able to evolve a locomotion behaviour for a standard six-legged walking machine in approximately 10 generations and show how it can be used for incremental evolution of a complex walking machine.$$$The entire source code used in this paper is publicly available through GitHub.",BACKGROUND BACKGROUND/OBJECTIVES OBJECTIVES OBJECTIVES OBJECTIVES RESULTS METHODS
D03916,"Morphological declension, which aims to inflect nouns to indicate number, case and gender, is an important task in natural language processing (NLP).$$$This research proposal seeks to address the degree to which Recurrent Neural Networks (RNNs) are efficient in learning to decline noun cases.$$$Given the challenge of data sparsity in processing morphologically rich languages and also, the flexibility of sentence structures in such languages, we believe that modeling morphological dependencies can improve the performance of neural network models.$$$It is suggested to carry out various experiments to understand the interpretable features that may lead to a better generalization of the learned models on cross-lingual tasks.",BACKGROUND OBJECTIVES OBJECTIVES/METHODS RESULTS
D06964,"Person re-identification (Re-ID) aims at recognizing the same person from images taken across different cameras.$$$To address this task, one typically requires a large amount labeled data for training an effective Re-ID model, which might not be practical for real-world applications.$$$To alleviate this limitation, we choose to exploit a sufficient amount of pre-existing labeled data from a different (auxiliary) dataset.$$$By jointly considering such an auxiliary dataset and the dataset of interest (but without label information), our proposed adaptation and re-identification network (ARN) performs unsupervised domain adaptation, which leverages information across datasets and derives domain-invariant features for Re-ID purposes.$$$In our experiments, we verify that our network performs favorably against state-of-the-art unsupervised Re-ID approaches, and even outperforms a number of baseline Re-ID methods which require fully supervised data for training.",BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS
D06650,"This paper discusses online algorithms for inverse dynamics modelling in robotics.$$$Several model classes including rigid body dynamics (RBD) models, data-driven models and semiparametric models (which are a combination of the previous two classes) are placed in a common framework.$$$While model classes used in the literature typically exploit joint velocities and accelerations, which need to be approximated resorting to numerical differentiation schemes, in this paper a new `derivative-free' framework is proposed that does not require this preprocessing step.$$$An extensive experimental study with real data from the right arm of the iCub robot is presented, comparing different model classes and estimation procedures, showing that the proposed `derivative-free' methods outperform existing methodologies.",BACKGROUND BACKGROUND OBJECTIVES/METHODS RESULTS/CONCLUSIONS
D02118,"Strategic suppression of grades, as well as early offers and contracts, are well-known phenomena in the matching process where graduating students apply to jobs or further education.$$$In this paper, we consider a game theoretic model of these phenomena introduced by Ostrovsky and Schwarz, and study the loss in social welfare resulting from strategic behavior of the schools, employers, and students.$$$We model grading of students as a game where schools suppress grades in order to improve their students' placements.$$$We also consider the quality loss due to unraveling of the matching market, the strategic behavior of students and employers in offering early contracts with the goal to improve the quality.$$$Our goal is to evaluate if strategic grading or unraveling of the market (or a combination of the two) can cause significant welfare loss compared to the optimal assignment of students to jobs.$$$To measure welfare of the assignment, we assume that welfare resulting from a job -- student pair is a separable and monotone function of student ability and the quality of the jobs.$$$Assuming uniform student quality distribution, we show that the quality loss from the above strategic manipulation is bounded by at most a factor of 2, and give improved bounds for some special cases of welfare functions.",BACKGROUND OBJECTIVES METHODS METHODS OBJECTIVES METHODS RESULTS
D02441,"Applications in political redistricting demand quantitative measures of geometric compactness to distinguish between simple and contorted shapes of legislative voting districts.$$$While the isoperimetric quotient, or ratio of area to perimeter squared, is commonly used in practice, it is sensitive to noisy data and irrelevant geographic features like coastline.$$$These issues are addressed in theory by the isoperimetric profile, which plots the minimum perimeter needed to inscribe shapes of different prescribed areas within the boundary of a shape; algorithms for computing this profile, however, are not known in practice.$$$Hence, in this paper, we propose a convex Eulerian relaxation of the isoperimetric profile using total variation.$$$We prove theoretical properties of our relaxation, showing that it still satisfies an isoperimetric inequality and yields a convex function of the prescribed area.$$$Furthermore, we provide a discretization of the problem, an optimization technique, and experiments demonstrating the value of our relaxation.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS
D04344,"Botnets continue to be an active threat against firms or companies and individuals worldwide.$$$Previous research regarding botnets has unveiled information on how the system and their stakeholders operate, but an insight on the economic structure that supports these stakeholders is lacking.$$$The objective of this research is to analyse the business model and determine the revenue stream of a botnet owner.$$$We also study the botnet life-cycle and determine the costs associated with it on the basis of four case studies.$$$We conclude that building a full scale cyber army from scratch is very expensive where as acquiring a previously developed botnet requires a little cost.$$$We find that initial setup and monthly costs were minimal compared to total revenue.",BACKGROUND OBJECTIVES OBJECTIVES OBJECTIVES CONCLUSIONS CONCLUSIONS
D01935,"This paper presents new alternatives to the well-known Bloom filter data structure.$$$The Bloom filter, a compact data structure supporting set insertion and membership queries, has found wide application in databases, storage systems, and networks.$$$Because the Bloom filter performs frequent random reads and writes, it is used almost exclusively in RAM, limiting the size of the sets it can represent.$$$This paper first describes the quotient filter, which supports the basic operations of the Bloom filter, achieving roughly comparable performance in terms of space and time, but with better data locality.$$$Operations on the quotient filter require only a small number of contiguous accesses.$$$The quotient filter has other advantages over the Bloom filter: it supports deletions, it can be dynamically resized, and two quotient filters can be efficiently merged.$$$The paper then gives two data structures, the buffered quotient filter and the cascade filter, which exploit the quotient filter advantages and thus serve as SSD-optimized alternatives to the Bloom filter.$$$The cascade filter has better asymptotic I/O performance than the buffered quotient filter, but the buffered quotient filter outperforms the cascade filter on small to medium data sets.$$$Both data structures significantly outperform recently-proposed SSD-optimized Bloom filter variants, such as the elevator Bloom filter, buffered Bloom filter, and forest-structured Bloom filter.$$$In experiments, the cascade filter and buffered quotient filter performed insertions 8.6-11 times faster than the fastest Bloom filter variant and performed lookups 0.94-2.56 times faster.",BACKGROUND/OBJECTIVES BACKGROUND BACKGROUND/OBJECTIVES METHODS/RESULTS METHODS/RESULTS RESULTS RESULTS RESULTS RESULTS RESULTS
D02200,"In the framework of convolutional neural networks that lie at the heart of deep learning, downsampling is often performed with a max-pooling operation that only retains the element with maximum activation, while completely discarding the information contained in other elements in a pooling region.$$$To address this issue, a novel pooling scheme, Ordinal Pooling Network (OPN), is introduced in this work.$$$OPN rearranges all the elements of a pooling region in a sequence and assigns different weights to these elements based upon their orders in the sequence, where the weights are learned via the gradient-based optimisation.$$$The results of our small-scale experiments on image classification task demonstrate that this scheme leads to a consistent improvement in the accuracy over max-pooling operation.$$$This improvement is expected to increase in deeper networks, where several layers of pooling become necessary.",BACKGROUND METHODS METHODS RESULTS CONCLUSIONS
D00051,"In wind farms, wake interaction leads to losses in power capture and accelerated structural degradation when compared to freestanding turbines.$$$One method to reduce wake losses is by misaligning the rotor with the incoming flow using its yaw actuator, thereby laterally deflecting the wake away from downstream turbines.$$$However, this demands an accurate and computationally tractable model of the wind farm dynamics.$$$This problem calls for a closed-loop solution.$$$This tutorial paper fills the scientific gap by demonstrating the full closed-loop controller synthesis cycle using a steady-state surrogate model.$$$Furthermore, a novel, computationally efficient and modular communication interface is presented that enables researchers to straight-forwardly test their control algorithms in large-eddy simulations.$$$High-fidelity simulations of a 9-turbine farm show a power production increase of up to 11% using the proposed closed-loop controller compared to traditional, greedy wind farm operation.",BACKGROUND BACKGROUND BACKGROUND METHODS OBJECTIVES OTHERS CONCLUSIONS
D03640,"Performing astronomical data analysis using only personal computers is becoming impractical for the very large data sets produced nowadays.$$$As analysis is not a task that can be automatized to its full extent, the idea of moving processing where the data is located means also moving the whole scientific process towards the archives and data centers.$$$Using Jupyter Notebooks as a remote service is a recent trend in data analysis that aims to deal with this problem, but harnessing the infrastructure to serve the astronomer without increasing the complexity of the service is a challenge.$$$In this paper we present the architecture and features of JOVIAL, a Cloud service where astronomers can safely use Jupyter notebooks over a personal space designed for high-performance processing under the high-availability principle.$$$We show that features existing only in specific packages can be adapted to run in the notebooks, and that algorithms can be adapted to run across the data center without necessarily redesigning them.",BACKGROUND BACKGROUND/OBJECTIVES BACKGROUND OBJECTIVES/METHODS RESULTS/CONCLUSIONS
D04767,"Person-to-person evaluations are prevalent in all kinds of discourse and important for establishing reputations, building social bonds, and shaping public opinion.$$$Such evaluations can be analyzed separately using signed social networks and textual sentiment analysis, but this misses the rich interactions between language and social context.$$$To capture such interactions, we develop a model that predicts individual A's opinion of individual B by synthesizing information from the signed social network in which A and B are embedded with sentiment analysis of the evaluative texts relating A to B.$$$We prove that this problem is NP-hard but can be relaxed to an efficiently solvable hinge-loss Markov random field, and we show that this implementation outperforms text-only and network-only versions in two very different datasets involving community-level decision-making: the Wikipedia Requests for Adminship corpus and the Convote U.S. Congressional speech corpus.",BACKGROUND BACKGROUND METHODS RESULTS
D06953,"Standardized corpora of undeciphered scripts, a necessary starting point for computational epigraphy, requires laborious human effort for their preparation from raw archaeological records.$$$Automating this process through machine learning algorithms can be of significant aid to epigraphical research.$$$Here, we take the first steps in this direction and present a deep learning pipeline that takes as input images of the undeciphered Indus script, as found in archaeological artifacts, and returns as output a string of graphemes, suitable for inclusion in a standard corpus.$$$The image is first decomposed into regions using Selective Search and these regions are classified as containing textual and/or graphical information using a convolutional neural network.$$$Regions classified as potentially containing text are hierarchically merged and trimmed to remove non-textual information.$$$The remaining textual part of the image is segmented using standard image processing techniques to isolate individual graphemes.$$$This set is finally passed to a second convolutional neural network to classify the graphemes, based on a standard corpus.$$$The classifier can identify the presence or absence of the most frequent Indus grapheme, the ""jar"" sign, with an accuracy of 92%.$$$Our results demonstrate the great potential of deep learning approaches in computational epigraphy and, more generally, in the digital humanities.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS METHODS RESULTS CONCLUSIONS
D02861,"In this paper, we propose a novel deep learning architecture for multi-label zero-shot learning (ML-ZSL), which is able to predict multiple unseen class labels for each input instance.$$$Inspired by the way humans utilize semantic knowledge between objects of interests, we propose a framework that incorporates knowledge graphs for describing the relationships between multiple labels.$$$Our model learns an information propagation mechanism from the semantic label space, which can be applied to model the interdependencies between seen and unseen class labels.$$$With such investigation of structured knowledge graphs for visual reasoning, we show that our model can be applied for solving multi-label classification and ML-ZSL tasks.$$$Compared to state-of-the-art approaches, comparable or improved performances can be achieved by our method.",BACKGROUND/OBJECTIVES BACKGROUND/METHODS RESULTS CONCLUSIONS RESULTS
D03083,"In this study, a shell-and-tube heat exchanger (STHX) design based on seven continuous independent design variables is proposed.$$$Delayed Rejection Adaptive Metropolis hasting (DRAM) was utilized as a powerful tool in the Markov chain Monte Carlo (MCMC) sampling method.$$$This Reverse Sampling (RS) method was used to find the probability distribution of design variables of the shell and tube heat exchanger.$$$Thanks to this probability distribution, an uncertainty analysis was also performed to find the quality of these variables.$$$In addition, a decision-making strategy based on confidence intervals of design variables and on the Total Annual Cost (TAC) provides the final selection of design variables.$$$Results indicated high accuracies for the estimation of design variables which leads to marginally improved performance compared to commonly used optimization methods.$$$In order to verify the capability of the proposed method, a case of study is also presented, it shows that a significant cost reduction is feasible with respect to multi-objective and single-objective optimization methods.$$$Furthermore, the selected variables have good quality (in terms of probability distribution) and a lower TAC was also achieved.$$$Results show that the costs of the proposed design are lower than those obtained from optimization method reported in previous studies.$$$The algorithm was also used to determine the impact of using probability values for the design variables rather than single values to obtain the best heat transfer area and pumping power.$$$In particular, a reduction of the TAC up to 3.5% was achieved in the case considered.",OBJECTIVES METHODS RESULTS RESULTS RESULTS CONCLUSIONS OTHERS RESULTS CONCLUSIONS OBJECTIVES CONCLUSIONS
D05434,"A broad class of software engineering problems can be generalized as the ""total recall problem"".$$$This short paper claims that identifying and exploring total recall language processing problems in software engineering is an important task with wide applicability.$$$To make that case, we show that by applying and adapting the state of the art active learning and text mining, solutions of the total recall problem, can help solve two important software engineering tasks: (a) supporting large literature reviews and (b) identifying software security vulnerabilities.$$$Furthermore, we conjecture that (c) test case prioritization and (d) static warning identification can also be categorized as the total recall problem.$$$The widespread applicability of ""total recall"" to software engineering suggests that there exists some underlying framework that encompasses not just natural language processing, but a wide range of important software engineering tasks.",BACKGROUND OBJECTIVES/CONCLUSIONS METHODS/RESULTS RESULTS CONCLUSIONS
D00430,"It is essential to find new ways of enabling experts in different disciplines to collaborate more efficient in the development of ever more complex systems, under increasing market pressures.$$$One possible solution for this challenge is to use a heterogeneous model-based approach where different teams can produce their conventional models and carry out their usual mono-disciplinary analysis, but in addition, the different models can be coupled for simulation (co-simulation), allowing the study of the global behavior of the system.$$$Due to its potential, co-simulation is being studied in many different disciplines but with limited sharing of findings.$$$Our aim with this work is to summarize, bridge, and enhance future research in this multidisciplinary area.$$$We provide an overview of co-simulation approaches, research challenges, and research opportunities, together with a detailed taxonomy with different aspects of the state of the art of co-simulation and classification for the past five years.$$$The main research needs identified are: finding generic approaches for modular, stable and accurate coupling of simulation units; and expressing the adaptations required to ensure that the coupling is correct.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS/CONCLUSIONS
D06892,"Grammatical Evolution (GE) is a population-based evolutionary algorithm, where a formal grammar is used in the genotype to phenotype mapping process.$$$PonyGE2 is an open source implementation of GE in Python, developed at UCD's Natural Computing Research and Applications group.$$$It is intended as an advertisement and a starting-point for those new to GE, a reference for students and researchers, a rapid-prototyping medium for our own experiments, and a Python workout.$$$As well as providing the characteristic genotype to phenotype mapping of GE, a search algorithm engine is also provided.$$$A number of sample problems and tutorials on how to use and adapt PonyGE2 have been developed.",BACKGROUND METHODS OBJECTIVES METHODS RESULTS
D04155,"Non-uniform and multi-illuminant color constancy are important tasks, the solution of which will allow to discard information about lighting conditions in the image.$$$Non-uniform illumination and shadows distort colors of real-world objects and mostly do not contain valuable information.$$$Thus, many computer vision and image processing techniques would benefit from automatic discarding of this information at the pre-processing step.$$$In this work we propose novel view on this classical problem via generative end-to-end algorithm, namely image conditioned Generative Adversarial Network.$$$We also demonstrate the potential of the given approach for joint shadow detection and removal.$$$Forced by the lack of training data, we render the largest existing shadow removal dataset and make it publicly available.$$$It consists of approximately 6,000 pairs of wide field of view synthetic images with and without shadows.",BACKGROUND BACKGROUND BACKGROUND METHODS RESULTS RESULTS CONCLUSIONS
D03890,"We represent planning as a set of loosely coupled network flow problems, where each network corresponds to one of the state variables in the planning domain.$$$The network nodes correspond to the state variable values and the network arcs correspond to the value transitions.$$$The planning problem is to find a path (a sequence of actions) in each network such that, when merged, they constitute a feasible plan.$$$In this paper we present a number of integer programming formulations that model these loosely coupled networks with varying degrees of flexibility.$$$Since merging may introduce exponentially many ordering constraints we implement a so-called branch-and-cut algorithm, in which these constraints are dynamically generated and added to the formulation when needed.$$$Our results are very promising, they improve upon previous planning as integer programming approaches and lay the foundation for integer programming approaches for cost optimal planning.",BACKGROUND/METHODS BACKGROUND/METHODS BACKGROUND/METHODS OBJECTIVES METHODS RESULTS/CONCLUSIONS
D01413,"A wide array of dynamic bandwidth allocation (DBA) mechanisms have recently been proposed for improving bandwidth utilization and reducing idle times and packets delays in passive optical networks (PONs).$$$The DBA evaluation studies commonly assumed that the report message for communicating the bandwidth demands of the distributed optical network units (ONUs) to the central optical line terminal (OLT) is scheduled for the end of an ONU's upstream transmission, after the ONU's payload data transmissions.$$$In this article, we conduct a detailed investigation of the impact of the report message scheduling (RMS), either at the beginning (i.e., before the pay load data) or the end of an ONU upstream transmission on PON performance.$$$We analytically characterize the reduction in channel idle time with reporting at the beginning of an upstream transmission compared to reporting at the end.$$$Our extensive simulation experiments consider both the Ethernet Passive Optical Networking (EPON) standard and the Gigabit PON (GPON) standard.$$$We find that for DBAs with offline sizing and scheduling of ONU upstream transmission grants at the end of a polling cycle, which processes requests from all ONUs, reporting at the beginning gives substantial reductions of mean packet delay at high loads.$$$For high-performing DBAs with online grant sizing and scheduling, which immediately processes individual ONU requests, or interleaving of ONUs groups, both reporting at the beginning or end give essentially the same average packet delays.",BACKGROUND BACKGROUND OBJECTIVES/METHODS METHODS METHODS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS
D01733,"A variety of representation learning approaches have been investigated for reinforcement learning; much less attention, however, has been given to investigating the utility of sparse coding.$$$Outside of reinforcement learning, sparse coding representations have been widely used, with non-convex objectives that result in discriminative representations.$$$In this work, we develop a supervised sparse coding objective for policy evaluation.$$$Despite the non-convexity of this objective, we prove that all local minima are global minima, making the approach amenable to simple optimization strategies.$$$We empirically show that it is key to use a supervised objective, rather than the more straightforward unsupervised sparse coding approach.$$$We compare the learned representations to a canonical fixed sparse representation, called tile-coding, demonstrating that the sparse coding representation outperforms a wide variety of tilecoding representations.",BACKGROUND BACKGROUND BACKGROUND/METHODS BACKGROUND/OBJECTIVES/CONCLUSIONS OBJECTIVES/METHODS/CONCLUSIONS METHODS/RESULTS/CONCLUSIONS
D02250,"Widespread adoption of indoor positioning systems based on WiFi fingerprinting is at present hindered by the large efforts required for measurements collection during the offline phase.$$$Two approaches were recently proposed to address such issue: crowdsourcing and RSS radiomap prediction based on either interpolation or propagation channel model fitting from a small set of measurements.$$$RSS prediction promises better positioning accuracy when compared to crowdsourcing but no systematic analysis of the impact of system parameters on positioning accuracy is available.$$$This paper fills this gap by introducing ViFi, an indoor positioning system that relies on RSS prediction based on Multi-Wall Multi-Floor (MWMF) propagation model to generate a discrete RSS radiomap (virtual fingerprints).$$$The ViFi system is subject to an extensive experimental analysis in order to address the role of all relevant system parameters.$$$Experimental results obtained in two different testbeds show that the introduction of virtual fingerprints allows reduction by a factor of 10 of the number of measurements, without significant loss in positioning accuracy.$$$The use of two testbeds also allows to derive general guidelines for the design and the implementation of a virtual fingerprinting system.",BACKGROUND BACKGROUND BACKGROUND/OBJECTIVES METHODS METHODS RESULTS METHODS
D06649,"This research introduces a new constraint domain for reasoning about data with uncertainty.$$$It extends convex modeling with the notion of p-box to gain additional quantifiable information on the data whereabouts.$$$Unlike existing approaches, the p-box envelops an unknown probability instead of approximating its representation.$$$The p-box bounds are uniform cumulative distribution functions (cdf) in order to employ linear computations in the probabilistic domain.$$$The reasoning by means of p-box cdf-intervals is an interval computation which is exerted on the real domain then it is projected onto the cdf domain.$$$This operation conveys additional knowledge represented by the obtained probabilistic bounds.$$$Empirical evaluation shows that, with minimal overhead, the output solution set realizes a full enclosure of the data along with tighter bounds on its probabilistic distributions.",OBJECTIVES METHODS METHODS METHODS METHODS RESULTS RESULTS/CONCLUSIONS
D02782,"Lane detection is to detect lanes on the road and provide the accurate location and shape of each lane.$$$It severs as one of the key techniques to enable modern assisted and autonomous driving systems.$$$However, several unique properties of lanes challenge the detection methods.$$$The lack of distinctive features makes lane detection algorithms tend to be confused by other objects with similar local appearance.$$$Moreover, the inconsistent number of lanes on a road as well as diverse lane line patterns, e.g. solid, broken, single, double, merging, and splitting lines further hamper the performance.$$$In this paper, we propose a deep neural network based method, named LaneNet, to break down the lane detection into two stages: lane edge proposal and lane line localization.$$$Stage one uses a lane edge proposal network for pixel-wise lane edge classification, and the lane line localization network in stage two then detects lane lines based on lane edge proposals.$$$Please note that the goal of our LaneNet is built to detect lane line only, which introduces more difficulties on suppressing the false detections on the similar lane marks on the road like arrows and characters.$$$Despite all the difficulties, our lane detection is shown to be robust to both highway and urban road scenarios method without relying on any assumptions on the lane number or the lane line patterns.$$$The high running speed and low computational cost endow our LaneNet the capability of being deployed on vehicle-based systems.$$$Experiments validate that our LaneNet consistently delivers outstanding performances on real world traffic scenarios.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND METHODS METHODS METHODS RESULTS RESULTS RESULTS
D04355,"In this work, we explore the outage probability (OP) analysis of selective decode and forward (SDF) cooperation protocol employing multiple-input multipleoutput (MIMO) orthogonal space-time block-code (OSTBC) over time varying Rayleigh fading channel conditions with imperfect channel state information (CSI) and mobile nodes.$$$The closed-form expressions of the per-block average OP, probability distribution function (PDF) of sum of independent and identically distributed (i.i.d.)$$$Gamma random variables (RVs), and cumulative distribution function (CDF) are derived and used to investigate the performance of the relaying network.$$$A mathematical framework is developed to derive the optimal source-relay power allocation factors.$$$It is shown that source node mobility affects the per-block average OP performance more significantly than the destination node mobility.$$$Nevertheless, in other node mobility situations, cooperative systems are constrained by an error floor with a higher signal to noise ratio (SNR) regimes.$$$Simulation results show that the equal power allocation is the only possible optimal solution when source to relay link is stronger than the relay to destination link.$$$Also, we allocate almost all the power to the source node when source to relay link is weaker than the relay to destination link.$$$Simulation results also show that OP simulated plots are in close agreement with the OP analytic plots at high SNR regimes.",BACKGROUND OBJECTIVES RESULTS METHODS CONCLUSIONS CONCLUSIONS CONCLUSIONS CONCLUSIONS CONCLUSIONS
D03947,"A growing number of people are changing the way they consume news, replacing the traditional physical newspapers and magazines by their virtual online versions or/and weblogs.$$$The interactivity and immediacy present in online news are changing the way news are being produced and exposed by media corporations.$$$News websites have to create effective strategies to catch people's attention and attract their clicks.$$$In this paper we investigate possible strategies used by online news corporations in the design of their news headlines.$$$We analyze the content of 69,907 headlines produced by four major global media corporations during a minimum of eight consecutive months in 2014.$$$In order to discover strategies that could be used to attract clicks, we extracted features from the text of the news headlines related to the sentiment polarity of the headline.$$$We discovered that the sentiment of the headline is strongly related to the popularity of the news and also with the dynamics of the posted comments on that particular news.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES RESULTS RESULTS CONCLUSIONS
D04519,"In this paper, we consider a class of nonlinear regression problems without the assumption of being independent and identically distributed.$$$We propose a correspondent mini-max problem for nonlinear regression and outline a numerical algorithm.$$$Such an algorithm can be applied in regression and machine learning problems, and yield better results than traditional regression and machine learning methods.",OBJECTIVES METHODS RESULTS
D03797,"Dynamically typed programming languages such as JavaScript and Python defer type checking to run time.$$$In order to maximize performance, dynamic language VM implementations must attempt to eliminate redundant dynamic type checks.$$$However, type inference analyses are often costly and involve tradeoffs between compilation time and resulting precision.$$$This has lead to the creation of increasingly complex multi-tiered VM architectures.$$$This paper introduces lazy basic block versioning, a simple JIT compilation technique which effectively removes redundant type checks from critical code paths.$$$This novel approach lazily generates type-specialized versions of basic blocks on-the-fly while propagating context-dependent type information.$$$This does not require the use of costly program analyses, is not restricted by the precision limitations of traditional type analyses and avoids the implementation complexity of speculative optimization techniques.$$$We have implemented intraprocedural lazy basic block versioning in a JavaScript JIT compiler.$$$This approach is compared with a classical flow-based type analysis.$$$Lazy basic block versioning performs as well or better on all benchmarks.$$$On average, 71% of type tests are eliminated, yielding speedups of up to 50%.$$$We also show that our implementation generates more efficient machine code than TraceMonkey, a tracing JIT compiler for JavaScript, on several benchmarks.$$$The combination of implementation simplicity, low algorithmic complexity and good run time performance makes basic block versioning attractive for baseline JIT compilers.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS METHODS RESULTS RESULTS RESULTS CONCLUSIONS
D04247,"Wireless networking allows users to access information and services regardless of location and physical infrastructure.$$$It is a fast growing technology due to its availability of wireless devices, flexibility, ease of installation and configuration.$$$With this rapid expansion of information and Communication Technology (ICT), the consumption of energy is also increasing.$$$In the early age of wireless technology, computing infrastructure focused on everywhere access, capacity and speed of technology.$$$But now computing infrastructure should be energy efficient because, in wireless networking, devices are mostly powered by a battery that is a limited source of energy and is a challenge for the researchers.$$$In computing infrastructure energy saving and environmental protection has become a global demand.$$$This paper proposed a computing infrastructure based on green computing for energy efficient wireless networking.$$$Further, some challenges and techniques like power consumption in network architecture, algorithm efficiency, virtualization, and dynamic power saving will be discussed to make energy efficient computing infrastructure.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OTHERS OBJECTIVES OBJECTIVES/RESULTS/CONCLUSIONS OBJECTIVES/RESULTS/CONCLUSIONS
D04109,"As smart meters continue to be deployed around the world collecting unprecedented levels of fine-grained data about consumers, we need to find mechanisms that are fair to both, (1) the electric utility who needs the data to improve their operations, and (2) the consumer who has a valuation of privacy but at the same time benefits from sharing consumption data.$$$In this paper we address this problem by proposing privacy contracts between electric utilities and consumers with the goal of maximizing the social welfare of both.$$$Our mathematical model designs an optimization problem between a population of users that have different valuations on privacy and the costs of operation by the utility.$$$We then show how contracts can change depending on the probability of a privacy breach.$$$This line of research can help inform not only current but also future smart meter collection practices.",BACKGROUND OBJECTIVES METHODS RESULTS CONCLUSIONS
D05649,"We consider the problem of representing collective behavior of large populations and predicting the evolution of a population distribution over a discrete state space.$$$A discrete time mean field game (MFG) is motivated as an interpretable model founded on game theory for understanding the aggregate effect of individual actions and predicting the temporal evolution of population distributions.$$$We achieve a synthesis of MFG and Markov decision processes (MDP) by showing that a special MFG is reducible to an MDP.$$$This enables us to broaden the scope of mean field game theory and infer MFG models of large real-world systems via deep inverse reinforcement learning.$$$Our method learns both the reward function and forward dynamics of an MFG from real data, and we report the first empirical test of a mean field game model of a real-world social media population.",OBJECTIVES METHODS METHODS METHODS RESULTS
D02274,"Case Law has a significant impact on the proceedings of legal cases.$$$Therefore, the information that can be obtained from previous court cases is valuable to lawyers and other legal officials when performing their duties.$$$This paper describes a methodology of applying discourse relations between sentences when processing text documents related to the legal domain.$$$In this study, we developed a mechanism to classify the relationships that can be observed among sentences in transcripts of United States court cases.$$$First, we defined relationship types that can be observed between sentences in court case transcripts.$$$Then we classified pairs of sentences according to the relationship type by combining a machine learning model and a rule-based approach.$$$The results obtained through our system were evaluated using human judges.$$$To the best of our knowledge, this is the first study where discourse relationships between sentences have been used to determine relationships among sentences in legal court case transcripts.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS METHODS OTHERS
D03393,"Background: Clinical decision support systems (CDSS) are a category of health information technologies that can assist clinicians to choose optimal treatments.$$$These support systems are based on clinical trials and expert knowledge; however, the amount of data available to these systems is limited.$$$For this reason, CDSSs could be significantly improved by using the knowledge obtained by treating patients.$$$This knowledge is mainly contained in patient records, whose usage is restricted due to privacy and confidentiality constraints.$$$Methods: A treatment effectiveness measure, containing valuable information for treatment prescription, was defined and a method to extract this measure from patient records was developed.$$$This method uses an advanced cryptographic technology, known as secure Multiparty Computation (henceforth referred to as MPC), to preserve the privacy of the patient records and the confidentiality of the clinicians' decisions.$$$Results: Our solution enables to compute the effectiveness measure of a treatment based on patient records, while preserving privacy.$$$Moreover, clinicians are not burdened with the computational and communication costs introduced by the privacy-preserving techniques that are used.$$$Our system is able to compute the effectiveness of 100 treatments for a specific patient in less than 24 minutes, querying a database containing 20,000 patient records.$$$Conclusion: This paper presents a novel and efficient clinical decision support system, that harnesses the potential and insights acquired from treatment data, while preserving the privacy of patient records and the confidentiality of clinician decisions.",BACKGROUND BACKGROUND BACKGROUND/OBJECTIVES BACKGROUND METHODS METHODS RESULTS RESULTS RESULTS CONCLUSIONS
D05774,"Deep neural networks with discrete latent variables offer the promise of better symbolic reasoning, and learning abstractions that are more useful to new tasks.$$$There has been a surge in interest in discrete latent variable models, however, despite several recent improvements, the training of discrete latent variable models has remained challenging and their performance has mostly failed to match their continuous counterparts.$$$Recent work on vector quantized autoencoders (VQ-VAE) has made substantial progress in this direction, with its perplexity almost matching that of a VAE on datasets such as CIFAR-10.$$$In this work, we investigate an alternate training technique for VQ-VAE, inspired by its connection to the Expectation Maximization (EM) algorithm.$$$Training the discrete bottleneck with EM helps us achieve better image generation results on CIFAR-10, and together with knowledge distillation, allows us to develop a non-autoregressive machine translation model whose accuracy almost matches a strong greedy autoregressive baseline Transformer, while being 3.3 times faster at inference.",BACKGROUND BACKGROUND/OBJECTIVES BACKGROUND METHODS RESULTS/CONCLUSIONS
D02307,"Time-varying delays adversely affect the performance of networked control sys-tems (NCS) and in the worst-case can destabilize the entire system.$$$Therefore, modelling network delays is important for designing NCS.$$$However, modelling time-varying delays is challenging because of their dependence on multiple pa-rameters such as length, contention, connected devices, protocol employed, and channel loading.$$$Further, these multiple parameters are inherently random and de-lays vary in a non-linear fashion with respect to time.$$$This makes estimating ran-dom delays challenging.$$$This investigation presents a methodology to model de-lays in NCS using experiments and general regression neural network (GRNN) due to their ability to capture non-linear relationship.$$$To compute the optimal smoothing parameter that computes the best estimates, genetic algorithm is used.$$$The objective of the genetic algorithm is to compute the optimal smoothing pa-rameter that minimizes the mean absolute percentage error (MAPE).$$$Our results illustrate that the resulting GRNN is able to predict the delays with less than 3% error.$$$The proposed delay model gives a framework to design compensation schemes for NCS subjected to time-varying delays.",BACKGROUND OTHERS BACKGROUND BACKGROUND OTHERS OBJECTIVES METHODS OBJECTIVES RESULTS CONCLUSIONS
D04560,"Although the recent progress is substantial, deep learning methods can be vulnerable to the maliciously generated adversarial examples.$$$In this paper, we present a novel training procedure and a thresholding test strategy, towards robust detection of adversarial examples.$$$In training, we propose to minimize the reverse cross-entropy (RCE), which encourages a deep network to learn latent representations that better distinguish adversarial examples from normal ones.$$$In testing, we propose to use a thresholding strategy as the detector to filter out adversarial examples for reliable predictions.$$$Our method is simple to implement using standard algorithms, with little extra training cost compared to the common cross-entropy minimization.$$$We apply our method to defend various attacking methods on the widely used MNIST and CIFAR-10 datasets, and achieve significant improvements on robust predictions under all the threat models in the adversarial setting.",BACKGROUND METHODS METHODS METHODS CONCLUSIONS RESULTS
D03625,"Identification of minimum number of local regions of a handwritten character image, containing well-defined discriminating features which are sufficient for a minimal but complete description of the character is a challenging task.$$$A new region selection technique based on the idea of an enhanced Harmony Search methodology has been proposed here.$$$The powerful framework of Harmony Search has been utilized to search the region space and detect only the most informative regions for correctly recognizing the handwritten character.$$$The proposed method has been tested on handwritten samples of Bangla Basic, Compound and mixed (Basic and Compound characters) characters separately with SVM based classifier using a longest run based feature-set obtained from the image subregions formed by a CG based quad-tree partitioning approach.$$$Applying this methodology on the above mentioned three types of datasets, respectively 43.75%, 12.5% and 37.5% gains have been achieved in terms of region reduction and 2.3%, 0.6% and 1.2% gains have been achieved in terms of recognition accuracy.$$$The results show a sizeable reduction in the minimal number of descriptive regions as well a significant increase in recognition accuracy for all the datasets using the proposed technique.$$$Thus the time and cost related to feature extraction is decreased without dampening the corresponding recognition accuracy.",BACKGROUND METHODS OBJECTIVES METHODS RESULTS CONCLUSIONS CONCLUSIONS
D05815,"This paper presents a new dataset called HUMBI - a large corpus of high fidelity models of behavioral signals in 3D from a diverse population measured by a massive multi-camera system.$$$With our novel design of a portable imaging system (consists of 107 HD cameras), we collect human behaviors from 164 subjects across gender, ethnicity, age, and physical condition at a public venue.$$$Using the multiview image streams, we reconstruct high fidelity models of five elementary parts: gaze, face, hands, body, and cloth.$$$As a byproduct, the 3D model provides geometrically consistent image annotation via 2D projection, e.g., body part segmentation.$$$This dataset is a significant departure from the existing human datasets that suffers from subject diversity.$$$We hope the HUMBI opens up a new opportunity for the development for behavioral imaging.",OBJECTIVES METHODS METHODS METHODS RESULTS CONCLUSIONS
D01741,Adams' extension of parsing expression grammars enables specifying indentation sensitivity using two non-standard grammar constructs --- indentation by a binary relation and alignment.$$$This paper proposes a step-by-step transformation of well-formed Adams' grammars for elimination of the alignment construct from the grammar.$$$The idea that alignment could be avoided was suggested by Adams but no process for achieving this aim has been described before.,BACKGROUND RESULTS BACKGROUND
D05611,"Active communication between robots and humans is essential for effective human-robot interaction.$$$To accomplish this objective, Cloud Robotics (CR) was introduced to make robots enhance their capabilities.$$$It enables robots to perform extensive computations in the cloud by sharing their outcomes.$$$Outcomes include maps, images, processing power, data, activities, and other robot resources.$$$But due to the colossal growth of data and traffic, CR suffers from serious latency issues.$$$Therefore, it is unlikely to scale a large number of robots particularly in human-robot interaction scenarios, where responsiveness is paramount.$$$Furthermore, other issues related to security such as privacy breaches and ransomware attacks can increase.$$$To address these problems, in this paper, we have envisioned the next generation of social robotic architectures based on Fog Robotics (FR) that inherits the strengths of Fog Computing to augment the future social robotic systems.$$$These new architectures can escalate the dexterity of robots by shoving the data closer to the robot.$$$Additionally, they can ensure that human-robot interaction is more responsive by resolving the problems of CR.$$$Moreover, experimental results are further discussed by considering a scenario of FR and latency as a primary factor comparing to CR models.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS OTHERS RESULTS/CONCLUSIONS
D02119,"Medical errors are leading causes of death in the US and as such, prevention of these errors is paramount to promoting health care.$$$Patient Safety Event reports are narratives describing potential adverse events to the patients and are important in identifying and preventing medical errors.$$$We present a neural network architecture for identifying the type of safety events which is the first step in understanding these narratives.$$$Our proposed model is based on a soft neural attention model to improve the effectiveness of encoding long sequences.$$$Empirical results on two large-scale real-world datasets of patient safety reports demonstrate the effectiveness of our method with significant improvements over existing methods.",BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS
D06045,"The original Pascaline was a mechanical calculator able to sum and subtract integers.$$$It encodes information in the angles of mechanical wheels and through a set of gears, and aided by gravity, could perform the calculations.$$$Here, we show that such a concept can be realized in electronics using memory elements such as memristive systems.$$$By using memristive emulators we have demonstrated experimentally the memcomputing version of the mechanical Pascaline, capable of processing and storing the numerical results in the multiple levels of each memristive element.$$$Our result is the first experimental demonstration of multidigit arithmetics with multi-level memory devices that further emphasizes the versatility and potential of memristive systems for future massively-parallel high-density computing architectures.",BACKGROUND BACKGROUND OBJECTIVES RESULTS CONCLUSIONS
D02685,"This paper presents a bionic reflex control strategy for a kinematically constrained robotic finger.$$$Here, the bionic reflex is achieved through a force tracking impedance control strategy.$$$The dynamic model of the finger is reduced subject to kinematic constraints.$$$Thereafter, an impedance control strategy that allows exact tracking of forces is discussed.$$$Simulation results for a single finger holding a rectangular object against a flat surface are presented.$$$Bionic reflex response time is of the order of milliseconds.",BACKGROUND/OBJECTIVES METHODS METHODS METHODS RESULTS CONCLUSIONS
D05621,"The discrimination and simplicity of features are very important for effective and efficient pedestrian detection.$$$However, most state-of-the-art methods are unable to achieve good tradeoff between accuracy and efficiency.$$$Inspired by some simple inherent attributes of pedestrians (i.e., appearance constancy and shape symmetry), we propose two new types of non-neighboring features (NNF): side-inner difference features (SIDF) and symmetrical similarity features (SSF).$$$SIDF can characterize the difference between the background and pedestrian and the difference between the pedestrian contour and its inner part.$$$SSF can capture the symmetrical similarity of pedestrian shape.$$$However, it's difficult for neighboring features to have such above characterization abilities.$$$Finally, we propose to combine both non-neighboring and neighboring features for pedestrian detection.$$$It's found that non-neighboring features can further decrease the average miss rate by 4.44%.$$$Experimental results on INRIA and Caltech pedestrian datasets demonstrate the effectiveness and efficiency of the proposed method.$$$Compared to the state-of-the-art methods without using CNN, our method achieves the best detection performance on Caltech, outperforming the second best method (i.e., Checkboards) by 1.63%.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS OBJECTIVES METHODS RESULTS RESULTS CONCLUSIONS
D04368,"Automated detection of abnormalities in data has been studied in research area in recent years because of its diverse applications in practice including video surveillance, industrial damage detection and network intrusion detection.$$$However, building an effective anomaly detection system is a non-trivial task since it requires to tackle challenging issues of the shortage of annotated data, inability of defining anomaly objects explicitly and the expensive cost of feature engineering procedure.$$$Unlike existing appoaches which only partially solve these problems, we develop a unique framework to cope the problems above simultaneously.$$$Instead of hanlding with ambiguous definition of anomaly objects, we propose to work with regular patterns whose unlabeled data is abundant and usually easy to collect in practice.$$$This allows our system to be trained completely in an unsupervised procedure and liberate us from the need for costly data annotation.$$$By learning generative model that capture the normality distribution in data, we can isolate abnormal data points that result in low normality scores (high abnormality scores).$$$Moreover, by leverage on the power of generative networks, i.e. energy-based models, we are also able to learn the feature representation automatically rather than replying on hand-crafted features that have been dominating anomaly detection research over many decades.$$$We demonstrate our proposal on the specific application of video anomaly detection and the experimental results indicate that our method performs better than baselines and are comparable with state-of-the-art methods in many benchmark video anomaly detection datasets.",BACKGROUND BACKGROUND BACKGROUND/OBJECTIVES BACKGROUND/OBJECTIVES/METHODS METHODS/CONCLUSIONS METHODS/CONCLUSIONS METHODS/CONCLUSIONS RESULTS/CONCLUSIONS
D01551,"Recommender systems benefit us in tackling the problem of information overload by predicting our potential choices among diverse niche objects.$$$So far, a variety of personalized recommendation algorithms have been proposed and most of them are based on similarities, such as collaborative filtering and mass diffusion.$$$Here, we propose a novel vertex similarity index named CosRA, which combines advantages of both the cosine index and the resource-allocation (RA) index.$$$By applying the CosRA index to real recommender systems including MovieLens, Netflix and RYM, we show that the CosRA-based method has better performance in accuracy, diversity and novelty than some benchmark methods.$$$Moreover, the CosRA index is free of parameters, which is a significant advantage in real applications.$$$Further experiments show that the introduction of two turnable parameters cannot remarkably improve the overall performance of the CosRA index.",BACKGROUND BACKGROUND OBJECTIVES/METHODS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS
D00231,"Covariant-contravariant simulation and conformance simulation generalize plain simulation and try to capture the fact that it is not always the case that ""the larger the number of behaviors, the better"".$$$We have previously studied their logical characterizations and in this paper we present the axiomatizations of the preorders defined by the new simulation relations and their induced equivalences.$$$The interest of our results lies in the fact that the axiomatizations help us to know the new simulations better, understanding in particular the role of the contravariant characteristics and their interplay with the covariant ones; moreover, the axiomatizations provide us with a powerful tool to (algebraically) prove results of the corresponding semantics.$$$But we also consider our results interesting from a metatheoretical point of view: the fact that the covariant-contravariant simulation equivalence is indeed ground axiomatizable when there is no action that exhibits both a covariant and a contravariant behaviour, but becomes non-axiomatizable whenever we have together actions of that kind and either covariant or contravariant actions, offers us a new subtle example of the narrow border separating axiomatizable and non-axiomatizable semantics.$$$We expect that by studying these examples we will be able to develop a general theory separating axiomatizable and non-axiomatizable semantics.",BACKGROUND BACKGROUND/OBJECTIVES METHODS RESULTS/CONCLUSIONS OTHERS
D00535,"Deep generative models are tremendously successful in learning low-dimensional latent representations that well-describe the data.$$$These representations, however, tend to much distort relationships between points, i.e. pairwise distances tend to not reflect semantic similarities well.$$$This renders unsupervised tasks, such as clustering, difficult when working with the latent representations.$$$We demonstrate that taking the geometry of the generative model into account is sufficient to make simple clustering algorithms work well over latent representations.$$$Leaning on the recent finding that deep generative models constitute stochastically immersed Riemannian manifolds, we propose an efficient algorithm for computing geodesics (shortest paths) and computing distances in the latent space, while taking its distortion into account.$$$We further propose a new architecture for modeling uncertainty in variational autoencoders, which is essential for understanding the geometry of deep generative models.$$$Experiments show that the geodesic distance is very likely to reflect the internal structure of the data.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/METHODS OBJECTIVES/METHODS RESULTS/CONCLUSIONS
D00240,"Computation models such as circuits describe sequences of computation steps that are carried out one after the other.$$$In other words, algorithm design is traditionally subject to the restriction imposed by a fixed causal order.$$$We address a novel computing paradigm beyond quantum computing, replacing this assumption by mere logical consistency: We study non-causal circuits, where a fixed time structure within a gate is locally assumed whilst the global causal structure between the gates is dropped.$$$We present examples of logically consistent non- causal circuits outperforming all causal ones; they imply that suppressing loops entirely is more restrictive than just avoiding the contradictions they can give rise to.$$$That fact is already known for correlations as well as for communication, and we here extend it to computation.",BACKGROUND BACKGROUND OBJECTIVES/METHODS RESULTS RESULTS/CONCLUSIONS
D06098,"Weighted low-rank approximation (WLRA), a dimensionality reduction technique for data analysis, has been successfully used in several applications, such as in collaborative filtering to design recommender systems or in computer vision to recover structure from motion.$$$In this paper, we study the computational complexity of WLRA and prove that it is NP-hard to find an approximate solution, even when a rank-one approximation is sought.$$$Our proofs are based on a reduction from the maximum-edge biclique problem, and apply to strictly positive weights as well as binary weights (the latter corresponding to low-rank matrix approximation with missing data).",BACKGROUND OBJECTIVES/RESULTS/CONCLUSIONS METHODS
D05442,Computationally efficient classification system architecture is proposed.$$$It utilizes fast tensor-vector multiplication algorithm to apply linear operators upon input signals .$$$The approach is applicable to wide variety of recognition system architectures ranging from single stage matched filter bank classifiers to complex neural networks with unlimited number of hidden layers.,OBJECTIVES/RESULTS METHODS CONCLUSIONS
D05941,"Chirality plays an important role in physics, chemistry, biology, and other fields.$$$It describes an essential symmetry in structure.$$$However, chirality invariants are usually complicated in expression or difficult to evaluate.$$$In this paper, we present five general three-dimensional chirality invariants based on the generating functions.$$$And the five chiral invariants have four characteristics:(1) They play an important role in the detection of symmetry, especially in the treatment of 'false zero' problem.$$$(2) Three of the five chiral invariants decode an universal chirality index.$$$(3) Three of them are proposed for the first time.$$$(4) The five chiral invariants have low order no bigger than 4, brief expression, low time complexity O(n) and can act as descriptors of three-dimensional objects in shape analysis.$$$The five chiral invariants give a geometric view to study the chiral invariants.$$$And the experiments show that the five chirality invariants are effective and efficient, they can be used as a tool for symmetry detection or features in shape analysis.",BACKGROUND BACKGROUND BACKGROUND RESULTS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS
D03818,"This work presents a model predictive controller (MPC) that is able to handle linear time-varying (LTV) plants with PWM control.$$$The MPC is based on a planner that employs a PAM or impulsive approximation as a hot-start and then uses explicit linearization around successive PWM solutions for rapidly improving the solution by means of linear programming.$$$As an example, the problem of rendezvous of spacecraft for eccentric target orbits is considered.$$$The problem is modeled by the LTV Tschauner-Hempel equations, whose transition matrix is explicit; this is exploited by the algorithm for rapid convergence.$$$The efficacy of the method is shown in a simulation study.",OBJECTIVES/METHODS METHODS OBJECTIVES BACKGROUND/METHODS RESULTS
D05921,"Virtualization is generally adopted in server and desktop environments to provide for fault tolerance, resource management, and energy efficiency.$$$Virtualization enables parallel execution of multiple operating systems (OSs) while sharing the hardware resources.$$$Virtualization was previously not deemed as feasible technology for mobile and embedded devices due to their limited processing and memory resource.$$$However, the enterprises are advocating Bring Your Own Device (BYOD) applications that enable co-existence of heterogeneous OSs on a single mobile device.$$$Moreover, embedded device require virtualization for logical isolation of secure and general purpose OSs on a single device.$$$In this paper, we investigate the processor architectures in the mobile and embedded space while examining their formal visualizability.$$$We also compare the virtualization solutions enabling coexistence of multiple OSs in Multicore Processor System-on-Chip (MPSoC) mobile and embedded systems.$$$We advocate that virtualization is necessary to manage resource in MPSoC designs and to enable BYOD, security, and logical isolation use cases.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND/OBJECTIVES OBJECTIVES METHODS METHODS RESULTS/CONCLUSIONS
D03134,"This article presents two area/latency optimized gate level asynchronous full adder designs which correspond to early output logic.$$$The proposed full adders are constructed using the delay-insensitive dual-rail code and adhere to the four-phase return-to-zero handshaking.$$$For an asynchronous ripple carry adder (RCA) constructed using the proposed early output full adders, the relative-timing assumption becomes necessary and the inherent advantages of the relative-timed RCA are: (1) computation with valid inputs, i.e., forward latency is data-dependent, and (2) computation with spacer inputs involves a bare minimum constant reverse latency of just one full adder delay, thus resulting in the optimal cycle time.$$$With respect to different 32-bit RCA implementations, and in comparison with the optimized strong-indication, weak-indication, and early output full adder designs, one of the proposed early output full adders achieves respective reductions in latency by 67.8, 12.3 and 6.1 %, while the other proposed early output full adder achieves corresponding reductions in area by 32.6, 24.6 and 6.9 %, with practically no power penalty.$$$Further, the proposed early output full adders based asynchronous RCAs enable minimum reductions in cycle time by 83.4, 15, and 8.8 % when considering carry-propagation over the entire RCA width of 32-bits, and maximum reductions in cycle time by 97.5, 27.4, and 22.4 % for the consideration of a typical carry chain length of 4 full adder stages, when compared to the least of the cycle time estimates of various strong-indication, weak-indication, and early output asynchronous RCAs of similar size.$$$All the asynchronous full adders and RCAs were realized using standard cells in a semi-custom design fashion based on a 32/28 nm CMOS process technology.",BACKGROUND/METHODS BACKGROUND METHODS/RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS METHODS
D01889,"Skin cancer is a major public health problem, with over 5 million newly diagnosed cases in the United States each year.$$$Melanoma is the deadliest form of skin cancer, responsible for over 9,000 deaths each year.$$$In this paper, we propose an ensemble of deep convolutional neural networks to classify dermoscopy images into three classes.$$$To achieve the highest classification accuracy, we fuse the outputs of the softmax layers of four different neural architectures.$$$For aggregation, we consider the individual accuracies of the networks weighted by the confidence values provided by their final softmax layers.$$$This fusion-based approach outperformed all the individual neural networks regarding classification accuracy.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS RESULTS
D01329,"We establish basic information about the set of tight tensors, the tensors with continuous regular symmetry.$$$Our motivation is Strassen's astounding asymptotic rank conjecture that the asymptotic rank of any tight tensor is minimal.$$$In particular, we determine the dimension of the set of tight tensors.$$$Surprisingly we prove this dimension equals the dimension of the set of oblique tensors, a less restrictive class of tensors that Strassen identified as useful for his laser method.",OBJECTIVES BACKGROUND CONCLUSIONS RESULTS/CONCLUSIONS
D03384,"This paper proposes a parallel optimization algorithm for cooperative automation of large-scale connected vehicles.$$$The task of cooperative automation is formulated as a centralized optimization problem taking the whole decision space of all vehicles into account.$$$Considering the uncertainty of the environment, the problem is solved in a receding horizon fashion.$$$Then, we employ the alternating direction method of multipliers (ADMM) to solve the centralized optimization in a parallel way, which scales more favorably to large-scale instances.$$$Also, Taylor series is used to linearize nonconvex constraints caused by coupling collision avoidance constraints among interactive vehicles.$$$Simulations with two typical traffic scenes for multiple vehicles demonstrate the effectiveness and efficiency of our method.",OBJECTIVES METHODS METHODS METHODS METHODS RESULTS/CONCLUSIONS
D01455,"We present a technique that uses images, videos and sensor data taken from first-person point-of-view devices to perform egocentric field-of-view (FOV) localization.$$$We define egocentric FOV localization as capturing the visual information from a person's field-of-view in a given environment and transferring this information onto a reference corpus of images and videos of the same space, hence determining what a person is attending to.$$$Our method matches images and video taken from the first-person perspective with the reference corpus and refines the results using the first-person's head orientation information obtained using the device sensors.$$$We demonstrate single and multi-user egocentric FOV localization in different indoor and outdoor environments with applications in augmented reality, event understanding and studying social interactions.",OBJECTIVES/METHODS BACKGROUND METHODS RESULTS/CONCLUSIONS
D05717,"People usually get involved in multiple social networks to enjoy new services or to fulfill their needs.$$$Many new social networks try to attract users of other existing networks to increase the number of their users.$$$Once a user (called source user) of a social network (called source network) joins a new social network (called target network), a new inter-network link (called anchor link) is formed between the source and target networks.$$$In this paper, we concentrated on predicting the formation of such anchor links between heterogeneous social networks.$$$Unlike conventional link prediction problems in which the formation of a link between two existing users within a single network is predicted, in anchor link prediction, the target user is missing and will be added to the target network once the anchor link is created.$$$To solve this problem, we use meta-paths as a powerful tool for utilizing heterogeneous information in both the source and target networks.$$$To this end, we propose an effective general meta-path-based approach called Connector and Recursive Meta-Paths (CRMP).$$$By using those two different categories of meta-paths, we model different aspects of social factors that may affect a source user to join the target network, resulting in the formation of a new anchor link.$$$Extensive experiments on real-world heterogeneous social networks demonstrate the effectiveness of the proposed method against the recent methods.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES BACKGROUND METHODS METHODS METHODS RESULTS
D05777,"Availability of large amount of clinical data is opening up new research avenues in a number of fields.$$$An exciting field in this respect is healthcare, where secondary use of healthcare data is beginning to revolutionize healthcare.$$$Except for availability of Big Data, both medical data from healthcare institutions (such as EMR data) and data generated from health and wellbeing devices (such as personal trackers), a significant contribution to this trend is also being made by recent advances on machine learning, specifically deep learning algorithms.",BACKGROUND BACKGROUND BACKGROUND
D02139,"In future traffic scenarios, vehicles and other traffic participants will be interconnected and equipped with various types of sensors, allowing for cooperation based on data or information exchange.$$$This article presents an approach to cooperative tracking of cyclists using smart devices and infrastructure-based sensors.$$$A smart device is carried by the cyclists and an intersection is equipped with a wide angle stereo camera system.$$$Two tracking models are presented and compared.$$$The first model is based on the stereo camera system detections only, whereas the second model cooperatively combines the camera based detections with velocity and yaw rate data provided by the smart device.$$$Our aim is to overcome limitations of tracking approaches based on single data sources.$$$We show in numerical evaluations on scenes where cyclists are starting or turning right that the cooperation leads to an improvement in both the ability to keep track of a cyclist and the accuracy of the track particularly when it comes to occlusions in the visual system.$$$We, therefore, contribute to the safety of vulnerable road users in future traffic.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS OBJECTIVES RESULTS CONCLUSIONS
D04443,"We study a two-level uncapacitated lot-sizing problem with inventory bounds that occurs in a supply chain composed of a supplier and a retailer.$$$The first level with the demands is the retailer level and the second one is the supplier level.$$$The aim is to minimize the cost of the supply chain so as to satisfy the demands when the quantity of item that can be held in inventory at each period is limited.$$$The inventory bounds can be imposed at the retailer level, at the supplier level or at both levels.$$$We propose a polynomial dynamic programming algorithm to solve this problem when the inventory bounds are set on the retailer level.$$$When the inventory bounds are set on the supplier level, we show that the problem is NP-hard.$$$We give a pseudo-polynomial algorithm which solves this problem when there are inventory bounds on both levels.$$$In the case where demand lot-splitting is not allowed, i.e. each demand has to be satisfied by a single order, we prove that the uncapacitated lot-sizing problem with inventory bounds is strongly NP-hard.$$$This implies that the two-level lot-sizing problems with inventory bounds are also strongly NP-hard when demand lot-splitting is considered.",BACKGROUND BACKGROUND BACKGROUND/OBJECTIVES BACKGROUND/OBJECTIVES METHODS/RESULTS RESULTS METHODS/RESULTS RESULTS RESULTS
D02577,"Mobile edge cloud is emerging as a promising technology to the internet of things and cyber-physical system applications such as smart home and intelligent video surveillance.$$$In a smart home, various sensors are deployed to monitor the home environment and physiological health of individuals.$$$The data collected by sensors are sent to an application, where numerous algorithms for emotion and sentiment detection, activity recognition and situation management are applied to provide healthcare- and emergency-related services and to manage resources at the home.$$$The executions of these algorithms require a vast amount of computing and storage resources.$$$To address the issue, the conventional approach is to send the collected data to an application on an internet cloud.$$$This approach has several problems such as high communication latency, communication energy consumption and unnecessary data traffic to the core network.$$$To overcome the drawbacks of the conventional cloud-based approach, a new system called mobile edge cloud is proposed.$$$In mobile edge cloud, multiple mobiles and stationary devices interconnected through wireless local area networks are combined to create a small cloud infrastructure at a local physical area such as a home.$$$Compared to traditional mobile distributed computing systems, mobile edge cloud introduces several complex challenges due to the heterogeneous computing environment, heterogeneous and dynamic network environment, node mobility, and limited battery power.$$$The real-time requirements associated with the internet of things and cyber-physical system applications make the problem even more challenging.$$$In this paper, we describe the applications and challenges associated with the design and development of mobile edge cloud system and propose an architecture based on a cross layer design approach for effective decision making.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES OTHERS OTHERS OBJECTIVES
D02273,"Network coding can significantly improve the transmission rate of communication networks with packet loss compared with routing.$$$However, using network coding usually incurs high computational and storage costs in the network devices and terminals.$$$For example, some network coding schemes require the computational and/or storage capacities of an intermediate network node to increase linearly with the number of packets for transmission, making such schemes difficult to be implemented in a router-like device that has only constant computational and storage capacities.$$$In this paper, we introduce BATched Sparse code (BATS code), which enables a digital fountain approach to resolve the above issue.$$$BATS code is a coding scheme that consists of an outer code and an inner code.$$$The outer code is a matrix generation of a fountain code.$$$It works with the inner code that comprises random linear coding at the intermediate network nodes.$$$BATS codes preserve such desirable properties of fountain codes as ratelessness and low encoding/decoding complexity.$$$The computational and storage capacities of the intermediate network nodes required for applying BATS codes are independent of the number of packets for transmission.$$$Almost capacity-achieving BATS code schemes are devised for unicast networks, two-way relay networks, tree networks, a class of three-layer networks, and the butterfly network.$$$For general networks, under different optimization criteria, guaranteed decoding rates for the receiving nodes can be obtained.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS RESULTS RESULTS RESULTS RESULTS
D02619,"Conventional seq2seq chatbot models only try to find the sentences with the highest probabilities conditioned on the input sequences, without considering the sentiment of the output sentences.$$$Some research works trying to modify the sentiment of the output sequences were reported.$$$In this paper, we propose five models to scale or adjust the sentiment of the chatbot response: persona-based model, reinforcement learning, plug and play model, sentiment transformation network and cycleGAN, all based on the conventional seq2seq model.$$$We also develop two evaluation metrics to estimate if the responses are reasonable given the input.$$$These metrics together with other two popularly used metrics were used to analyze the performance of the five proposed models on different aspects, and reinforcement learning and cycleGAN were shown to be very attractive.$$$The evaluation metrics were also found to be well correlated with human evaluation.",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES METHODS RESULTS
D06356,"Nowadays, Tehran Urban and Suburban Railway System (TUSRS) is going to be completed by eight lines and 149 stations.$$$This complex transportation system contains 168 links between each station pairs and 20 cross-section and Y-branch stations among all eight lines.$$$In this study, we considered TUSRS as a complex network and undertook several analyzes based on graph theory.$$$Examining e.g. centrality measures, we identified central stations within TUSRS.$$$This analysis could be useful for redistributing strategy of the overcrowded stations and improving the organization of maintaining system.$$$These findings are also promising for better designing the systems of tomorrow in other metropolitan areas in Iran.",BACKGROUND BACKGROUND METHODS METHODS CONCLUSIONS CONCLUSIONS
D02624,"qPCF is a paradigmatic quantum programming language that ex- tends PCF with quantum circuits and a quantum co-processor.$$$Quantum circuits are treated as classical data that can be duplicated and manipulated in flexible ways by means of a dependent type system.$$$The co-processor is essentially a standard QRAM device, albeit we avoid to store permanently quantum states in between two co-processor's calls.$$$Despite its quantum features, qPCF retains the classic programming approach of PCF.$$$We introduce qPCF syntax, typing rules, and its operational semantics.$$$We prove fundamental properties of the system, such as Preservation and Progress Theorems.$$$Moreover, we provide some higher-order examples of circuit encoding.",OBJECTIVES/RESULTS OBJECTIVES/RESULTS BACKGROUND RESULTS METHODS RESULTS/CONCLUSIONS CONCLUSIONS
D00469,"In this work, we study the extent to which structural connectomes and topological derivative measures are unique to individual changes within human brains.$$$To do so, we classify structural connectome pairs from two large longitudinal datasets as either belonging to the same individual or not.$$$Our data is comprised of 227 individuals from the Alzheimer's Disease Neuroimaging Initiative (ADNI) and 226 from the Parkinson's Progression Markers Initiative (PPMI).$$$We achieve 0.99 area under the ROC curve score for features which represent either weights or network structure of the connectomes (node degrees, PageRank and local efficiency).$$$Our approach may be useful for eliminating noisy features as a preprocessing step in brain aging studies and early diagnosis classification problems.",OBJECTIVES METHODS METHODS RESULTS CONCLUSIONS
D00080,"Artificial intelligence and machine learning have been major research interests in computer science for the better part of the last few decades.$$$However, all too recently, both AI and ML have rapidly grown to be media frenzies, pressuring companies and researchers to claim they use these technologies.$$$As ML continues to percolate into daily life, we, as computer scientists and machine learning researchers, are responsible for ensuring we clearly convey the extent of our work and the humanity of our models.$$$Regularizing ML for mass adoption requires a rigorous standard for model interpretability, a deep consideration for human bias in data, and a transparent understanding of a model's societal effects.",BACKGROUND BACKGROUND OBJECTIVES RESULTS
D06360,"Moments capture a huge part of our lives.$$$Accurate recognition of these moments is challenging due to the diverse and complex interpretation of the moments.$$$Action recognition refers to the act of classifying the desired action/activity present in a given video.$$$In this work, we perform experiments on Moments in Time dataset to recognize accurately activities occurring in 3 second clips.$$$We use state of the art techniques for visual, auditory and spatio temporal localization and develop method to accurately classify the activity in the Moments in Time dataset.$$$Our novel approach of using Visual Based Textual features and fusion techniques performs well providing an overall 89.23 % Top - 5 accuracy on the 20 classes - a significant improvement over the Baseline TRN model.",BACKGROUND BACKGROUND/OBJECTIVES BACKGROUND OBJECTIVES METHODS RESULTS
D04954,"In January 2015 we distributed an online survey about failures in robotics and intelligent systems across robotics researchers.$$$The aim of this survey was to find out which types of failures currently exist, what their origins are, and how systems are monitored and debugged - with a special focus on performance bugs.$$$This report summarizes the findings of the survey.",BACKGROUND OBJECTIVES CONCLUSIONS
D03464,"Faster R-CNN is one of the most representative and successful methods for object detection, and has been becoming increasingly popular in various objection detection applications.$$$In this report, we propose a robust deep face detection approach based on Faster R-CNN.$$$In our approach, we exploit several new techniques including new multi-task loss function design, online hard example mining, and multi-scale training strategy to improve Faster R-CNN in multiple aspects.$$$The proposed approach is well suited for face detection, so we call it Face R-CNN.$$$Extensive experiments are conducted on two most popular and challenging face detection benchmarks, FDDB and WIDER FACE, to demonstrate the superiority of the proposed approach over state-of-the-arts.",BACKGROUND OBJECTIVES METHODS METHODS RESULTS
D02064,"Blind quantum computation protocols allow a user with limited quantum technology to delegate an intractable computation to a quantum server while keeping the computation perfectly secret.$$$Whereas in some protocols a user can verify that calculated outcomes are correct, a third party cannot do this, which allows a dishonest user or owner to benefit illegally.$$$I propose a new blind quantum computation protocol with a new property called public verifiability, which enables any third party to assure that a party does not benefit from attempted deception.",BACKGROUND OBJECTIVES METHODS/RESULTS/CONCLUSIONS
D04878,"How common is self-citation in scholarly publication, and does the practice vary by gender?$$$Using novel methods and a data set of 1.5 million research papers in the scholarly database JSTOR published between 1779 and 2011, the authors find that nearly 10 percent of references are self-citations by a paper's authors.$$$The findings also show that between 1779 and 2011, men cited their own papers 56 percent more than did women.$$$In the last two decades of data, men self-cited 70 percent more than women.$$$Women are also more than 10 percentage points more likely than men to not cite their own previous work at all.$$$While these patterns could result from differences in the number of papers that men and women authors have published rather than gender-specific patterns of self-citation behavior, this gender gap in self-citation rates has remained stable over the last 50 years, despite increased representation of women in academia.$$$The authors break down self-citation patterns by academic field and number of authors and comment on potential mechanisms behind these observations.$$$These findings have important implications for scholarly visibility and cumulative advantage in academic careers.",OBJECTIVES METHODS/RESULTS RESULTS RESULTS RESULTS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS CONCLUSIONS
D06370,"Expressive efficiency refers to the relation between two architectures A and B, whereby any function realized by B could be replicated by A, but there exists functions realized by A, which cannot be replicated by B unless its size grows significantly larger.$$$For example, it is known that deep networks are exponentially efficient with respect to shallow networks, in the sense that a shallow network must grow exponentially large in order to approximate the functions represented by a deep network of polynomial size.$$$In this work, we extend the study of expressive efficiency to the attribute of network connectivity and in particular to the effect of ""overlaps"" in the convolutional process, i.e., when the stride of the convolution is smaller than its filter size (receptive field).$$$To theoretically analyze this aspect of network's design, we focus on a well-established surrogate for ConvNets called Convolutional Arithmetic Circuits (ConvACs), and then demonstrate empirically that our results hold for standard ConvNets as well.$$$Specifically, our analysis shows that having overlapping local receptive fields, and more broadly denser connectivity, results in an exponential increase in the expressive capacity of neural networks.$$$Moreover, while denser connectivity can increase the expressive capacity, we show that the most common types of modern architectures already exhibit exponential increase in expressivity, without relying on fully-connected layers.",BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS
D03934,"Machine Learning models are vulnerable to adversarial attacks that rely on perturbing the input data.$$$This work proposes a novel strategy using Autoencoder Deep Neural Networks to defend a machine learning model against two gradient-based attacks: The Fast Gradient Sign attack and Fast Gradient attack.$$$First we use an autoencoder to denoise the test data, which is trained with both clean and corrupted data.$$$Then, we reduce the dimension of the denoised data using the hidden layer representation of another autoencoder.$$$We perform this experiment for multiple values of the bound of adversarial perturbations, and consider different numbers of reduced dimensions.$$$When the test data is preprocessed using this cascaded pipeline, the tested deep neural network classifier yields a much higher accuracy, thus mitigating the effect of the adversarial perturbation.",BACKGROUND OBJECTIVES METHODS METHODS METHODS RESULTS/CONCLUSIONS
D05508,"Though the ability of human beings to deal with probabilities has been put into question, the assessment of rarity is a crucial competence underlying much of human decision-making and is pervasive in spontaneous narrative behaviour.$$$This paper proposes a new model of rarity and randomness assessment, designed to be cognitively plausible.$$$Intuitive randomness is defined as a function of structural complexity.$$$It is thus possible to assign probability to events without being obliged to consider the set of alternatives.$$$The model is tested on Lottery sequences and compared with subjects' preferences.",BACKGROUND OBJECTIVES OBJECTIVES RESULTS METHODS
D06464,"The time domain inter-cell interference coordination techniques specified in LTE Rel.$$$10 standard improves the throughput of picocell-edge users by protecting them from macrocell interference.$$$On the other hand, it also degrades the aggregate capacity in macrocell because the macro base station (MBS) does not transmit data during certain subframes known as almost blank subframes.$$$The MBS data transmission using reduced power subframes was standardized in LTE Rel.$$$11, which can improve the capacity in macrocell while not causing high interference to the nearby picocells.$$$In order to get maximum benefit from the reduced power subframes, setting the key system parameters, such as the amount of power reduction, carries critical importance.$$$Using stochastic geometry, this paper lays down a theoretical foundation for the performance evaluation of heterogeneous networks with reduced power subframes and range expansion bias.$$$The analytic expressions for average capacity and 5th percentile throughput are derived as a function of transmit powers, node densities, and interference coordination parameters in a heterogeneous network scenario, and are validated through Monte Carlo simulations.$$$Joint optimization of range expansion bias, power reduction factor, scheduling thresholds, and duty cycle of reduced power subframes are performed to study the trade-offs between aggregate capacity of a cell and fairness among the users.$$$To validate our analysis, we also compare the stochastic geometry based theoretical results with the real MBS deployment (in the city of London) and the hexagonal-grid model.$$$Our analysis shows that with optimum parameter settings, the LTE Rel.$$$11 with reduced power subframes can provide substantially better performance than the LTE Rel.$$$10 with almost blank subframes, in terms of both aggregate capacity and fairness.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS METHODS/RESULTS CONCLUSIONS CONCLUSIONS CONCLUSIONS
D03173,"The increasing number of applications requiring the solution of large scale singular value problems have rekindled interest in iterative methods for the SVD.$$$Some promising recent ad- vances in large scale iterative methods are still plagued by slow convergence and accuracy limitations for computing smallest singular triplets.$$$Furthermore, their current implementations in MATLAB cannot address the required large problems.$$$Recently, we presented a preconditioned, two-stage method to effectively and accurately compute a small number of extreme singular triplets.$$$In this research, we present a high-performance software, PRIMME SVDS, that implements our hybrid method based on the state-of-the-art eigensolver package PRIMME for both largest and smallest singular values.$$$PRIMME SVDS fills a gap in production level software for computing the partial SVD, especially with preconditioning.$$$The numerical experiments demonstrate its superior performance compared to other state-of-the-art software and its good parallel performance under strong and weak scaling.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES BACKGROUND RESULTS CONCLUSIONS
D01895,"Heterogeneous information networks (HINs) are ubiquitous in real-world applications.$$$Due to the heterogeneity in HINs, the typed edges may not fully align with each other.$$$In order to capture the semantic subtlety, we propose the concept of aspects with each aspect being a unit representing one underlying semantic facet.$$$Meanwhile, network embedding has emerged as a powerful method for learning network representation, where the learned embedding can be used as features in various downstream applications.$$$Therefore, we are motivated to propose a novel embedding learning framework---AspEm---to preserve the semantic information in HINs based on multiple aspects.$$$Instead of preserving information of the network in one semantic space, AspEm encapsulates information regarding each aspect individually.$$$In order to select aspects for embedding purpose, we further devise a solution for AspEm based on dataset-wide statistics.$$$To corroborate the efficacy of AspEm, we conducted experiments on two real-words datasets with two types of applications---classification and link prediction.$$$Experiment results demonstrate that AspEm can outperform baseline network embedding learning methods by considering multiple aspects, where the aspects can be selected from the given HIN in an unsupervised manner.",BACKGROUND BACKGROUND OBJECTIVES/METHODS BACKGROUND OBJECTIVES/METHODS METHODS OBJECTIVES/METHODS METHODS CONCLUSIONS
D02207,"An open concept of rough evolution and an axiomatic approach to granules was also developed recently by the present author.$$$Subsequently the concepts were used in the formal framework of rough Y-systems (RYS) for developing on granular correspondences by her.$$$These have since been used for a new approach towards comparison of rough algebraic semantics across different semantic domains by way of correspondences that preserve rough evolution and try to avoid contamination.$$$In this research paper, new methods are proposed and a semantics for handling possibly contaminated operations and structured bigness is developed.$$$These would also be of natural interest for relative consistency of one collection of knowledge relative other.",BACKGROUND BACKGROUND METHODS RESULTS CONCLUSIONS
D00995,"We show that there exists an inherent tension between the goal of adversarial robustness and that of standard generalization.$$$Specifically, training robust models may not only be more resource-consuming, but also lead to a reduction of standard accuracy.$$$We demonstrate that this trade-off between the standard accuracy of a model and its robustness to adversarial perturbations provably exists even in a fairly simple and natural setting.$$$These findings also corroborate a similar phenomenon observed in practice.$$$Further, we argue that this phenomenon is a consequence of robust classifiers learning fundamentally different feature representations than standard classifiers.$$$These differences, in particular, seem to result in unexpected benefits: the representations learned by robust models tend to align better with salient data characteristics and human perception.",CONCLUSIONS CONCLUSIONS RESULTS OTHERS METHODS RESULTS
D01802,We reveal a complete set of constraints that need to be imposed on a set of 3-by-3 matrices to ensure that the matrices represent genuine homographies associated with multiple planes between two views.$$$We also show how to exploit the constraints to obtain more accurate estimates of homography matrices between two views.$$$Our study resolves a long-standing research question and provides a fresh perspective and a more in-depth understanding of the multiple homography estimation task.,OBJECTIVES RESULTS CONCLUSIONS
D06849,"This study provides a comprehensive test of a head-related impulse response (HRIR) cues for a spatial auditory brain-computer interface (saBCI) speller paradigm.$$$We present a comparison with the conventional virtual sound headphone-based spatial auditory modality.$$$We propose and optimize the three types of sound spatialization settings using a variable elevation in order to evaluate the HRIR efficacy for the saBCI.$$$Three experienced and seven naive BCI users participated in the three experimental setups based on ten presented Japanese syllables.$$$The obtained EEG auditory evoked potentials (AEP) resulted with encouragingly good and stable P300 responses in online BCI experiments.$$$Our case study indicated that users could perceive elevation in the saBCI experiments generated using the HRIR measured from a general head model.$$$The saBCI accuracy and information transfer rate (ITR) scores have been improved comparing to the classical horizontal plane-based virtual spatial sound reproduction modality, as far as the healthy users in the current pilot study are concerned.",OBJECTIVES METHODS METHODS METHODS RESULTS CONCLUSIONS CONCLUSIONS
D01453,"We survey research that studies the connection between the computational complexity of optimization problems on the one hand, and the duality gap between the primal and dual optimization problems on the other.$$$To our knowledge, this is the first survey that connects the two very important areas.$$$We further look at a similar phenomenon in finite model theory relating to complexity and optimization.",BACKGROUND BACKGROUND OBJECTIVES
D01587,"Today computers have become an integral part of life.$$$However, most people's interaction with computers in on end-user-level.$$$Computer engineers are needed while designing and developing a structure of computer systems, software and hardware systems and also they need when implementing and solving problems while using these systems.$$$Training of qualified computer engineers is vital to have a say in future technology.$$$Recently, big data analysis, cloud technologies, wearable technologies, mobile and online services become popular.$$$For that reason, computer engineering education should update itself regularly and keep up with the latest improvements.$$$In this study, it is touched on some topics which are suggested to extend computer engineering curricula such as big data analyses, wearable technologies internet of things, cloud technologies, identity management and cyber security which are expected to widening in the area and also demanded that computer engineering student should be qualified on.$$$Related topics will be described and usage areas will be explained, developments and future roles will be mentioned and also expected achievements will be described.$$$These achievement's relevance with learning outcomes of departments which are accredited by MUDEK will be defined.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND/OBJECTIVES BACKGROUND/OBJECTIVES OBJECTIVES METHODS CONCLUSIONS CONCLUSIONS
D04014,"The area of Handwritten Signature Verification has been broadly researched in the last decades, but remains an open research problem.$$$The objective of signature verification systems is to discriminate if a given signature is genuine (produced by the claimed individual), or a forgery (produced by an impostor).$$$This has demonstrated to be a challenging task, in particular in the offline (static) scenario, that uses images of scanned signatures, where the dynamic information about the signing process is not available.$$$Many advancements have been proposed in the literature in the last 5-10 years, most notably the application of Deep Learning methods to learn feature representations from signature images.$$$In this paper, we present how the problem has been handled in the past few decades, analyze the recent advancements in the field, and the potential directions for future research.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND/OBJECTIVES METHODS/RESULTS/CONCLUSIONS
D00458,"This work deals with a classic problem: ""Given a set of coins among which there is a counterfeit coin of a different weight, find this counterfeit coin using ordinary balance scales, with the minimum number of weighings possible, and indicate whether it weighs less or more than the rest"".$$$The method proposed here not only calculates the minimum number of weighings necessary, but also indicates how to perform these weighings, it is easily mechanizeable and valid for any number of coins.$$$Instructions are also given as to how to generalize the procedure to include cases where there is more than one counterfeit coin.",OBJECTIVES OBJECTIVES OBJECTIVES
D01452,"In this work, we consider a two-level hierarchical MIMO antenna array system, where each antenna of the upper level is made up of a subarray on the lower one.$$$The concept of spatial multiplexing is applied twice in this situation: Firstly, the spatial multiplexing of a Line-of-Sight (LoS) MIMO system is exploited.$$$It is based on appropriate (sub-)array distances and achieves multiplexing gain due to phase differences among the signals at the receive (sub-)arrays.$$$Secondly, one or more additional reflected paths of different angles (separated from the LoS path by different spatial beams at the subarrays) are used to exploit spatial multiplexing between paths.$$$By exploiting the above two multiplexing kinds simultaneously, a high dimensional system with maximum spatial multiplexing is proposed by jointly using 'phase differences' within paths and 'angular differences' between paths.$$$The system includes an advanced hybrid beamforming architecture with large subarray separation, which could occur in millimeter wave backhaul scenarios.$$$The possible gains of the system w.r.t. a pure LOS MIMO system are illustrated by evaluating the capacities with total transmit power constraints.",OBJECTIVES BACKGROUND BACKGROUND BACKGROUND METHODS METHODS OTHERS
D05965,The X-problem of number 3 for one dimension and related observations are discussed,BACKGROUND/OBJECTIVES/METHODS/RESULTS/CONCLUSIONS
D05369,"Convolution Neural Networks, known as ConvNets exceptionally perform well in many complex machine learning tasks.$$$The architecture of ConvNets demands the huge and rich amount of data and involves with a vast number of parameters that leads the learning takes to be computationally expensive, slow convergence towards the global minima, trap in local minima with poor predictions.$$$In some cases, architecture overfits the data and make the architecture difficult to generalise for new samples that were not in the training set samples.$$$To address these limitations, many regularization and optimization strategies are developed for the past few years.$$$Also, studies suggested that these techniques significantly increase the performance of the networks as well as reducing the computational cost.$$$In implementing these techniques, one must thoroughly understand the theoretical concept of how this technique works in increasing the expressive power of the networks.$$$This article is intended to provide the theoretical concepts and mathematical formulation of the most commonly used strategies in developing a ConvNet architecture.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND/OBJECTIVES BACKGROUND OBJECTIVES OBJECTIVES
D00438,"We propose an efficient nonparametric strategy for learning a message operator in expectation propagation (EP), which takes as input the set of incoming messages to a factor node, and produces an outgoing message as output.$$$This learned operator replaces the multivariate integral required in classical EP, which may not have an analytic expression.$$$We use kernel-based regression, which is trained on a set of probability distributions representing the incoming messages, and the associated outgoing messages.$$$The kernel approach has two main advantages: first, it is fast, as it is implemented using a novel two-layer random feature representation of the input message distributions; second, it has principled uncertainty estimates, and can be cheaply updated online, meaning it can request and incorporate new training data when it encounters inputs on which it is uncertain.$$$In experiments, our approach is able to solve learning problems where a single message operator is required for multiple, substantially different data sets (logistic regression for a variety of classification problems), where it is essential to accurately assess uncertainty and to efficiently and robustly update the message operator.",OBJECTIVES METHODS METHODS METHODS RESULTS
D05947,"Bipartite networks are currently regarded as providing a major insight into the organization of many real-world systems, unveiling the mechanisms driving the interactions occurring between distinct groups of nodes.$$$One of the most important issues encountered when modeling bipartite networks is devising a way to obtain a (monopartite) projection on the layer of interest, which preserves as much as possible the information encoded into the original bipartite structure.$$$In the present paper we propose an algorithm to obtain statistically-validated projections of bipartite networks, according to which any two nodes sharing a statistically-significant number of neighbors are linked.$$$Since assessing the statistical significance of nodes similarity requires a proper statistical benchmark, here we consider a set of four null models, defined within the exponential random graph framework.$$$Our algorithm outputs a matrix of link-specific p-values, from which a validated projection is straightforwardly obtainable, upon running a multiple hypothesis testing procedure.$$$Finally, we test our method on an economic network (i.e. the countries-products World Trade Web representation) and a social network (i.e.$$$MovieLens, collecting the users' ratings of a list of movies).$$$In both cases non-trivial communities are detected: while projecting the World Trade Web on the countries layer reveals modules of similarly-industrialized nations, projecting it on the products layer allows communities characterized by an increasing level of complexity to be detected; in the second case, projecting MovieLens on the films layer allows clusters of movies whose affinity cannot be fully accounted for by genre similarity to be individuated.",BACKGROUND BACKGROUND OBJECTIVES/METHODS OBJECTIVES/METHODS METHODS RESULTS RESULTS RESULTS/CONCLUSIONS
D05013,"This short text summarizes the work in biology proposed in our book, Perspectives on Organisms, where we analyse the unity proper to organisms by looking at it from different viewpoints.$$$We discuss the theoretical roles of biological time, complexity, theoretical symmetries, singularities and critical transitions.$$$We explicitly borrow from the conclusions in some key chapters and introduce them by a reflection on ""incompleteness"", also proposed in the book.$$$We consider that incompleteness is a fundamental notion to understand the way in which we construct knowledge.$$$Then we will introduce an approach to biological dynamics where randomness is central to the theoretical determination: randomness does not oppose biological stability but contributes to it by variability, adaptation, and diversity.$$$Then, evolutionary and ontogenetic trajectories are continual changes of coherence structures involving symmetry changes within an ever-changing global stability.",OBJECTIVES OBJECTIVES OBJECTIVES/METHODS METHODS RESULTS RESULTS
D01972,We describe an XML file format for storing data from computations in algebra and geometry.$$$We also present a formal specification based on a RELAX-NG schema.,OBJECTIVES/RESULTS OBJECTIVES/RESULTS
D05027,"Unpaired Image-to-Image translation aims to convert the image from one domain (input domain A) to another domain (target domain B), without providing paired examples for the training.$$$The state-of-the-art, Cycle-GAN demonstrated the power of Generative Adversarial Networks with Cycle-Consistency Loss.$$$While its results are promising, there is scope for optimization in the training process.$$$This paper introduces a new neural network architecture, which only learns the translation from domain A to B and eliminates the need for reverse mapping (B to A), by introducing a new Deviation-loss term.$$$Furthermore, few other improvements to the Cycle-GAN are found and utilized in this new architecture, contributing to significantly lesser training duration.",BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS
D02717,"This paper gives sufficient conditions for a class of bang-bang extremals with multiple switches to be locally optimal in the strong topology.$$$The conditions are the natural generalizations of the ones considered in previous papers for more specific cases.$$$We require both the strict bang-bang Legendre condition, and the second order conditions for the finite dimensional problem obtained by moving the switching times of the reference trajectory.",RESULTS METHODS METHODS
D00470,"The back-propagation algorithm is widely used for learning in artificial neural networks.$$$A challenge in machine learning is to create models that generalize to new data samples not seen in the training data.$$$Recently, a common flaw in several machine learning algorithms was discovered: small perturbations added to the input data lead to consistent misclassification of data samples.$$$Samples that easily mislead the model are called adversarial examples.$$$Training a ""maxout"" network on adversarial examples has shown to decrease this vulnerability, but also increase classification performance.$$$This paper shows that adversarial training has a regularizing effect also in networks with logistic, hyperbolic tangent and rectified linear units.$$$A simple extension to the back-propagation method is proposed, that adds an adversarial gradient to the training.$$$The extension requires an additional forward and backward pass to calculate a modified input sample, or mini batch, used as input for standard back-propagation learning.$$$The first experimental results on MNIST show that the ""adversarial back-propagation"" method increases the resistance to adversarial examples and boosts the classification performance.$$$The extension reduces the classification error on the permutation invariant MNIST from 1.60% to 0.95% in a logistic network, and from 1.40% to 0.78% in a network with rectified linear units.$$$Results on CIFAR-10 indicate that the method has a regularizing effect similar to dropout in fully connected networks.$$$Based on these promising results, adversarial back-propagation is proposed as a stand-alone regularizing method that should be further investigated.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND RESULTS METHODS METHODS RESULTS RESULTS RESULTS CONCLUSIONS
D00504,"A scientist may publish tens or hundreds of papers over a career, but these contributions are not evenly spaced in time.$$$Sixty years of studies on career productivity patterns in a variety of fields suggest an intuitive and universal pattern: productivity tends to rise rapidly to an early peak and then gradually declines.$$$Here, we test the universality of this conventional narrative by analyzing the structures of individual faculty productivity time series, constructed from over 200,000 publications and matched with hiring data for 2453 tenure-track faculty in all 205 Ph.D-granting computer science departments in the U.S. and Canada.$$$Unlike prior studies, which considered only some faculty or some institutions, or lacked common career reference points, here we combine a large bibliographic dataset with comprehensive information on career transitions that covers an entire field of study.$$$We show that the conventional narrative confidently describes only one fifth of faculty, regardless of department prestige or researcher gender, and the remaining four fifths of faculty exhibit a rich diversity of productivity patterns.$$$To explain this diversity, we introduce a simple model of productivity trajectories, and explore correlations between its parameters and researcher covariates, showing that departmental prestige predicts overall individual productivity and the timing of the transition from first- to last-author publications.$$$These results demonstrate the unpredictability of productivity over time, and open the door for new efforts to understand how environmental and individual factors shape scientific productivity.",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES/METHODS RESULTS METHODS/RESULTS CONCLUSIONS
D01048,"We propose and study a novel continuous space-time model for wireless networks which takes into account the stochastic interactions in both space through interference and in time due to randomness in traffic.$$$Our model consists of an interacting particle birth-death dynamics incorporating information-theoretic spectrum sharing.$$$Roughly speaking, particles (or more generally wireless links) arrive according to a Poisson Point Process on space-time, and stay for a duration governed by the local configuration of points present and then exit the network after completion of a file transfer.$$$We analyze this particle dynamics to derive an explicit condition for time ergodicity (i.e. stability) which is tight.$$$We also prove that when the dynamics is ergodic, the steady-state point process of links (or particles) exhibits a form statistical clustering.$$$Based on the clustering, we propose a conjecture which we leverage to derive approximations, bounds and asymptotics on performance characteristics such as delay and mean number of links per unit-space in the stationary regime.$$$The mathematical analysis is combined with discrete event simulation to study the performance of this type of networks.",OBJECTIVES METHODS METHODS RESULTS RESULTS CONCLUSIONS METHODS
D03361,"Model transformations are the cornerstone of Model-Driven Engineering, and provide the essential mechanisms for manipulating and transforming models.$$$Checking whether the output of a model transformation is correct is a manual and error-prone task, this is referred to as the oracle problem in the software testing literature.$$$The correctness of the model transformation program is crucial for the proper generation of its output, so it should be tested.$$$Metamorphic testing is a testing technique to alleviate the oracle problem consisting on exploiting the relations between different inputs and outputs of the program under test, so-called metamorphic relations.$$$In this paper we give an insight into our approach to generically define metamorphic relations for model transformations, which can be automatically instantiated given any specific model transformation.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES
D05940,"In the face of scarcity in detailed training annotations, the ability to perform object localization tasks in real-time with weak-supervision is very valuable.$$$However, the computational cost of generating and evaluating region proposals is heavy.$$$We adapt the concept of Class Activation Maps (CAM) into the very first weakly-supervised 'single-shot' detector that does not require the use of region proposals.$$$To facilitate this, we propose a novel global pooling technique called Spatial Pyramid Averaged Max (SPAM) pooling for training this CAM-based network for object extent localisation with only weak image-level supervision.$$$We show this global pooling layer possesses a near ideal flow of gradients for extent localization, that offers a good trade-off between the extremes of max and average pooling.$$$Our approach only requires a single network pass and uses a fast-backprojection technique, completely omitting any region proposal steps.$$$To the best of our knowledge, this is the first approach to do so.$$$Due to this, we are able to perform inference in real-time at 35fps, which is an order of magnitude faster than all previous weakly supervised object localization frameworks.",BACKGROUND BACKGROUND OBJECTIVES/METHODS METHODS RESULTS/CONCLUSIONS RESULTS RESULTS/CONCLUSIONS RESULTS
D03872,"Big Data concern large-volume, growing data sets that are complex and have multiple autonomous sources.$$$Earlier technologies were not able to handle storage and processing of huge data thus Big Data concept comes into existence.$$$This is a tedious job for users unstructured data.$$$So, there should be some mechanism which classify unstructured data into organized form which helps user to easily access required data.$$$Classification techniques over big transactional database provide required data to the users from large datasets more simple way.$$$There are two main classification techniques, supervised and unsupervised.$$$In this paper we focused on to study of different supervised classification techniques.$$$Further this paper shows a advantages and limitations.",BACKGROUND OBJECTIVES BACKGROUND OBJECTIVES METHODS METHODS OBJECTIVES RESULTS
D03930,"In a previous paper with Adam Brandenburger, we used sheaf theory to analyze the structure of non-locality and contextuality.$$$Moreover, on the basis of this formulation, we showed that the phenomena of non-locality and contextuality can be characterized precisely in terms of obstructions to the existence of global sections.$$$Our aim in the present work is to build on these results, and to use the powerful tools of sheaf cohomology to study the structure of non-locality and contextuality.$$$We use the Cech cohomology on an abelian presheaf derived from the support of a probabilistic model, viewed as a compatible family of distributions, in order to define a cohomological obstruction for the family as a certain cohomology class.$$$This class vanishes if the family has a global section.$$$Thus the non-vanishing of the obstruction provides a sufficient (but not necessary) condition for the model to be contextual.$$$We show that for a number of salient examples, including PR boxes, GHZ states, the Peres-Mermin magic square, and the 18-vector configuration due to Cabello et al. giving a proof of the Kochen-Specker theorem in four dimensions, the obstruction does not vanish, thus yielding cohomological witnesses for contextuality.",BACKGROUND BACKGROUND OBJECTIVES/METHODS METHODS RESULTS RESULTS RESULTS
D06101,"Random fields are useful mathematical objects in the characterization of non-deterministic complex systems.$$$A fundamental issue in the evolution of dynamical systems is how intrinsic properties of such structures change in time.$$$In this paper, we propose to quantify how changes in the spatial dependence structure affect the Riemannian metric tensor that equips the model's parametric space.$$$Defining Fisher curves, we measure the variations in each component of the metric tensor when visiting different entropic states of the system.$$$Simulations show that the geometric deformations induced by the metric tensor in case of a decrease in the inverse temperature are not reversible for an increase of the same amount, provided there is significant variation in the system entropy: the process of taking a system from a lower entropy state A to a higher entropy state B and then bringing it back to A, induces a natural intrinsic one-way direction of evolution.$$$In this context, Fisher curves resemble mathematical models of hysteresis in which the natural orientation is pointed by an arrow of time.",BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS CONCLUSIONS
D04765,"Subtraction-free computational complexity is the version of arithmetic circuit complexity that allows only three operations: addition, multiplication, and division.$$$We use cluster transformations to design efficient subtraction-free algorithms for computing Schur functions and their skew, double, and supersymmetric analogues, thereby generalizing earlier results by P. Koev.$$$We develop such algorithms for computing generating functions of spanning trees, both directed and undirected.$$$A comparison to the lower bound due to M. Jerrum and M. Snir shows that in subtraction-free computations, ""division can be exponentially powerful.""$$$Finally, we give a simple example where the gap between ordinary and subtraction-free complexity is exponential.",BACKGROUND METHODS/RESULTS RESULTS METHODS/RESULTS RESULTS
D05414,"Text segmentation, the task of dividing a document into contiguous segments based on its semantic structure, is a longstanding challenge in language understanding.$$$Previous work on text segmentation focused on unsupervised methods such as clustering or graph search, due to the paucity in labeled data.$$$In this work, we formulate text segmentation as a supervised learning problem, and present a large new dataset for text segmentation that is automatically extracted and labeled from Wikipedia.$$$Moreover, we develop a segmentation model based on this dataset and show that it generalizes well to unseen natural text.",BACKGROUND BACKGROUND METHODS RESULTS
D03616,"Any non-trivial concurrent system warrants synchronisation, regardless of the concurrency model.$$$Actor-based concurrency serialises all computations in an actor through asynchronous message passing.$$$In contrast, lock-based concurrency serialises some computations by following a lock--unlock protocol for accessing certain data.$$$Both systems require sound reasoning about pointers and aliasing to exclude data-races.$$$If actor isolation is broken, so is the single-thread-of-control abstraction.$$$Similarly for locks, if a datum is accessible outside of the scope of the lock, the datum is not governed by the lock.$$$In this paper we discuss how to balance aliasing and synchronisation.$$$In previous work, we defined a type system that guarantees data-race freedom of actor-based concurrency and lock-based concurrency.$$$This paper extends this work by the introduction of two programming constructs; one for decoupling isolation and synchronisation and one for constructing higher-level atomicity guarantees from lower-level synchronisation.$$$We focus predominantly on actors, and in particular the Encore programming language, but our ultimate goal is to define our constructs in such a way that they can be used both with locks and actors, given that combinations of both models occur frequently in actual systems.$$$We discuss the design space, provide several formalisations of different semantics and discuss their properties, and connect them to case studies showing how our proposed constructs can be useful.$$$We also report on an on-going implementation of our proposed constructs in Encore.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES BACKGROUND/METHODS OBJECTIVES/METHODS OBJECTIVES/METHODS METHODS/RESULTS OTHERS
D06157,"Sounds are essential to how humans perceive and interact with the world and are captured in recordings and shared on the Internet on a minute-by-minute basis.$$$These recordings, which are predominantly videos, constitute the largest archive of sounds we know.$$$However, most of these recordings have undescribed content making necessary methods for automatic sound analysis, indexing and retrieval.$$$These methods have to address multiple challenges, such as the relation between sounds and language, numerous and diverse sound classes, and large-scale evaluation.$$$We propose a system that continuously learns from the web relations between sounds and language, improves sound recognition models over time and evaluates its learning competency in the large-scale without references.$$$We introduce the Never-Ending Learner of Sounds (NELS), a project for continuously learning of sounds and their associated knowledge, available on line in nels.cs.cmu.edu",BACKGROUND OBJECTIVES BACKGROUND METHODS METHODS RESULTS/CONCLUSIONS
D00474,"Most of the mammal species hold polygynous mating systems.$$$The majority of the marriage systems of mankind were also polygynous over civilized history, however, socially imposed monogamy gradually prevails throughout the world.$$$This is difficult to understand because those mostly influential in society are themselves benefitted from polygyny.$$$Actually, the puzzle of monogamous marriage could be explained by a simple mechanism, which lies in the sexual selection dynamics of civilized human societies, driven by wealth redistribution.$$$The discussions in this paper are mainly based on the approach of social computing, with a combination of both experimental and analytical analysis.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/RESULTS METHODS
D06084,"We propose a method for visual question answering which combines an internal representation of the content of an image with information extracted from a general knowledge base to answer a broad range of image-based questions.$$$This allows more complex questions to be answered using the predominant neural network-based approach than has previously been possible.$$$It particularly allows questions to be asked about the contents of an image, even when the image itself does not contain the whole answer.$$$The method constructs a textual representation of the semantic content of an image, and merges it with textual information sourced from a knowledge base, to develop a deeper understanding of the scene viewed.$$$Priming a recurrent neural network with this combined information, and the submitted question, leads to a very flexible visual question answering approach.$$$We are specifically able to answer questions posed in natural language, that refer to information not contained in the image.$$$We demonstrate the effectiveness of our model on two publicly available datasets, Toronto COCO-QA and MS COCO-VQA and show that it produces the best reported results in both cases.",OBJECTIVES OBJECTIVES OBJECTIVES METHODS RESULTS RESULTS RESULTS
D02887,"Modal logics are widely used in computer science.$$$The complexity of their satisfiability problems has been an active field of research since the 1970s.$$$We prove that even very ""simple"" modal logics can be undecidable: We show that there is an undecidable modal logic that can be obtained by restricting the allowed models with a first-order formula in which only universal quantifiers appear.",BACKGROUND BACKGROUND RESULTS
D01555,"While both cost-sensitive learning and online learning have been studied extensively, the effort in simultaneously dealing with these two issues is limited.$$$Aiming at this challenge task, a novel learning framework is proposed in this paper.$$$The key idea is based on the fusion of online ensemble algorithms and the state of the art batch mode cost-sensitive bagging/boosting algorithms.$$$Within this framework, two separately developed research areas are bridged together, and a batch of theoretically sound online cost-sensitive bagging and online cost-sensitive boosting algorithms are first proposed.$$$Unlike other online cost-sensitive learning algorithms lacking theoretical analysis of asymptotic properties, the convergence of the proposed algorithms is guaranteed under certain conditions, and the experimental evidence with benchmark data sets also validates the effectiveness and efficiency of the proposed methods.",BACKGROUND OBJECTIVES METHODS OTHERS RESULTS
D04128,"In machine learning, novelty detection is the task of identifying novel unseen data.$$$During training, only samples from the normal class are available.$$$Test samples are classified as normal or abnormal by assignment of a novelty score.$$$Here we propose novelty detection methods based on training variational autoencoders (VAEs) on normal data.$$$Since abnormal samples are not used during training, we define novelty metrics based on the (partially complementary) assumptions that the VAE is less capable of reconstructing abnormal samples well; that abnormal samples more strongly violate the VAE regularizer; and that abnormal samples differ from normal samples not only in input-feature space, but also in the VAE latent space and VAE output.$$$These approaches, combined with various possibilities of using (e.g.$$$sampling) the probabilistic VAE to obtain scalar novelty scores, yield a large family of methods.$$$We apply these methods to magnetic resonance imaging, namely to the detection of diffusion-space (q-space) abnormalities in diffusion MRI scans of multiple sclerosis patients, i.e.$$$to detect multiple sclerosis lesions without using any lesion labels for training.$$$Many of our methods outperform previously proposed q-space novelty detection methods.$$$We also evaluate the proposed methods on the MNIST handwritten digits dataset and show that many of them are able to outperform the state of the art.",BACKGROUND BACKGROUND BACKGROUND METHODS OBJECTIVES OTHERS OBJECTIVES METHODS OBJECTIVES RESULTS RESULTS
D01941,"Deep generative architectures provide a way to model not only images, but also complex, 3-dimensional objects, such as point clouds.$$$In this work, we present a novel method to obtain meaningful representations of 3D shapes that can be used for clustering and reconstruction.$$$Contrary to existing methods for 3D point cloud generation that train separate decoupled models for representation learning and generation, our approach is the first end-to-end solution that allows to simultaneously learn a latent space of representation and generate 3D shape out of it.$$$To achieve this goal, we extend a deep Adversarial Autoencoder model (AAE) to accept 3D input and create 3D output.$$$Thanks to our end-to-end training regime, the resulting method called 3D Adversarial Autoencoder (3dAAE) obtains either binary or continuous latent space that covers much wider portion of training data distribution, hence allowing smooth interpolation between the shapes.$$$Finally, our extensive quantitative evaluation shows that 3dAAE provides state-of-the-art results on a set of benchmark tasks.",BACKGROUND OBJECTIVES OBJECTIVES/METHODS OBJECTIVES/METHODS RESULTS RESULTS
D06187,"Synthetic image translation has significant potentials in autonomous transportation systems.$$$That is due to the expense of data collection and annotation as well as the unmanageable diversity of real-words situations.$$$The main issue with unpaired image-to-image translation is the ill-posed nature of the problem.$$$In this work, we propose a novel method for constraining the output space of unpaired image-to-image translation.$$$We make the assumption that the environment of the source domain is known (e.g. synthetically generated), and we propose to explicitly enforce preservation of the ground-truth labels on the translated images.$$$We experiment on preserving ground-truth information such as semantic segmentation, disparity, and instance segmentation.$$$We show significant evidence that our method achieves improved performance over the state-of-the-art model of UNIT for translating images from SYNTHIA to Cityscapes.$$$The generated images are perceived as more realistic in human surveys and outperforms UNIT when used in a domain adaptation scenario for semantic segmentation.",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES METHODS METHODS RESULTS RESULTS
D05948,"The segmentation of liver lesions is crucial for detection, diagnosis and monitoring progression of liver cancer.$$$However, design of accurate automated methods remains challenging due to high noise in CT scans, low contrast between liver and lesions, as well as large lesion variability.$$$We propose a 3D automatic, unsupervised method for liver lesions segmentation using a phase separation approach.$$$It is assumed that liver is a mixture of two phases: healthy liver and lesions, represented by different image intensities polluted by noise.$$$The Cahn-Hilliard equation is used to remove the noise and separate the mixture into two distinct phases with well-defined interfaces.$$$This simplifies the lesion detection and segmentation task drastically and enables to segment liver lesions by thresholding the Cahn-Hilliard solution.$$$The method was tested on 3Dircadb and LITS dataset.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS RESULTS/CONCLUSIONS RESULTS
D05460,"Natural disasters are a large threat for people especially in developing countries such as Laos.$$$ICT-based disaster management systems aim at supporting disaster warning and response efforts.$$$However, the ability to directly communicate in both directions between local and administrative level is often not supported, and a tight integration into administrative workflows is missing.$$$In this paper, we present the smartphone-based disaster and reporting system Mobile4D.$$$It allows for bi-directional communication while being fully involved in administrative processes.$$$We present the system setup and discuss integration into administrative structures in Lao PDR.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS RESULTS
D06682,"We propose a DC proximal Newton algorithm for solving nonconvex regularized sparse learning problems in high dimensions.$$$Our proposed algorithm integrates the proximal Newton algorithm with multi-stage convex relaxation based on the difference of convex (DC) programming, and enjoys both strong computational and statistical guarantees.$$$Specifically, by leveraging a sophisticated characterization of sparse modeling structures/assumptions (i.e., local restricted strong convexity and Hessian smoothness), we prove that within each stage of convex relaxation, our proposed algorithm achieves (local) quadratic convergence, and eventually obtains a sparse approximate local optimum with optimal statistical properties after only a few convex relaxations.$$$Numerical experiments are provided to support our theory.",OBJECTIVES METHODS METHODS RESULTS
D06498,"We have developed a system that automatically detects online jihadist hate speech with over 80% accuracy, by using techniques from Natural Language Processing and Machine Learning.$$$The system is trained on a corpus of 45,000 subversive Twitter messages collected from October 2014 to December 2016.$$$We present a qualitative and quantitative analysis of the jihadist rhetoric in the corpus, examine the network of Twitter users, outline the technical procedure used to train the system, and discuss examples of use.",RESULTS METHODS OTHERS
D06151,"Phylogenetic networks are increasingly used in evolutionary biology to represent the history of species that have undergone reticulate events such as horizontal gene transfer, hybrid speciation and recombination.$$$One of the most fundamental questions that arise in this context is whether the evolution of a gene with one copy in all species can be explained by a given network.$$$In mathematical terms, this is often translated in the following way: is a given phylogenetic tree contained in a given phylogenetic network?$$$Recently this tree containment problem has been widely investigated from a computational perspective, but most studies have only focused on the topology of the phylo- genies, ignoring a piece of information that, in the case of phylogenetic trees, is routinely inferred by evolutionary analyses: branch lengths.$$$These measure the amount of change (e.g., nucleotide substitutions) that has occurred along each branch of the phylogeny.$$$Here, we study a number of versions of the tree containment problem that explicitly account for branch lengths.$$$We show that, although length information has the potential to locate more precisely a tree within a network, the problem is computationally hard in its most general form.$$$On a positive note, for a number of special cases of biological relevance, we provide algorithms that solve this problem efficiently.$$$This includes the case of networks of limited complexity, for which it is possible to recover, among the trees contained by the network with the same topology as the input tree, the closest one in terms of branch lengths.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES RESULTS RESULTS RESULTS
D02901,"Neural networks are becoming central in several areas of computer vision and image processing and different architectures have been proposed to solve specific problems.$$$The impact of the loss layer of neural networks, however, has not received much attention in the context of image processing: the default and virtually only choice is L2.$$$In this paper, we bring attention to alternative choices for image restoration.$$$In particular, we show the importance of perceptually-motivated losses when the resulting image is to be evaluated by a human observer.$$$We compare the performance of several losses, and propose a novel, differentiable error function.$$$We show that the quality of the results improves significantly with better loss functions, even when the network architecture is left unchanged.",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES METHODS RESULTS
D01202,"Makespan minimization in tasks scheduling of infrastructure as a service (IaaS) cloud is an NP-hard problem.$$$A number of techniques had been used in the past to optimize the makespan time of scheduled tasks in IaaS cloud, which is propotional to the execution cost billed to customers.$$$In this paper, we proposed a League Championship Algorithm (LCA) based makespan time minimization scheduling technique in IaaS cloud.$$$The LCA is a sports-inspired population based algorithmic framework for global optimization over a continuous search space.$$$Three other existing algorithms that is, First Come First Served (FCFS), Last Job First (LJF) and Best Effort First (BEF) were used to evaluate the performance of the proposed algorithm.$$$All algorithms under consideration assumed to be non-preemptive.$$$The results obtained shows that, the LCA scheduling technique perform moderately better than the other algorithms in minimizing the makespan time of scheduled tasks in IaaS cloud.",BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS RESULTS CONCLUSIONS
D05715,"Modern Graphics Processing Units (GPUs) are now considered accelerators for general purpose computation.$$$A tight interaction between the GPU and the interconnection network is the strategy to express the full potential on capability computing of a multi-GPU system on large HPC clusters; that is the reason why an efficient and scalable interconnect is a key technology to finally deliver GPUs for scientific HPC.$$$In this paper we show the latest architectural and performance improvement of the APEnet+ network fabric, a FPGA-based PCIe board with 6 fully bidirectional off-board links with 34 Gbps of raw bandwidth per direction, and X8 Gen2 bandwidth towards the host PC.$$$The board implements a Remote Direct Memory Access (RDMA) protocol that leverages upon peer-to-peer (P2P) capabilities of Fermi- and Kepler-class NVIDIA GPUs to obtain real zero-copy, low-latency GPU-to-GPU transfers.$$$Finally, we report on the development activities for 2013 focusing on the adoption of the latest generation 28 nm FPGAs and the preliminary tests performed on this new platform.",BACKGROUND BACKGROUND OBJECTIVES/METHODS METHODS RESULTS
D06270,"Starting from a heuristic learning scheme for N-person games, we derive a new class of continuous-time learning dynamics consisting of a replicator-like drift adjusted by a penalty term that renders the boundary of the game's strategy space repelling.$$$These penalty-regulated dynamics are equivalent to players keeping an exponentially discounted aggregate of their on-going payoffs and then using a smooth best response to pick an action based on these performance scores.$$$Owing to this inherent duality, the proposed dynamics satisfy a variant of the folk theorem of evolutionary game theory and they converge to (arbitrarily precise) approximations of Nash equilibria in potential games.$$$Motivated by applications to traffic engineering, we exploit this duality further to design a discrete-time, payoff-based learning algorithm which retains these convergence properties and only requires players to observe their in-game payoffs: moreover, the algorithm remains robust in the presence of stochastic perturbations and observation errors, and it does not require any synchronization between players.",METHODS RESULTS RESULTS CONCLUSIONS
D00536,"In this paper, we extend state of the art Model Predictive Control (MPC) approaches to generate safe bipedal walking on slippery surfaces.$$$In this setting, we formulate walking as a trade off between realizing a desired walking velocity and preserving robust foot-ground contact.$$$Exploiting this formulation inside MPC, we show that safe walking on various flat terrains can be achieved by compromising three main attributes, i. e. walking velocity tracking, the Zero Moment Point (ZMP) modulation, and the Required Coefficient of Friction (RCoF) regulation.$$$Simulation results show that increasing the walking velocity increases the possibility of slippage, while reducing the slippage possibility conflicts with reducing the tip-over possibility of the contact and vice versa.",OBJECTIVES METHODS METHODS/RESULTS RESULTS
D05690,"We survey more recent attempts at enumerating the number of mountain-valley assignments that allow a given crease pattern to locally fold flat.$$$In particular, we solve this problem for square twist tessellations and generalize the method used to a broader family of crease patterns.$$$We also describe the more difficult case of the Miura-ori and a recently-discovered bijection with 3-vertex colorings of grid graphs.",OBJECTIVES RESULTS RESULTS
D03734,"We present a transition-based dependency parser that uses a convolutional neural network to compose word representations from characters.$$$The character composition model shows great improvement over the word-lookup model, especially for parsing agglutinative languages.$$$These improvements are even better than using pre-trained word embeddings from extra data.$$$On the SPMRL data sets, our system outperforms the previous best greedy parser (Ballesteros et al., 2015) by a margin of 3% on average.",METHODS RESULTS/CONCLUSIONS RESULTS RESULTS
D05800,"Many real world tasks such as reasoning and physical interaction require identification and manipulation of conceptual entities.$$$A first step towards solving these tasks is the automated discovery of distributed symbol-like representations.$$$In this paper, we explicitly formalize this problem as inference in a spatial mixture model where each component is parametrized by a neural network.$$$Based on the Expectation Maximization framework we then derive a differentiable clustering method that simultaneously learns how to group and represent individual entities.$$$We evaluate our method on the (sequential) perceptual grouping task and find that it is able to accurately recover the constituent objects.$$$We demonstrate that the learned representations are useful for next-step prediction.",BACKGROUND OBJECTIVES OBJECTIVES/METHODS METHODS RESULTS RESULTS
D02875,"Organisations store huge amounts of data from multiple heterogeneous sources in the form of Knowledge Graphs (KGs).$$$One of the ways to query these KGs is to use SPARQL queries over a database engine.$$$Since SPARQL follows exact match semantics, the queries may return too few or no results.$$$Recent works have proposed query relaxation where the query engine judiciously replaces a query predicate with similar predicates using weighted relaxation rules mined from the KG.$$$The space of possible relaxations is potentially too large to fully explore and users are typically interested in only top-k results, so such query engines use top-k algorithms for query processing.$$$However, they may still process all the relaxations, many of whose answers do not contribute towards top-k answers.$$$This leads to computation overheads and delayed response times.$$$We propose Spec-QP, a query planning framework that speculatively determines which relaxations will have their results in the top-k answers.$$$Only these relaxations are processed using the top-k operators.$$$We, therefore, reduce the computation overheads and achieve faster response times without adversely affecting the quality of results.$$$We tested Spec-QP over two datasets - XKG and Twitter, to demonstrate the efficiency of our planning framework at reducing runtimes with reasonable accuracy for query engines supporting relaxations.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES OBJECTIVES METHODS METHODS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS
D05179,"The variance component tests used in genomewide association studies of thousands of individuals become computationally exhaustive when multiple traits are analysed in the context of omics studies.$$$We introduce two high-throughput algorithms -- CLAK-CHOL and CLAK-EIG -- for single and multiple phenotype genome-wide association studies (GWAS).$$$The algorithms, generated with the help of an expert system, reduce the computational complexity to the point that thousands of traits can be analyzed for association with millions of polymorphisms in a course of days on a standard workstation.$$$By taking advantage of problem specific knowledge, CLAK-CHOL and CLAK-EIG significantly outperform the current state-of-the-art tools in both single and multiple trait analysis.",BACKGROUND/OBJECTIVES METHODS/RESULTS RESULTS RESULTS/CONCLUSIONS
D03143,"A publication trend in Physics Education by employing bibliometric analysis leads the researchers to describe current scientific movement.$$$This paper tries to answer ""What do Physics education scientists concentrate in their publications?"" by analyzing the productivity and development of publications on the subject category of Physics Education in the period 1980--2013.$$$The Web of Science databases in the research areas of ""EDUCATION - EDUCATIONAL RESEARCH"" was used to extract the publication trends.$$$The study involves 1360 publications, including 840 articles, 503 proceedings paper, 22 reviews, 7 editorial material, 6 Book review, and one Biographical item.$$$Number of publications with ""Physical Education"" in topic increased from 0.14 % (n = 2) in 1980 to 16.54 % (n = 225) in 2011.$$$Total number of receiving citations is 8071, with approximately citations per papers of 5.93.$$$The results show the publication and citations in Physic Education has increased dramatically while the Malaysian share is well ranked.",OBJECTIVES/RESULTS/CONCLUSIONS OBJECTIVES/METHODS/CONCLUSIONS BACKGROUND/METHODS METHODS RESULTS RESULTS RESULTS/CONCLUSIONS
D03114,"Twitter is one of the most popular social media.$$$Due to the ease of availability of data, Twitter is used significantly for research purposes.$$$Twitter is known to evolve in many aspects from what it was at its birth; nevertheless, how it evolved its own linguistic style is still relatively unknown.$$$In this paper, we study the evolution of various sociolinguistic aspects of Twitter over large time scales.$$$To the best of our knowledge, this is the first comprehensive study on the evolution of such aspects of this OSN.$$$We performed quantitative analysis both on the word level as well as on the hashtags since it is perhaps one of the most important linguistic units of this social media.$$$We studied the (in)formality aspects of the linguistic styles in Twitter and find that it is neither fully formal nor completely informal; while on one hand, we observe that Out-Of-Vocabulary words are decreasing over time (pointing to a formal style), on the other hand it is quite evident that whitespace usage is getting reduced with a huge prevalence of running texts (pointing to an informal style).$$$We also analyze and propose quantitative reasons for repetition and coalescing of hashtags in Twitter.$$$We believe that such phenomena may be strongly tied to different evolutionary aspects of human languages.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES OBJECTIVES/METHODS METHODS/RESULTS/CONCLUSIONS RESULTS OTHERS
D03807,"Semantic segmentation and object detection research have recently achieved rapid progress.$$$However, the former task has no notion of different instances of the same object, and the latter operates at a coarse, bounding-box level.$$$We propose an Instance Segmentation system that produces a segmentation map where each pixel is assigned an object class and instance identity label.$$$Most approaches adapt object detectors to produce segments instead of boxes.$$$In contrast, our method is based on an initial semantic segmentation module, which feeds into an instance subnetwork.$$$This subnetwork uses the initial category-level segmentation, along with cues from the output of an object detector, within an end-to-end CRF to predict instances.$$$This part of our model is dynamically instantiated to produce a variable number of instances per image.$$$Our end-to-end approach requires no post-processing and considers the image holistically, instead of processing independent proposals.$$$Therefore, unlike some related work, a pixel cannot belong to multiple instances.$$$Furthermore, far more precise segmentations are achieved, as shown by our state-of-the-art results (particularly at high IoU thresholds) on the Pascal VOC and Cityscapes datasets.",BACKGROUND BACKGROUND OBJECTIVES BACKGROUND METHODS METHODS METHODS METHODS OBJECTIVES/METHODS RESULTS
D05039,"We have formerly introduced Deep Random Secrecy, a new cryptologic technique capable to ensure secrecy as close as desired from perfection against unlimited passive eavesdropping opponents.$$$We have also formerly introduced an extended protocol, based on Deep Random Secrecy, capable to resist to unlimited active MITM.$$$The main limitation of those protocols, in their initial presented version, is the important quantity of information that needs to be exchanged between the legitimate partners to distill secure digits.$$$We have defined and shown existence of an absolute constant, called Cryptologic Limit, which represents the upper-bound of Secrecy rate that can be reached by Deep Random Secrecy protocols.$$$At last, we have already presented practical algorithms to generate Deep Randomness from classical computing resources.$$$This article is presenting an optimization technique, based on recombination and reuse of random bits; this technique enables to dramatically increase the bandwidth performance of formerly introduced protocols, without jeopardizing the entropy of secret information.$$$That optimization enables to envision an implementation of Deep Random Secrecy at very reasonable cost.$$$The article also summarizes former results in the perspective of a comprehensive implementation.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND METHODS OBJECTIVES OBJECTIVES
D04054,"Structured prediction energy networks (SPENs; Belanger & McCallum 2016) use neural network architectures to define energy functions that can capture arbitrary dependencies among parts of structured outputs.$$$Prior work used gradient descent for inference, relaxing the structured output to a set of continuous variables and then optimizing the energy with respect to them.$$$We replace this use of gradient descent with a neural network trained to approximate structured argmax inference.$$$This ""inference network"" outputs continuous values that we treat as the output structure.$$$We develop large-margin training criteria for joint training of the structured energy function and inference network.$$$On multi-label classification we report speed-ups of 10-60x compared to (Belanger et al, 2017) while also improving accuracy.$$$For sequence labeling with simple structured energies, our approach performs comparably to exact inference while being much faster at test time.$$$We then demonstrate improved accuracy by augmenting the energy with a ""label language model"" that scores entire output label sequences, showing it can improve handling of long-distance dependencies in part-of-speech tagging.$$$Finally, we show how inference networks can replace dynamic programming for test-time inference in conditional random fields, suggestive for their general use for fast inference in structured settings.",BACKGROUND BACKGROUND OBJECTIVES/METHODS OBJECTIVES/METHODS OBJECTIVES/METHODS RESULTS RESULTS METHODS/RESULTS CONCLUSIONS
D03044,"We review the work on data-driven grasp synthesis and the methodologies for sampling and ranking candidate grasps.$$$We divide the approaches into three groups based on whether they synthesize grasps for known, familiar or unknown objects.$$$This structure allows us to identify common object representations and perceptual processes that facilitate the employed data-driven grasp synthesis technique.$$$In the case of known objects, we concentrate on the approaches that are based on object recognition and pose estimation.$$$In the case of familiar objects, the techniques use some form of a similarity matching to a set of previously encountered objects.$$$Finally for the approaches dealing with unknown objects, the core part is the extraction of specific features that are indicative of good grasps.$$$Our survey provides an overview of the different methodologies and discusses open problems in the area of robot grasping.$$$We also draw a parallel to the classical approaches that rely on analytic formulations.",OBJECTIVES METHODS METHODS METHODS METHODS METHODS RESULTS RESULTS
D01336,"In this paper, we leverage the efficiency of Binarized Neural Networks (BNNs) to learn complex state transition models of planning domains with discretized factored state and action spaces.$$$In order to directly exploit this transition structure for planning, we present two novel compilations of the learned factored planning problem with BNNs based on reductions to Weighted Partial Maximum Boolean Satisfiability (FD-SAT-Plan+) as well as Binary Linear Programming (FD-BLP-Plan+).$$$Theoretically, we show that our SAT-based Bi-Directional Neuron Activation Encoding is asymptotically the most compact encoding in the literature and maintains the generalized arc-consistency property through unit propagation -- an important property that facilitates efficiency in SAT solvers.$$$Experimentally, we validate the computational efficiency of our Bi-Directional Neuron Activation Encoding in comparison to an existing neuron activation encoding and demonstrate the effectiveness of learning complex transition models with BNNs.$$$We test the runtime efficiency of both FD-SAT-Plan+ and FD-BLP-Plan+ on the learned factored planning problem showing that FD-SAT-Plan+ scales better with increasing BNN size and complexity.$$$Finally, we present a finite-time incremental constraint generation algorithm based on generalized landmark constraints to improve the planning accuracy of our encodings through simulated or real-world interaction.",OBJECTIVES/METHODS METHODS RESULTS RESULTS METHODS/RESULTS METHODS/RESULTS
D06245,"A new transform over finite fields, the finite field Hartley transform (FFHT), was recently introduced and a number of promising applications on the design of efficient multiple access systems and multilevel spread spectrum sequences were proposed.$$$The FFHT exhibits interesting symmetries, which are exploited to derive tailored fast transform algorithms.$$$The proposed fast algorithms are based on successive decompositions of the FFHT by means of Hadamard-Walsh transforms (HWT).$$$The introduced decompositions meet the lower bound on the multiplicative complexity for all the cases investigated.$$$The complexity of the new algorithms is compared with that of traditional algorithms.",OBJECTIVES/CONCLUSIONS METHODS OBJECTIVES/METHODS METHODS/RESULTS CONCLUSIONS
D02765,"Current remote sensing image classification problems have to deal with an unprecedented amount of heterogeneous and complex data sources.$$$Upcoming missions will soon provide large data streams that will make land cover/use classification difficult.$$$Machine learning classifiers can help at this, and many methods are currently available.$$$A popular kernel classifier is the Gaussian process classifier (GPC), since it approaches the classification problem with a solid probabilistic treatment, thus yielding confidence intervals for the predictions as well as very competitive results to state-of-the-art neural networks and support vector machines.$$$However, its computational cost is prohibitive for large scale applications, and constitutes the main obstacle precluding wide adoption.$$$This paper tackles this problem by introducing two novel efficient methodologies for Gaussian Process (GP) classification.$$$We first include the standard random Fourier features approximation into GPC, which largely decreases its computational cost and permits large scale remote sensing image classification.$$$In addition, we propose a model which avoids randomly sampling a number of Fourier frequencies, and alternatively learns the optimal ones within a variational Bayes approach.$$$The performance of the proposed methods is illustrated in complex problems of cloud detection from multispectral imagery and infrared sounding data.$$$Excellent empirical results support the proposal in both computational cost and accuracy.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/METHODS METHODS METHODS RESULTS RESULTS/CONCLUSIONS
D05562,"Distributed parameter estimation for large-scale systems is an active research problem.$$$The goal is to derive a distributed algorithm in which each agent obtains a local estimate of its own subset of the global parameter vector, based on local measurements as well as information received from its neighbours.$$$A recent algorithm has been proposed, which yields the optimal solution (i.e., the one that would be obtained using a centralized method) in finite time, provided the communication network forms an acyclic graph.$$$If instead, the graph is cyclic, the only available alternative algorithm, which is based on iterative matrix inversion, achieving the optimal solution, does so asymptotically.$$$However, it is also known that, in the cyclic case, the algorithm designed for acyclic graphs produces a solution which, although non optimal, is highly accurate.$$$In this paper we do a theoretical study of the accuracy of this algorithm, in communication networks forming cyclic graphs.$$$To this end, we provide bounds for the sub-optimality of the estimation error and the estimation error covariance, for a class of systems whose topological sparsity and signal-to-noise ratio satisfy certain condition.$$$Our results show that, at each node, the accuracy improves exponentially with the so-called loop-free depth.$$$Also, although the algorithm no longer converges in finite time in the case of cyclic graphs, simulation results show that the convergence is significantly faster than that of methods based on iterative matrix inversion.$$$Our results suggest that, depending on the loop-free depth, the studied algorithm may be the preferred option even in applications with cyclic communication graphs.",BACKGROUND OBJECTIVES BACKGROUND BACKGROUND RESULTS OBJECTIVES METHODS RESULTS RESULTS RESULTS
D01965,"The ability of having a sparse representation for a certain class of signals has many applications in data analysis, image processing, and other research fields.$$$Among sparse representations, the cosparse analysis model has recently gained increasing interest.$$$Many signals exhibit a multidimensional structure, e.g. images or three-dimensional MRI scans.$$$Most data analysis and learning algorithms use vectorized signals and thereby do not account for this underlying structure.$$$The drawback of not taking the inherent structure into account is a dramatic increase in computational cost.$$$We propose an algorithm for learning a cosparse Analysis Operator that adheres to the preexisting structure of the data, and thus allows for a very efficient implementation.$$$This is achieved by enforcing a separable structure on the learned operator.$$$Our learning algorithm is able to deal with multidimensional data of arbitrary order.$$$We evaluate our method on volumetric data at the example of three-dimensional MRI scans.",BACKGROUND BACKGROUND BACKGROUND METHODS BACKGROUND/OBJECTIVES OBJECTIVES/METHODS/RESULTS METHODS METHODS RESULTS/CONCLUSIONS
D01560,"We study the problem of searching for and tracking a collection of moving targets using a robot with a limited Field-Of-View (FOV) sensor.$$$The actual number of targets present in the environment is not known a priori.$$$We propose a search and tracking framework based on the concept of Bayesian Random Finite Sets (RFSs).$$$Specifically, we generalize the Gaussian Mixture Probability Hypothesis Density (GM-PHD) filter which was previously applied for tracking problems to allow for simultaneous search and tracking with a limited FOV sensor.$$$The proposed framework can extract individual target tracks as well as estimate the number and the spatial density of targets.$$$We also show how to use the Gaussian Process (GP) regression to extract and predict non-linear target trajectories in this framework.$$$We demonstrate the efficacy of our techniques through representative simulations and a real data collected from an aerial robot.",OBJECTIVES BACKGROUND METHODS METHODS RESULTS METHODS RESULTS
D00186,"Since the proof of the four color theorem in 1976, computer-generated proofs have become a reality in mathematics and computer science.$$$During the last decade, we have seen formal proofs using verified proof assistants being used to verify the validity of such proofs.$$$In this paper, we describe a formalized theory of size-optimal sorting networks.$$$From this formalization we extract a certified checker that successfully verifies computer-generated proofs of optimality on up to 8 inputs.$$$The checker relies on an untrusted oracle to shortcut the search for witnesses on more than 1.6 million NP-complete subproblems.",BACKGROUND BACKGROUND OBJECTIVES RESULTS METHODS
D05532,"We present a new back propagation based training algorithm for discrete-time spiking neural networks (SNN).$$$Inspired by recent deep learning algorithms on binarized neural networks, binary activation with a straight-through gradient estimator is used to model the leaky integrate-fire spiking neuron, overcoming the difficulty in training SNNs using back propagation.$$$Two SNN training algorithms are proposed: (1) SNN with discontinuous integration, which is suitable for rate-coded input spikes, and (2) SNN with continuous integration, which is more general and can handle input spikes with temporal information.$$$Neuromorphic hardware designed in 40nm CMOS exploits the spike sparsity and demonstrates high classification accuracy (>98% on MNIST) and low energy (48.4-773 nJ/image).",OBJECTIVES METHODS CONCLUSIONS RESULTS
D05072,"The question whether an ontology can safely be replaced by another, possibly simpler, one is fundamental for many ontology engineering and maintenance tasks.$$$It underpins, for example, ontology versioning, ontology modularization, forgetting, and knowledge exchange.$$$What safe replacement means depends on the intended application of the ontology.$$$If, for example, it is used to query data, then the answers to any relevant ontology-mediated query should be the same over any relevant data set; if, in contrast, the ontology is used for conceptual reasoning, then the entailed subsumptions between concept expressions should coincide.$$$This gives rise to different notions of ontology inseparability such as query inseparability and concept inseparability, which generalize corresponding notions of conservative extensions.$$$We survey results on various notions of inseparability in the context of description logic ontologies, discussing their applications, useful model-theoretic characterizations, algorithms for determining whether two ontologies are inseparable (and, sometimes, for computing the difference between them if they are not), and the computational complexity of this problem.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/RESULTS
D06714,"Automatic feature extraction using neural networks has accomplished remarkable success for images, but for sound recognition, these models are usually modified to fit the nature of the multi-dimensional temporal representation of the audio signal in spectrograms.$$$This may not efficiently harness the time-frequency representation of the signal.$$$The ConditionaL Neural Network (CLNN) takes into consideration the interrelation between the temporal frames, and the Masked ConditionaL Neural Network (MCLNN) extends upon the CLNN by forcing a systematic sparseness over the network's weights using a binary mask.$$$The masking allows the network to learn about frequency bands rather than bins, mimicking a filterbank used in signal transformations such as MFCC.$$$Additionally, the Mask is designed to consider various combinations of features, which automates the feature hand-crafting process.$$$We applied the MCLNN for the Environmental Sound Recognition problem using the Urbansound8k, YorNoise, ESC-10 and ESC-50 datasets.$$$The MCLNN have achieved competitive performance compared to state-of-the-art Convolutional Neural Networks and hand-crafted attempts.",BACKGROUND OBJECTIVES METHODS METHODS METHODS CONCLUSIONS CONCLUSIONS
D05842,"We evaluate 8 different word embedding models on their usefulness for predicting the neural activation patterns associated with concrete nouns.$$$The models we consider include an experiential model, based on crowd-sourced association data, several popular neural and distributional models, and a model that reflects the syntactic context of words (based on dependency parses).$$$Our goal is to assess the cognitive plausibility of these various embedding models, and understand how we can further improve our methods for interpreting brain imaging data.$$$We show that neural word embedding models exhibit superior performance on the tasks we consider, beating experiential word representation model.$$$The syntactically informed model gives the overall best performance when predicting brain activation patterns from word embeddings; whereas the GloVe distributional method gives the overall best performance when predicting in the reverse direction (words vectors from brain images).$$$Interestingly, however, the error patterns of these different models are markedly different.$$$This may support the idea that the brain uses different systems for processing different kinds of words.$$$Moreover, we suggest that taking the relative strengths of different embedding models into account will lead to better models of the brain activity associated with words.",OBJECTIVES METHODS OBJECTIVES RESULTS RESULTS RESULTS CONCLUSIONS CONCLUSIONS
D06420,"We study the question of reconstructing a weighted, directed network up to isomorphism from its motifs.$$$In order to tackle this question we first relax the usual (strong) notion of graph isomorphism to obtain a relaxation that we call weak isomorphism.$$$Then we identify a definition of distance on the space of all networks that is compatible with weak isomorphism.$$$This global approach comes equipped with notions such as completeness, compactness, curves, and geodesics, which we explore throughout this paper.$$$Furthermore, it admits global-to-local inference in the following sense: we prove that two networks are weakly isomorphic if and only if all their motif sets are identical, thus answering the network reconstruction question.$$$Further exploiting the additional structure imposed by our network distance, we prove that two networks are weakly isomorphic if and only if certain essential associated structures---the skeleta of the respective networks---are strongly isomorphic.",OBJECTIVES METHODS METHODS RESULTS RESULTS RESULTS
D03663,In order to extract the best possible performance from asynchronous stochastic gradient descent one must increase the mini-batch size and scale the learning rate accordingly.$$$In order to achieve further speedup we introduce a technique that delays gradient updates effectively increasing the mini-batch size.$$$Unfortunately with the increase of mini-batch size we worsen the stale gradient problem in asynchronous stochastic gradient descent (SGD) which makes the model convergence poor.$$$We introduce local optimizers which mitigate the stale gradient problem and together with fine tuning our momentum we are able to train a shallow machine translation system 27% faster than an optimized baseline with negligible penalty in BLEU.,BACKGROUND/OBJECTIVES OBJECTIVES/METHODS METHODS/RESULTS METHODS/RESULTS/CONCLUSIONS
D03576,"Kernel alignment measures the degree of similarity between two kernels.$$$In this paper, inspired from kernel alignment, we propose a new Linear Discriminant Analysis (LDA) formulation, kernel alignment LDA (kaLDA).$$$We first define two kernels, data kernel and class indicator kernel.$$$The problem is to find a subspace to maximize the alignment between subspace-transformed data kernel and class indicator kernel.$$$Surprisingly, the kernel alignment induced kaLDA objective function is very similar to classical LDA and can be expressed using between-class and total scatter matrices.$$$This can be extended to multi-label data.$$$We use a Stiefel-manifold gradient descent algorithm to solve this problem.$$$We perform experiments on 8 single-label and 6 multi-label data sets.$$$Results show that kaLDA has very good performance on many single-label and multi-label problems.",BACKGROUND OBJECTIVES METHODS METHODS METHODS METHODS METHODS RESULTS RESULTS
D02078,"Collections of biological specimens are fundamental to scientific understanding and characterization of natural diversity.$$$This paper presents a system for liberating useful information from physical collections by bringing specimens into the digital domain so they can be more readily shared, analyzed, annotated and compared.$$$It focuses on insects and is strongly motivated by the desire to accelerate and augment current practices in insect taxonomy which predominantly use text, 2D diagrams and images to describe and characterize species.$$$While these traditional kinds of descriptions are informative and useful, they cannot cover insect specimens ""from all angles"" and precious specimens are still exchanged between researchers and collections for this reason.$$$Furthermore, insects can be complex in structure and pose many challenges to computer vision systems.$$$We present a new prototype for a practical, cost-effective system of off-the-shelf components to acquire natural-colour 3D models of insects from around 3mm to 30mm in length.$$$Colour images are captured from different angles and focal depths using a digital single lens reflex (DSLR) camera rig and two-axis turntable.$$$These 2D images are processed into 3D reconstructions using software based on a visual hull algorithm.$$$The resulting models are compact (around 10 megabytes), afford excellent optical resolution, and can be readily embedded into documents and web pages, as well as viewed on mobile devices.$$$The system is portable, safe, relatively affordable, and complements the sort of volumetric data that can be acquired by computed tomography.$$$This system provides a new way to augment the description and documentation of insect species holotypes, reducing the need to handle or ship specimens.$$$It opens up new opportunities to collect data for research, education, art, entertainment, biodiversity assessment and biosecurity control.",BACKGROUND OBJECTIVES OBJECTIVES BACKGROUND BACKGROUND METHODS METHODS METHODS RESULTS RESULTS CONCLUSIONS CONCLUSIONS
D01502,"DeepPrior is a simple approach based on Deep Learning that predicts the joint 3D locations of a hand given a depth map.$$$Since its publication early 2015, it has been outperformed by several impressive works.$$$Here we show that with simple improvements: adding ResNet layers, data augmentation, and better initial hand localization, we achieve better or similar performance than more sophisticated recent methods on the three main benchmarks (NYU, ICVL, MSRA) while keeping the simplicity of the original method.$$$Our new implementation is available at https://github.com/moberweger/deep-prior-pp .",METHODS BACKGROUND RESULTS OTHERS
D00234,"The amount of information available to the mathematics teacher is so enormous that the selection of desirable content is gradually becoming a huge task in itself.$$$With respect to the inclusion of elements of history of mathematics in mathematics instruction, the era of Big Data introduces a high likelihood of Recency Bias, a hitherto unconnected challenge for stakeholders in mathematics education.$$$This tendency to choose recent information at the expense of relevant older, composite, historical facts stands to defeat the aims and objectives of the epistemological and cultural approach to mathematics instructional delivery.$$$This study is a didactic discourse with focus on this threat to the history and pedagogy of mathematics, particularly as it affects mathematics education in Nigeria.$$$The implications for mathematics curriculum developers, teacher-training programmes, teacher lesson preparation, and publication of mathematics instructional materials were also deeply considered.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES CONCLUSIONS
D05490,"Peridynamics is a non-local generalization of continuum mechanics tailored to address discontinuous displacement fields arising in fracture mechanics.$$$As many non-local approaches, peridynamics requires considerable computing resources to solve practical problems.$$$Several implementations of peridynamics utilizing CUDA, OpenCL, and MPI were developed to address this important issue.$$$On modern supercomputers, asynchronous many task systems are emerging to address the new architecture of computational nodes.$$$This paper presents a peridynamics EMU nodal discretization implementation with the C++ Standard Library for Concurrency and Parallelism (HPX), an open source asynchronous many task run time system.$$$The code is designed for modular expandability, so as to simplify it to extend with new material models or discretizations.$$$The code is convergent for implicit time integration and recovers theoretical solutions.$$$Explicit time integration, convergence results are presented to showcase the agreement of results with theoretical claims in previous works.$$$Two benchmark tests on code scalability are applied demonstrating agreement between this code's scalability and theoretical estimations.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES METHODS METHODS METHODS RESULTS
D01499,"Recently, many methods to interpret and visualize deep neural network predictions have been proposed and significant progress has been made.$$$However, a more class-discriminative and visually pleasing explanation is required.$$$Thus, this paper proposes a region-based approach that estimates feature importance in terms of appropriately segmented regions.$$$By fusing the saliency maps generated from multi-scale segmentations, a more class-discriminative and visually pleasing map is obtained.$$$We incorporate this regional multi-scale concept into a prediction difference method that is model-agnostic.$$$An input image is segmented in several scales using the super-pixel method, and exclusion of a region is simulated by sampling a normal distribution constructed using the boundary prior.$$$The experimental results demonstrate that the regional multi-scale method produces much more class-discriminative and visually pleasing saliency maps.",BACKGROUND BACKGROUND OBJECTIVES METHODS CONCLUSIONS METHODS RESULTS
D04757,"Many network applications rely on the synchronization of coupled oscillators.$$$For example, such synchronization can provide networked devices with a common temporal reference necessary for coordinating actions or decoding transmitted messages.$$$In this paper, we study the problem of using distributed control to achieve both phase and frequency synchronization of a network of coupled heterogeneous nonlinear oscillators.$$$Not only do our controllers guarantee zero phase error in steady state under arbitrary frequency heterogeneity, but they also require little knowledge of the oscillator nonlinearities and network topology.$$$Furthermore, we provide a global convergence analysis, in the absence of noise and propagation delay, for the resulting nonlinear system whose phase vector evolves on the n-torus.",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES/RESULTS RESULTS
D04410,"Offline signature verification is one of the most challenging tasks in biometrics and document forensics.$$$Unlike other verification problems, it needs to model minute but critical details between genuine and forged signatures, because a skilled falsification might often resembles the real signature with small deformation.$$$This verification task is even harder in writer independent scenarios which is undeniably fiscal for realistic cases.$$$In this paper, we model an offline writer independent signature verification task with a convolutional Siamese network.$$$Siamese networks are twin networks with shared weights, which can be trained to learn a feature space where similar observations are placed in proximity.$$$This is achieved by exposing the network to a pair of similar and dissimilar observations and minimizing the Euclidean distance between similar pairs while simultaneously maximizing it between dissimilar pairs.$$$Experiments conducted on cross-domain datasets emphasize the capability of our network to model forgery in different languages (scripts) and handwriting styles.$$$Moreover, our designed Siamese network, named SigNet, exceeds the state-of-the-art results on most of the benchmark signature datasets, which paves the way for further research in this direction.",BACKGROUND OBJECTIVES OBJECTIVES METHODS METHODS METHODS CONCLUSIONS CONCLUSIONS
D03718,"We develop a multiexposure image fusion method based on texture features, which exploits the edge preserving and intraregion smoothing property of nonlinear diffusion filters based on partial differential equations (PDE).$$$With the captured multiexposure image series, we first decompose images into base layers and detail layers to extract sharp details and fine details, respectively.$$$The magnitude of the gradient of the image intensity is utilized to encourage smoothness at homogeneous regions in preference to inhomogeneous regions.$$$Then, we have considered texture features of the base layer to generate a mask (i.e., decision mask) that guides the fusion of base layers in multiresolution fashion.$$$Finally, well-exposed fused image is obtained that combines fused base layer and the detail layers at each scale across all the input exposures.$$$Proposed algorithm skipping complex High Dynamic Range Image (HDRI) generation and tone mapping steps to produce detail preserving image for display on standard dynamic range display devices.$$$Moreover, our technique is effective for blending flash/no-flash image pair and multifocus images, that is, images focused on different targets.",OBJECTIVES METHODS METHODS METHODS RESULTS CONCLUSIONS CONCLUSIONS
D02499,"Low-rank learning has attracted much attention recently due to its efficacy in a rich variety of real-world tasks, e.g., subspace segmentation and image categorization.$$$Most low-rank methods are incapable of capturing low-dimensional subspace for supervised learning tasks, e.g., classification and regression.$$$This paper aims to learn both the discriminant low-rank representation (LRR) and the robust projecting subspace in a supervised manner.$$$To achieve this goal, we cast the problem into a constrained rank minimization framework by adopting the least squares regularization.$$$Naturally, the data label structure tends to resemble that of the corresponding low-dimensional representation, which is derived from the robust subspace projection of clean data by low-rank learning.$$$Moreover, the low-dimensional representation of original data can be paired with some informative structure by imposing an appropriate constraint, e.g., Laplacian regularizer.$$$Therefore, we propose a novel constrained LRR method.$$$The objective function is formulated as a constrained nuclear norm minimization problem, which can be solved by the inexact augmented Lagrange multiplier algorithm.$$$Extensive experiments on image classification, human pose estimation, and robust face recovery have confirmed the superiority of our method.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS METHODS/CONCLUSIONS METHODS RESULTS/CONCLUSIONS
D00735,"Today in fast technology development in wireless mobile adhoc network there is vast scope for research.$$$As it is known that wireless communication for mobile network has many application areas like routing services, security services etc.$$$The mobile adhoc network is the wireless network for communication in which the mobile nodes are organized without any centralized administrator.$$$There are many Manet routing protocols like reactive, proactive, hybrid etc.$$$In this paper the reactive Manet routing protocol like DSR is simulated for traffic analysis for 50 mobile nodes for IP traffic flows.$$$Also throughput is analyzed for DSR and ER-DSR protocol.$$$And finally the memory utilized during simulation of DSR and ER-DSR is evaluated in order to compare both.",BACKGROUND BACKGROUND OBJECTIVES BACKGROUND OBJECTIVES METHODS RESULTS
D01650,"We present an open-source accessory for the NAO robot, which enables to test computationally demanding algorithms in an external platform while preserving robot's autonomy and mobility.$$$The platform has the form of a backpack, which can be 3D printed and replicated, and holds an ODROID XU4 board to process algorithms externally with ROS compatibility.$$$We provide also a software bridge between the B-Human's framework and ROS to have access to the robot's sensors close to real-time.$$$We tested the platform in several robotics applications such as data logging, visual SLAM, and robot vision with deep learning techniques.$$$The CAD model, hardware specifications and software are available online for the benefit of the community: https://github.com/uchile-robotics/nao-backpack",OBJECTIVES METHODS METHODS RESULTS RESULTS
D01957,"This paper addresses the problem of selecting an optimal sampling set for signals on graphs.$$$The proposed sampling set selection (SSS) is based on a localization operator that can consider both vertex domain and spectral domain localization.$$$We clarify the relationships among the proposed method, sensor position selection methods in machine learning, and conventional SSS methods based on graph frequency.$$$In contrast to the conventional graph signal processing-based approaches, the proposed method does not need to compute the eigendecomposition of a variation operator, while still considering (graph) frequency information.$$$We evaluate the performance of our approach through comparisons of prediction errors and execution time.",BACKGROUND METHODS METHODS METHODS RESULTS
D06493,"Face deidentification is an active topic amongst privacy and security researchers.$$$Early deidentification methods relying on image blurring or pixelization were replaced in recent years with techniques based on formal anonymity models that provide privacy guaranties and at the same time aim at retaining certain characteristics of the data even after deidentification.$$$The latter aspect is particularly important, as it allows to exploit the deidentified data in applications for which identity information is irrelevant.$$$In this work we present a novel face deidentification pipeline, which ensures anonymity by synthesizing artificial surrogate faces using generative neural networks (GNNs).$$$The generated faces are used to deidentify subjects in images or video, while preserving non-identity-related aspects of the data and consequently enabling data utilization.$$$Since generative networks are very adaptive and can utilize a diverse set of parameters (pertaining to the appearance of the generated output in terms of facial expressions, gender, race, etc.$$$), they represent a natural choice for the problem of face deidentification.$$$To demonstrate the feasibility of our approach, we perform experiments using automated recognition tools and human annotators.$$$Our results show that the recognition performance on deidentified images is close to chance, suggesting that the deidentification process based on GNNs is highly effective.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS RESULTS RESULTS/CONCLUSIONS
D02277,"Smile and Learn is an Ed-Tech company that runs a smart library with more that 100 applications, games and interactive stories, aimed at children aged 2 to 10 and their families.$$$Given the complexity of navigating all the content, the library implements a recommender system.$$$The purpose of this paper is to evaluate two aspects of such system: the influence of the order of recommendations on user exploratory behavior, and the impact of the choice of the recommendation algorithm on engagement.$$$The assessment, based on data collected between 2018/10/15 and 2018/12/01, required the analysis of the number of clicks performed on the recommendations depending on their ordering, and an A/B/C testing where two recommender algorithms were compared with a random recommendation that served as baseline.$$$The results suggest a direct connection between the order of the recommendation and the interest raised, and the superiority of recommendations based on popularity against other alternatives.",BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS/CONCLUSIONS
D04126,"The Biham-Middleton-Levine (BML) traffic model is a simple two-dimensional, discrete Cellular Automaton (CA) that has been used to study self-organization and phase transitions arising in traffic flows.$$$From the computational point of view, the BML model exhibits the usual features of discrete CA, where the state of the automaton are updated according to simple rules that depend on the state of each cell and its neighbors.$$$In this paper we study the impact of various optimizations for speeding up CA computations by using the BML model as a case study.$$$In particular, we describe and analyze the impact of several parallel implementations that rely on CPU features, such as multiple cores or SIMD instructions, and on GPUs.$$$Experimental evaluation provides quantitative measures of the payoff of each technique in terms of speedup with respect to a plain serial implementation.$$$Our findings show that the performance gap between CPU and GPU implementations of the BML traffic model can be reduced by clever exploitation of all CPU features.",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES/METHODS RESULTS CONCLUSIONS
D04465,"This paper presents an iterative smoothing technique for polygonal approximation of digital image boundary.$$$The technique starts with finest initial segmentation points of a curve.$$$The contribution of initially segmented points towards preserving the original shape of the image boundary is determined by computing the significant measure of every initial segmentation points which is sensitive to sharp turns, which may be missed easily when conventional significant measures are used for detecting dominant points.$$$The proposed method differentiates between the situations when a point on the curve between two points on a curve projects directly upon the line segment or beyond this line segment.$$$It not only identifies these situations, but also computes its significant contribution for these situations differently.$$$This situation-specific treatment allows preservation of points with high curvature even as revised set of dominant points are derived.$$$The experimental results show that the proposed technique competes well with the state of the art techniques.",METHODS METHODS METHODS METHODS METHODS METHODS RESULTS
D06460,"Network embedding (NE) is playing a principal role in network mining, due to its ability to map nodes into efficient low-dimensional embedding vectors.$$$However, two major limitations exist in state-of-the-art NE methods: structure preservation and uncertainty modeling.$$$Almost all previous methods represent a node into a point in space and focus on the local structural information, i.e., neighborhood information.$$$However, neighborhood information does not capture the global structural information and point vector representation fails in modeling the uncertainty of node representations.$$$In this paper, we propose a new NE framework, struc2gauss, which learns node representations in the space of Gaussian distributions and performs network embedding based on global structural information.$$$struc2gauss first employs a given node similarity metric to measure the global structural information, then generates structural context for nodes and finally learns node representations via Gaussian embedding.$$$Different structural similarity measures of networks and energy functions of Gaussian embedding are investigated.$$$Experiments conducted on both synthetic and real-world data sets demonstrate that struc2gauss effectively captures the global structural information while state-of-the-art network embedding methods fails to, outperforms other methods on the structure-based clustering task and provides more information on uncertainties of node representations.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS RESULTS/CONCLUSIONS
D03045,"While it is known that shared quantum entanglement can offer improved solutions to a number of purely cooperative tasks for groups of remote agents, controversy remains regarding the legitimacy of quantum games in a competitive setting--in particular, whether they offer any advantage beyond what is achievable using classical resources.$$$We construct a competitive game between four players based on the minority game where the maximal Nash-equilibrium payoff when played with the appropriate quantum resource is greater than that obtainable by classical means, assuming a local hidden variable model.$$$The game is constructed in a manner analogous to a Bell inequality.$$$This result is important in confirming the legitimacy of quantum games.",BACKGROUND RESULTS METHODS CONCLUSIONS
D01583,"Most Information Retrieval models compute the relevance score of a document for a given query by summing term weights specific to a document or a query.$$$Heuristic approaches, like TF-IDF, or probabilistic models, like BM25, are used to specify how a term weight is computed.$$$In this paper, we propose to leverage learning-to-rank principles to learn how to compute a term weight for a given document based on the term occurrence pattern.",BACKGROUND BACKGROUND OBJECTIVES
D05273,"The Rate Control Protocol (RCP) is a congestion control protocol that relies on explicit feedback from routers.$$$RCP estimates the flow rate using two forms of feedback: rate mismatch and queue size.$$$However, it remains an open design question whether queue size feedback in RCP is useful, given the presence of rate mismatch.$$$The model we consider has RCP flows operating over a single bottleneck, with heterogeneous time delays.$$$We first derive a sufficient condition for global stability, and then highlight how this condition favors the design choice of having only rate mismatch in the protocol definition.",BACKGROUND BACKGROUND BACKGROUND/OBJECTIVES METHODS RESULTS/CONCLUSIONS
D02376,"Current reconfiguration techniques are based on starting the system in a consistent configuration, in which all participating entities are in their initial state.$$$Starting from that state, the system must preserve consistency as long as a predefined churn rate of processors joins and leaves is not violated, and unbounded storage is available.$$$Many working systems cannot control this churn rate and do not have access to unbounded storage.$$$System designers that neglect the outcome of violating the above assumptions may doom the system to exhibit illegal behaviors.$$$We present the first automatically recovering reconfiguration scheme that recovers from transient faults, such as temporal violations of the above assumptions.$$$Our self-stabilizing solutions regain safety automatically by assuming temporal access to reliable failure detectors.$$$Once safety is re-established, the failure detector reliability is no longer needed.$$$Still, liveness is conditioned by the failure detector's unreliable signals.$$$We show that our self-stabilizing reconfiguration techniques can serve as the basis for the implementation of several dynamic services over message passing systems.$$$Examples include self-stabilizing reconfigurable virtual synchrony, which, in turn, can be used for implementing a self-stabilizing reconfigurable state-machine replication and self-stabilizing reconfigurable emulation of shared memory.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS/RESULTS METHODS/RESULTS METHODS/RESULTS RESULTS CONCLUSIONS
D04036,"The present study proposes LitStoryTeller, an interactive system for visually exploring the semantic structure of a scientific article.$$$We demonstrate how LitStoryTeller could be used to answer some of the most fundamental research questions, such as how a new method was built on top of existing methods, based on what theoretical proof and experimental evidences.$$$More importantly, LitStoryTeller can assist users to understand the full and interesting story a scientific paper, with a concise outline and important details.$$$The proposed system borrows a metaphor from screen play, and visualizes the storyline of a scientific paper by arranging its characters (scientific concepts or terminologies) and scenes (paragraphs/sentences) into a progressive and interactive storyline.$$$Such storylines help to preserve the semantic structure and logical thinking process of a scientific paper.$$$Semantic structures, such as scientific concepts and comparative sentences, are extracted using existing named entity recognition APIs and supervised classifiers, from a scientific paper automatically.$$$Two supplementary views, ranked entity frequency view and entity co-occurrence network view, are provided to help users identify the ""main plot"" of such scientific storylines.$$$When collective documents are ready, LitStoryTeller also provides a temporal entity evolution view and entity community view for collection digestion.",OBJECTIVES RESULTS RESULTS METHODS METHODS METHODS METHODS METHODS
D01870,"During the last decades, classical models in language theory have been extended by control mechanisms defined by monoids.$$$We study which monoids cause the extensions of context-free grammars, finite automata, or finite state transducers to exceed the capacity of the original model.$$$Furthermore, we investigate when, in the extended automata model, the nondeterministic variant differs from the deterministic one in capacity.$$$We show that all these conditions are in fact equivalent and present an algebraic characterization.$$$In particular, the open question of whether every language generated by a valence grammar over a finite monoid is context-free is provided with a positive answer.",BACKGROUND OBJECTIVES OBJECTIVES RESULTS RESULTS
D04990,"SRAM-based FPGAs are increasingly popular in the aerospace industry due to their field programmability and low cost.$$$However, they suffer from cosmic radiation induced Single Event Upsets (SEUs).$$$In safety-critical applications, the dependability of the design is a prime concern since failures may have catastrophic consequences.$$$An early analysis of the relationship between dependability metrics, performability-area trade-off, and different mitigation techniques for such applications can reduce the design effort while increasing the design confidence.$$$This paper introduces a novel methodology based on probabilistic model checking, for the analysis of the reliability, availability, safety and performance-area tradeoffs of safety-critical systems for early design decisions.$$$Starting from the high-level description of a system, a Markov reward model is constructed from the Control Data Flow Graph (CDFG) and a component characterization library targeting FPGAs.$$$The proposed model and exhaustive analysis capture all the failure states (based on the fault detection coverage) and repairs possible in the system.$$$We present quantitative results based on an FIR filter circuit to illustrate the applicability of the proposed approach and to demonstrate that a wide range of useful dependability and performability properties can be analyzed using the proposed methodology.$$$The modeling results show the relationship between different mitigation techniques and fault detection coverage, exposing their direct impact on the design for early decisions.",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES METHODS METHODS METHODS RESULTS RESULTS
D05822,"This paper presents a short and simple proof of the Four-Color Theorem, which can be utterly checkable by human mathematicians, without computer assistance.$$$The new key idea that has permitted it is presented in the Introduction.",OBJECTIVES/CONCLUSIONS METHODS
D03091,"Neural encoder-decoder models of machine translation have achieved impressive results, rivalling traditional translation models.$$$However their modelling formulation is overly simplistic, and omits several key inductive biases built into traditional models.$$$In this paper we extend the attentional neural translation model to include structural biases from word based alignment models, including positional bias, Markov conditioning, fertility and agreement over translation directions.$$$We show improvements over a baseline attentional model and standard phrase-based model over several language pairs, evaluating on difficult languages in a low resource setting.",BACKGROUND OBJECTIVES METHODS RESULTS
D04902,"The ubiquity of online fashion shopping demands effective recommendation services for customers.$$$In this paper, we study two types of fashion recommendation: (i) suggesting an item that matches existing components in a set to form a stylish outfit (a collection of fashion items), and (ii) generating an outfit with multimodal (images/text) specifications from a user.$$$To this end, we propose to jointly learn a visual-semantic embedding and the compatibility relationships among fashion items in an end-to-end fashion.$$$More specifically, we consider a fashion outfit to be a sequence (usually from top to bottom and then accessories) and each item in the outfit as a time step.$$$Given the fashion items in an outfit, we train a bidirectional LSTM (Bi-LSTM) model to sequentially predict the next item conditioned on previous ones to learn their compatibility relationships.$$$Further, we learn a visual-semantic space by regressing image features to their semantic representations aiming to inject attribute and category information as a regularization for training the LSTM.$$$The trained network can not only perform the aforementioned recommendations effectively but also predict the compatibility of a given outfit.$$$We conduct extensive experiments on our newly collected Polyvore dataset, and the results provide strong qualitative and quantitative evidence that our framework outperforms alternative methods.",BACKGROUND OBJECTIVES METHODS METHODS METHODS METHODS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS
D00218,"In this paper, we introduce ""Power Linear Unit"" (PoLU) which increases the nonlinearity capacity of a neural network and thus helps improving its performance.$$$PoLU adopts several advantages of previously proposed activation functions.$$$First, the output of PoLU for positive inputs is designed to be identity to avoid the gradient vanishing problem.$$$Second, PoLU has a non-zero output for negative inputs such that the output mean of the units is close to zero, hence reducing the bias shift effect.$$$Thirdly, there is a saturation on the negative part of PoLU, which makes it more noise-robust for negative inputs.$$$Furthermore, we prove that PoLU is able to map more portions of every layer's input to the same space by using the power function and thus increases the number of response regions of the neural network.$$$We use image classification for comparing our proposed activation function with others.$$$In the experiments, MNIST, CIFAR-10, CIFAR-100, Street View House Numbers (SVHN) and ImageNet are used as benchmark datasets.$$$The neural networks we implemented include widely-used ELU-Network, ResNet-50, and VGG16, plus a couple of shallow networks.$$$Experimental results show that our proposed activation function outperforms other state-of-the-art models with most networks.",OBJECTIVES METHODS METHODS METHODS METHODS METHODS RESULTS RESULTS RESULTS CONCLUSIONS
D04999,"The word E transformed everything is this world, as well as the whole globe itself.$$$To a great extend this helps for eco friendly green world.$$$In educational field, electronic medium has played a major role.$$$It influenced and changed almost every component of it to electronic medium like e-book, online courses, etc.$$$Throughout the world, leading universities are offering online courses voluntarily.$$$Generally we refer to it as Massive Online Open Courses (MOOCs).$$$There are many debates going on related to success and consequences of MOOCs.$$$Many are highlighting that these courses are self-paced, economical, and provide quality training to all irrespective of geographical constraints.$$$But many other academic people go against these points and keep listing many other disadvantages of MOOCs.$$$This paper explores the basics of MOOCs at the initial section.$$$Following section will deal with advantages and disadvantages of MOOCs in general.$$$We the researchers collected the details about the awareness of MOOCs among teachers and students in a higher education institution in Oman.$$$We have also collected the details about MOOCs implementation and usage within Oman educational society.$$$Based on the collected information, we have evaluated and presented the findings about MOOCs impact in Oman higher education.$$$We have felt that doing appropriate improvements in MOOCs may become an imperative medium in Oman educational institutions.$$$The suggestions are listed in the discussion and recommendation section.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES BACKGROUND OBJECTIVES OBJECTIVES OBJECTIVES OBJECTIVES METHODS RESULTS CONCLUSIONS CONCLUSIONS
D04537,"This paper describes the stages faced during the development of an Android program which obtains and decodes live images from DJI Phantom 3 Professional Drone and implements certain features of the TensorFlow Android Camera Demo application.$$$Test runs were made and outputs of the application were noted.$$$A lake was classified as seashore, breakwater and pier with the proximities of 24.44%, 21.16% and 12.96% respectfully.$$$The joystick of the UAV controller and laptop keyboard was classified with the proximities of 19.10% and 13.96% respectfully.$$$The laptop monitor was classified as screen, monitor and television with the proximities of 18.77%, 14.76% and 14.00% respectfully.$$$The computer used during the development of this study was classified as notebook and laptop with the proximities of 20.04% and 11.68% respectfully.$$$A tractor parked at a parking lot was classified with the proximity of 12.88%.$$$A group of cars in the same parking lot were classified as sports car, racer and convertible with the proximities of 31.75%, 18.64% and 13.45% respectfully at an inference time of 851ms.",BACKGROUND/RESULTS/CONCLUSIONS OTHERS RESULTS RESULTS RESULTS RESULTS RESULTS RESULTS
D03423,"This paper explores the problem of breast tissue classification of microscopy images.$$$Based on the predominant cancer type the goal is to classify images into four categories of normal, benign, in situ carcinoma, and invasive carcinoma.$$$Given a suitable training dataset, we utilize deep learning techniques to address the classification problem.$$$Due to the large size of each image in the training dataset, we propose a patch-based technique which consists of two consecutive convolutional neural networks.$$$The first ""patch-wise"" network acts as an auto-encoder that extracts the most salient features of image patches while the second ""image-wise"" network performs classification of the whole image.$$$The first network is pre-trained and aimed at extracting local information while the second network obtains global information of an input image.$$$We trained the networks using the ICIAR 2018 grand challenge on BreAst Cancer Histology (BACH) dataset.$$$The proposed method yields 95 % accuracy on the validation set compared to previously reported 77 % accuracy rates in the literature.$$$Our code is publicly available at https://github.com/ImagingLab/ICIAR2018",OBJECTIVES OBJECTIVES METHODS METHODS METHODS METHODS METHODS RESULTS OTHERS
D03270,"Artificial sound event detection (SED) has the aim to mimic the human ability to perceive and understand what is happening in the surroundings.$$$Nowadays, Deep Learning offers valuable techniques for this goal such as Convolutional Neural Networks (CNNs).$$$The Capsule Neural Network (CapsNet) architecture has been recently introduced in the image processing field with the intent to overcome some of the known limitations of CNNs, specifically regarding the scarce robustness to affine transformations (i.e., perspective, size, orientation) and the detection of overlapped images.$$$This motivated the authors to employ CapsNets to deal with the polyphonic-SED task, in which multiple sound events occur simultaneously.$$$Specifically, we propose to exploit the capsule units to represent a set of distinctive properties for each individual sound event.$$$Capsule units are connected through a so-called ""dynamic routing"" that encourages learning part-whole relationships and improves the detection performance in a polyphonic context.$$$This paper reports extensive evaluations carried out on three publicly available datasets, showing how the CapsNet-based algorithm not only outperforms standard CNNs but also allows to achieve the best results with respect to the state of the art algorithms.",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES OBJECTIVES OBJECTIVES METHODS/RESULTS
D00677,"Magnetic induction (MI) based communication and power transfer systems have gained an increased attention in the recent years.$$$Typical applications for these systems lie in the area of wireless charging, near-field communication, and wireless sensor networks.$$$For an optimal system performance, the power efficiency needs to be maximized.$$$Typically, this optimization refers to the impedance matching and tracking of the split-frequencies.$$$However, an important role of magnitude and phase of the input signal has been mostly overlooked.$$$Especially for the wireless power transfer systems with multiple transmitter coils, the optimization of the transmit signals can dramatically improve the power efficiency.$$$In this work, we propose an iterative algorithm for the optimization of the transmit signals for a transmitter with three orthogonal coils and multiple single coil receivers.$$$The proposed scheme significantly outperforms the traditional baseline algorithms in terms of power efficiency.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND/OBJECTIVES BACKGROUND/OBJECTIVES OBJECTIVES/METHODS RESULTS
D06725,"The research explores and examines factors for supplier evaluation and its impact on process improvement particularly aiming on a steel pipe manufacturing firm in Gujarat, India.$$$Data was collected using in-depth interview.$$$The questionnaire primarily involves the perception of evaluation of supplier.$$$Factors influencing supplier evaluation and its influence on process improvement is also examined in this study.$$$The model testing and validation were done using partial least square method.$$$Outcomes signified that the factors that influence the evaluation of the supplier are quality, cost, delivery and supplier relationship management.$$$The study depicted that quality and cost factors for supplier evaluation are insignificant.$$$The delivery and supplier relationship management have the significant influence on the evaluation of the supplier.$$$The research also depicted that supplier evaluation has a significant influence on process improvement.$$$Many researchers have considered quality, cost and delivery as the factors for evaluating the suppliers.$$$But for a company, it is quintessential to have a good relationship with the supplier.$$$Hence, the factor, supplier relationship management is considered for the study.$$$Also, the case study company focused more on quality and cost factors for the supplier evaluation of the firm.$$$However, delivery and supplier relationship management are also equally important for a firm in evaluating the supplier.",BACKGROUND/OBJECTIVES METHODS METHODS OBJECTIVES RESULTS RESULTS CONCLUSIONS CONCLUSIONS CONCLUSIONS BACKGROUND BACKGROUND BACKGROUND BACKGROUND OTHERS
D05211,"Deep learning is a fast-growing machine learning approach to perceive and understand large amounts of data.$$$In this paper, general information about the deep learning approach which is attracted much attention in the field of machine learning is given in recent years and an application about semantic image segmentation is carried out in order to help autonomous driving of autonomous vehicles.$$$This application is implemented with Fully Convolutional Network (FCN) architectures obtained by modifying the Convolutional Neural Network (CNN) architectures based on deep learning.$$$Experimental studies for the application are utilized 4 different FCN architectures named FCN-AlexNet,FCN-8s, FCN-16s and FCN-32s.$$$For the experimental studies, FCNs are first trained separately and validation accuracies of these trained network models on the used dataset is compared.$$$In addition, image segmentation inferences are visualized to take account of how precisely FCN architectures can segment objects.",BACKGROUND OBJECTIVES OBJECTIVES METHODS METHODS CONCLUSIONS
D03414,"Accurately determining dependency structure is critical to discovering a system's causal organization.$$$We recently showed that the transfer entropy fails in a key aspect of this---measuring information flow---due to its conflation of dyadic and polyadic relationships.$$$We extend this observation to demonstrate that this is true of all such Shannon information measures when used to analyze multivariate dependencies.$$$This has broad implications, particularly when employing information to express the organization and mechanisms embedded in complex systems, including the burgeoning efforts to combine complex network theory with information theory.$$$Here, we do not suggest that any aspect of information theory is wrong.$$$Rather, the vast majority of its informational measures are simply inadequate for determining the meaningful dependency structure within joint probability distributions.$$$Therefore, such information measures are inadequate for discovering intrinsic causal relations.$$$We close by demonstrating that such distributions exist across an arbitrary set of variables.",BACKGROUND/OBJECTIVES BACKGROUND RESULTS RESULTS/CONCLUSIONS OTHERS RESULTS/CONCLUSIONS CONCLUSIONS CONCLUSIONS
D04329,"In a multi-user millimeter (mm) wave communication system, we consider the problem of estimating the channel response between the central node (base station) and each of the user equipments (UE).$$$We propose three different strategies: 1) Each UE estimates its channel separately, 2) Base station estimates all the UEs channels jointly, and 3) Two stage process with estimation done at both UE and base station.$$$Exploiting the low rank nature of the mm wave channels, we propose a generalized block orthogonal matching pursuit (G.BOMP) framework for channel estimation in all the three strategies.$$$Our simulation results show that, the average beamforming gain of the G.BOMP algorithm is higher than that of the conventional OMP algorithm and other existing works on the multi-user mm wave system.",BACKGROUND/OBJECTIVES METHODS METHODS RESULTS/CONCLUSIONS
D01421,"This summary of the doctoral thesis is created to emphasize the close connection of the proposed spectral analysis method with the Discrete Fourier Transform (DFT), the most extensively studied and frequently used approach in the history of signal processing.$$$It is shown that in a typical application case, where uniform data readings are transformed to the same number of uniformly spaced frequencies, the results of the classical DFT and proposed approach coincide.$$$The difference in performance appears when the length of the DFT is selected to be greater than the length of the data.$$$The DFT solves the unknown data problem by padding readings with zeros up to the length of the DFT, while the proposed Extended DFT (EDFT) deals with this situation in a different way, it uses the Fourier integral transform as a target and optimizes the transform basis in the extended frequency range without putting such restrictions on the time domain.$$$Consequently, the Inverse DFT (IDFT) applied to the result of EDFT returns not only known readings, but also the extrapolated data, where classical DFT is able to give back just zeros, and higher resolution are achieved at frequencies where the data has been successfully extended.$$$It has been demonstrated that EDFT able to process data with missing readings or gaps inside or even nonuniformly distributed data.$$$Thus, EDFT significantly extends the usability of the DFT-based methods, where previously these approaches have been considered as not applicable.$$$The EDFT founds the solution in an iterative way and requires repeated calculations to get the adaptive basis, and this makes it numerical complexity much higher compared to DFT.$$$This disadvantage was a serious problem in the 1990s, when the method has been proposed.$$$Fortunately, since then the power of computers has increased so much that nowadays EDFT application could be considered as a real alternative.",BACKGROUND/OBJECTIVES BACKGROUND OBJECTIVES OBJECTIVES/METHODS METHODS/RESULTS RESULTS CONCLUSIONS CONCLUSIONS CONCLUSIONS CONCLUSIONS
D04254,"We define and study error detection and correction tasks that are useful for 3D reconstruction of neurons from electron microscopic imagery, and for image segmentation more generally.$$$Both tasks take as input the raw image and a binary mask representing a candidate object.$$$For the error detection task, the desired output is a map of split and merge errors in the object.$$$For the error correction task, the desired output is the true object.$$$We call this object mask pruning, because the candidate object mask is assumed to be a superset of the true object.$$$We train multiscale 3D convolutional networks to perform both tasks.$$$We find that the error-detecting net can achieve high accuracy.$$$The accuracy of the error-correcting net is enhanced if its input object mask is ""advice"" (union of erroneous objects) from the error-detecting net.",OBJECTIVES METHODS METHODS METHODS METHODS METHODS RESULTS RESULTS
D00382,"Sentiment understanding has been a long-term goal of AI in the past decades.$$$This paper deals with sentence-level sentiment classification.$$$Though a variety of neural network models have been proposed very recently, however, previous models either depend on expensive phrase-level annotation, whose performance drops substantially when trained with only sentence-level annotation; or do not fully employ linguistic resources (e.g., sentiment lexicons, negation words, intensity words), thus not being able to produce linguistically coherent representations.$$$In this paper, we propose simple models trained with sentence-level annotation, but also attempt to generating linguistically coherent representations by employing regularizers that model the linguistic role of sentiment lexicons, negation words, and intensity words.$$$Results show that our models are effective to capture the sentiment shifting effect of sentiment, negation, and intensity words, while still obtain competitive results without sacrificing the models' simplicity.",BACKGROUND OBJECTIVES BACKGROUND METHODS RESULTS/CONCLUSIONS
D06049,"This document summarizes the major milestones in mobile Augmented Reality between 1968 and 2014.$$$Major parts of the list were compiled by the member of the Christian Doppler Laboratory for Handheld Augmented Reality in 2010 (author list in alphabetical order) for the ISMAR society.$$$Later in 2013 it was updated, and more recent work was added during preparation of this report.$$$Permission is granted to copy and modify.",OTHERS OTHERS OTHERS OTHERS
D06756,"We introduce variational obstacle avoidance problems on Riemannian manifolds and derive necessary conditions for the existence of their normal extremals.$$$The problem consists of minimizing an energy functional depending on the velocity and covariant acceleration, among a set of admissible curves, and also depending on a navigation function used to avoid an obstacle on the workspace, a Riemannian manifold.$$$We study two different scenarios, a general one on a Riemannian manifold and, a sub-Riemannian problem.$$$By introducing a left-invariant metric on a Lie group, we also study the variational obstacle avoidance problem on a Lie group.$$$We apply the results to the obstacle avoidance problem of a planar rigid body and an unicycle.",BACKGROUND/CONCLUSIONS OBJECTIVES OBJECTIVES METHODS RESULTS/CONCLUSIONS
D01159,"Learning tasks on source code (i.e., formal languages) have been considered recently, but most work has tried to transfer natural language methods and does not capitalize on the unique opportunities offered by code's known syntax.$$$For example, long-range dependencies induced by using the same variable or function in distant locations are often not considered.$$$We propose to use graphs to represent both the syntactic and semantic structure of code and use graph-based deep learning methods to learn to reason over program structures.$$$In this work, we present how to construct graphs from source code and how to scale Gated Graph Neural Networks training to such large graphs.$$$We evaluate our method on two tasks: VarNaming, in which a network attempts to predict the name of a variable given its usage, and VarMisuse, in which the network learns to reason about selecting the correct variable that should be used at a given program location.$$$Our comparison to methods that use less structured program representations shows the advantages of modeling known structure, and suggests that our models learn to infer meaningful names and to solve the VarMisuse task in many cases.$$$Additionally, our testing showed that VarMisuse identifies a number of bugs in mature open-source projects.",BACKGROUND BACKGROUND OBJECTIVES/METHODS METHODS RESULTS RESULTS/CONCLUSIONS RESULTS
D03286,"In this work, we propose a novel sampling method for Design of Experiments.$$$This method allows to sample such input values of the parameters of a computational model for which the constructed surrogate model will have the least possible approximation error.$$$High efficiency of the proposed method is demonstrated by its comparison with other sampling techniques (LHS, Sobol' sequence sampling, and Maxvol sampling) on the problem of least-squares polynomial approximation.$$$Also, numerical experiments for the Lebesgue constant growth for the points sampled by the proposed method are carried out.",BACKGROUND OBJECTIVES RESULTS RESULTS
D05462,"This work is dedicated to introducing, executing, and assessing a three-stage speaker verification framework to enhance the degraded speaker verification performance in emotional talking environments.$$$Our framework is comprised of three cascaded stages: gender identification stage followed by an emotion identification stage followed by a speaker verification stage.$$$The proposed framework has been assessed on two distinct and independent emotional speech datasets: our collected dataset and Emotional Prosody Speech and Transcripts dataset.$$$Our results demonstrate that speaker verification based on both gender cues and emotion cues is superior to each of speaker verification based on gender cues only, emotion cues only, and neither gender cues nor emotion cues.$$$The achieved average speaker verification performance based on the suggested methodology is very similar to that attained in subjective assessment by human listeners.",OBJECTIVES METHODS METHODS RESULTS CONCLUSIONS
D04619,"This Note investigates the bias of the sampling importance resampling (SIR) filter in estimation of the state transition noise in the state space model.$$$The SIR filter may suffer from sample impoverishment that is caused by the resampling and therefore will benefit from a sampling proposal that has a heavier tail, e.g. the state transition noise simulated for particle preparation is bigger than the true noise involved with the state dynamics.$$$This is because a comparably big transition noise used for particle propagation can spread overlapped particles to counteract impoverishment, giving better approximation of the posterior.$$$As such, the SIR filter tends to yield a biased (bigger-than-the-truth) estimate of the transition noise if it is unknown and needs to be estimated, at least, in the forward-only filtering estimation.$$$The bias is elaborated via the direct roughening approach by means of both qualitative logical deduction and quantitative numerical simulation.",OBJECTIVES BACKGROUND BACKGROUND RESULTS METHODS
D00737,"In this paper, we derive a simple method for separating topological noise from topological features using a novel measure for comparing persistence barcodes called persistent entropy.",RESULTS
D02324,"Automated writing evaluation (AWE) has been shown to be an effective mechanism for quickly providing feedback to students.$$$It has already seen wide adoption in enterprise-scale applications and is starting to be adopted in large-scale contexts.$$$Training an AWE model has historically required a single batch of several hundred writing examples and human scores for each of them.$$$This requirement limits large-scale adoption of AWE since human-scoring essays is costly.$$$Here we evaluate algorithms for ensuring that AWE models are consistently trained using the most informative essays.$$$Our results show how to minimize training set sizes while maximizing predictive performance, thereby reducing cost without unduly sacrificing accuracy.$$$We conclude with a discussion of how to integrate this approach into large-scale AWE systems.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND METHODS RESULTS OTHERS
D00170,"In this paper, we analyze efficacy of the fast gradient sign method (FGSM) and the Carlini-Wagner's L2 (CW-L2) attack.$$$We prove that, within a certain regime, the untargeted FGSM can fool any convolutional neural nets (CNNs) with ReLU activation; the targeted FGSM can mislead any CNNs with ReLU activation to classify any given image into any prescribed class.$$$For a special two-layer neural network: a linear layer followed by the softmax output activation, we show that the CW-L2 attack increases the ratio of the classification probability between the target and ground truth classes.$$$Moreover, we provide numerical results to verify all our theoretical results.",BACKGROUND/OBJECTIVES RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS METHODS
D02652,"The proliferation of online biometric authentication has necessitated security requirements of biometric templates.$$$The existing secure biometric authentication schemes feature a server-centric model, where a service provider maintains a biometric database and is fully responsible for the security of the templates.$$$The end-users have to fully trust the server in storing, processing and managing their private templates.$$$As a result, the end-users' templates could be compromised by outside attackers or even the service provider itself.$$$In this paper, we propose a user-centric biometric authentication scheme (PassBio) that enables end-users to encrypt their own templates with our proposed light-weighted encryption scheme.$$$During authentication, all the templates remain encrypted such that the server will never see them directly.$$$However, the server is able to determine whether the distance of two encrypted templates is within a pre-defined threshold.$$$Our security analysis shows that no critical information of the templates can be revealed under both passive and active attacks.$$$PassBio follows a ""compute-then-compare"" computational model over encrypted data.$$$More specifically, our proposed Threshold Predicate Encryption (TPE) scheme can encrypt two vectors x and y in such a manner that the inner product of x and y can be evaluated and compared to a pre-defined threshold.$$$TPE guarantees that only the comparison result is revealed and no key information about x and y can be learned.$$$Furthermore, we show that TPE can be utilized as a flexible building block to evaluate different distance metrics such as Hamming distance and Euclidean distance over encrypted data.$$$Such a compute-then-compare computational model, enabled by TPE, can be widely applied in many interesting applications such as searching over encrypted data while ensuring data security and privacy.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES/METHODS OTHERS RESULTS METHODS METHODS METHODS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS
D00684,"The analysis of complex nonlinear systems is often carried out using simpler piecewise linear representations of them.$$$A principled and practical technique is proposed to linearize and evaluate arbitrary continuous nonlinear functions using polygonal (continuous piecewise linear) models under the L1 norm.$$$A thorough error analysis is developed to guide an optimal design of two kinds of polygonal approximations in the asymptotic case of a large budget of evaluation subintervals N. The method allows the user to obtain the level of linearization (N) for a target approximation error and vice versa.$$$It is suitable for, but not limited to, an efficient implementation in modern Graphics Processing Units (GPUs), allowing real-time performance of computationally demanding applications.$$$The quality and efficiency of the technique has been measured in detail on two nonlinear functions that are widely used in many areas of scientific computing and are expensive to evaluate",BACKGROUND OBJECTIVES METHODS METHODS RESULTS
D00743,"Object recognition is a key function in both human and machine vision.$$$While recent studies have achieved fMRI decoding of seen and imagined contents, the prediction is limited to training examples.$$$We present a decoding approach for arbitrary objects, using the machine vision principle that an object category is represented by a set of features rendered invariant through hierarchical processing.$$$We show that visual features including those from a convolutional neural network can be predicted from fMRI patterns and that greater accuracy is achieved for low/high-level features with lower/higher-level visual areas, respectively.$$$Predicted features are used to identify seen/imagined object categories (extending beyond decoder training) from a set of computed features for numerous object images.$$$Furthermore, the decoding of imagined objects reveals progressive recruitment of higher to lower visual representations.$$$Our results demonstrate a homology between human and machine vision and its utility for brain-based information retrieval.",BACKGROUND BACKGROUND METHODS RESULTS METHODS RESULTS CONCLUSIONS
D02133,"Cloud computing changed the way of computing as utility services offered through public network.$$$Selecting multiple providers for various computational requirements improves performance and minimizes cost of cloud services than choosing a single cloud provider.$$$Federated cloud improves scalability, cost minimization, performance maximization, collaboration with other providers, multi-site deployment for fault tolerance and recovery, reliability and less energy consumption.$$$Both providers and consumers could benefit from federated cloud where providers serve the consumers by satisfying Service Level Agreement, minimizing overall management and infrastructure cost; consumers get best services with less deployment cost and high availability.$$$Efficient provisioning of resources to consumers in federated cloud is a challenging task.$$$In this paper, the benefits of utilizing services from federated cloud, architecture with various coupling levels, different optimized resource provisioning methods and challenges associated with it are discussed and a comparative study is carried out over these aspects.",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES OBJECTIVES CONCLUSIONS
D01176,"Humans are routinely asked to evaluate the performance of other individuals, separating success from failure and affecting outcomes from science to education and sports.$$$Yet, in many contexts, the metrics driving the human evaluation process remain unclear.$$$Here we analyse a massive dataset capturing players' evaluations by human judges to explore human perception of performance in soccer, the world's most popular sport.$$$We use machine learning to design an artificial judge which accurately reproduces human evaluation, allowing us to demonstrate how human observers are biased towards diverse contextual features.$$$By investigating the structure of the artificial judge, we uncover the aspects of the players' behavior which attract the attention of human judges, demonstrating that human evaluation is based on a noticeability heuristic where only feature values far from the norm are considered to rate an individual's performance.",BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS/CONCLUSIONS
D00394,"A basic setup of a two-tier network, where two mobile users exchange messages with a multi-antenna macrocell basestation, is studied from a rate perspective subject to beamforming and power constraints.$$$The communication is facilitated by two femtocell basestations which act as relays as there is no direct link between the macrocell basestation and the mobile users.$$$We propose a scheme based on physical-layer network coding and compute-and-forward combined with a novel approach that solves the problem of beamformer design and power allocation.$$$We also show that the optimal beamformers are always a convex combination of the channels between the macro- and femtocell basestations.$$$We then establish the cut-set bound of the setup to show that the presented scheme almost achieves the capacity of the setup numerically.",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES/METHODS OBJECTIVES/METHODS/RESULTS
D00971,"A recent independent study resulted in a ranking system which ranked Astronomy and Computing (ASCOM) much higher than most of the older journals highlighting the niche prominence of the particular journal.$$$We investigate the remarkable ascendancy in reputation of ASCOM by proposing a novel differential equation based modeling.$$$The Modeling is a consequence of knowledge discovery from big data-centric methods, namely L1-SVD.$$$The inadequacy of the ranking method in explaining the reason behind the growth in reputation of ASCOM is reasonable to understand given that the study was post-facto.$$$Thus, we propose a growth model by accounting for the behavior of parameters that contribute to the growth of a field.$$$It is worthwhile to spend some time in analysing the cause and control variables behind rapid rise in reputation of a journal in a niche area.$$$We intent to probe and bring out parameters responsible for its growing influence.$$$Delay differential equations are used to model the change of influence on a journal's status by exploiting the effects of historical data.",BACKGROUND METHODS METHODS BACKGROUND METHODS OBJECTIVES OBJECTIVES METHODS
D05608,"The categorization of emotion names, i.e., the grouping of emotion words that have similar emotional connotations together, is a key tool of Social Psychology used to explore people's knowledge about emotions.$$$Without exception, the studies following that research line were based on the gauging of the perceived similarity between emotion names by the participants of the experiments.$$$Here we propose and examine a new approach to study the categories of emotion names - the similarities between target emotion names are obtained by comparing the contexts in which they appear in texts retrieved from the World Wide Web.$$$This comparison does not account for any explicit semantic information; it simply counts the number of common words or lexical items used in the contexts.$$$This procedure allows us to write the entries of the similarity matrix as dot products in a linear vector space of contexts.$$$The properties of this matrix were then explored using Multidimensional Scaling Analysis and Hierarchical Clustering.$$$Our main findings, namely, the underlying dimension of the emotion space and the categories of emotion names, were consistent with those based on people's judgments of emotion names similarities.",BACKGROUND BACKGROUND OBJECTIVES/METHODS METHODS METHODS METHODS RESULTS/CONCLUSIONS
D05403,"Markov Decision Processes (MDPs) have been used to formulate many decision-making problems in science and engineering.$$$The objective is to synthesize the best decision (action selection) policies to maximize expected rewards (or minimize costs) in a given stochastic dynamical environment.$$$In this paper, we extend this model by incorporating additional information that the transitions due to actions can be sequentially observed.$$$The proposed model benefits from this information and produces policies with better performance than those of standard MDPs.$$$The paper also presents an efficient offline linear programming based algorithm to synthesize optimal policies for the extended model.",BACKGROUND OBJECTIVES METHODS RESULTS METHODS
D03995,"External or internal domain-specific languages (DSLs) or (fluent) APIs?$$$Whoever you are -- a developer or a user of a DSL -- you usually have to choose your side; you should not!$$$What about metamorphic DSLs that change their shape according to your needs?$$$We report on our 4-years journey of providing the ""right"" support (in the domain of feature modeling), leading us to develop an external DSL, different shapes of an internal API, and maintain all these languages.$$$A key insight is that there is no one-size-fits-all solution or no clear superiority of a solution compared to another.$$$On the contrary, we found that it does make sense to continue the maintenance of an external and internal DSL.$$$The vision that we foresee for the future of software languages is their ability to be self-adaptable to the most appropriate shape (including the corresponding integrated development environment) according to a particular usage or task.$$$We call metamorphic DSL such a language, able to change from one shape to another shape.",BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS RESULTS CONCLUSIONS CONCLUSIONS
D04919,"The investigation of spatio-temporal dynamics of bacterial cells and their molecular components requires automated image analysis tools to track cell shape properties and molecular component locations inside the cells.$$$In the study of bacteria aging, the molecular components of interest are protein aggregates accumulated near bacteria boundaries.$$$This particular location makes very ambiguous the correspondence between aggregates and cells, since computing accurately bacteria boundaries in phase-contrast time-lapse imaging is a challenging task.$$$This paper proposes an active skeleton formulation for bacteria modeling which provides several advantages: an easy computation of shape properties (perimeter, length, thickness, orientation), an improved boundary accuracy in noisy images, and a natural bacteria-centered coordinate system that permits the intrinsic location of molecular components inside the cell.$$$Starting from an initial skeleton estimate, the medial axis of the bacterium is obtained by minimizing an energy function which incorporates bacteria shape constraints.$$$Experimental results on biological images and comparative evaluation of the performances validate the proposed approach for modeling cigar-shaped bacteria like Escherichia coli.$$$The Image-J plugin of the proposed method can be found online at http://fluobactracker.inrialpes.fr.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS OTHERS
D01380,"A large part of modern day communications are carried out through the medium of E-mails, especially corporate communications.$$$More and more people are using E-mail for personal uses too.$$$Companies also send notifications to their customers in E-mail.$$$In fact, in the Multinational business scenario E-mail is the most convenient and sought-after method of communication.$$$Important features of E-mail such as its speed, reliability, efficient storage options and a large number of added facilities make it highly popular among people from all sectors of business and society.$$$But being largely popular has its negative aspects too.$$$E-mails are the preferred medium for a large number of attacks over the internet.$$$Some of the most popular attacks over the internet include spams, and phishing mails.$$$Both spammers and phishers utilize E-mail services quite efficiently in spite of a large number of detection and prevention techniques already in place.$$$Very few methods are actually good in detection/prevention of spam/phishing related mails but they have higher false positives.$$$These techniques are implemented at the server and in addition to giving higher number of false positives, they add to the processing load on the server.$$$This paper outlines a novel approach to detect not only spam, but also scams, phishing and advertisement related mails.$$$In this method, we overcome the limitations of server-side detection techniques by utilizing some intelligence on the part of users.$$$Keywords parsing, token separation and knowledge bases are used in the background to detect almost all E-mail attacks.$$$The proposed methodology, if implemented, can help protect E-mail users from almost all kinds of unwanted mails with enhanced efficiency, reduced number of false positives while not increasing the load on E-mail servers.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS CONCLUSIONS
D05595,"In the last decade, computer-aided early diagnostics of Alzheimer's Disease (AD) and its prodromal form, Mild Cognitive Impairment (MCI), has been the subject of extensive research.$$$Some recent studies have shown promising results in the AD and MCI determination using structural and functional Magnetic Resonance Imaging (sMRI, fMRI), Positron Emission Tomography (PET) and Diffusion Tensor Imaging (DTI) modalities.$$$Furthermore, fusion of imaging modalities in a supervised machine learning framework has shown promising direction of research.$$$In this paper we first review major trends in automatic classification methods such as feature extraction based methods as well as deep learning approaches in medical image analysis applied to the field of Alzheimer's Disease diagnostics.$$$Then we propose our own design of a 3D Inception-based Convolutional Neural Network (CNN) for Alzheimer's Disease diagnostics.$$$The network is designed with an emphasis on the interior resource utilization and uses sMRI and DTI modalities fusion on hippocampal ROI.$$$The comparison with the conventional AlexNet-based network using data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset (http://adni.loni.usc.edu) demonstrates significantly better performance of the proposed 3D Inception-based CNN.",BACKGROUND OBJECTIVES/RESULTS OBJECTIVES/METHODS/RESULTS OBJECTIVES/METHODS METHODS METHODS METHODS/RESULTS/CONCLUSIONS
D05156,"A recent theoretical analysis shows the equivalence between non-negative matrix factorization (NMF) and spectral clustering based approach to subspace clustering.$$$As NMF and many of its variants are essentially linear, we introduce a nonlinear NMF with explicit orthogonality and derive general kernel-based orthogonal multiplicative update rules to solve the subspace clustering problem.$$$In nonlinear orthogonal NMF framework, we propose two subspace clustering algorithms, named kernel-based non-negative subspace clustering KNSC-Ncut and KNSC-Rcut and establish their connection with spectral normalized cut and ratio cut clustering.$$$We further extend the nonlinear orthogonal NMF framework and introduce a graph regularization to obtain a factorization that respects a local geometric structure of the data after the nonlinear mapping.$$$The proposed NMF-based approach to subspace clustering takes into account the nonlinear nature of the manifold, as well as its intrinsic local geometry, which considerably improves the clustering performance when compared to the several recently proposed state-of-the-art methods.",BACKGROUND OBJECTIVES/RESULTS OBJECTIVES/METHODS/RESULTS OBJECTIVES/METHODS/RESULTS CONCLUSIONS
D04366,"We propose an approach to address two issues that commonly occur during training of unsupervised GANs.$$$First, since GANs use only a continuous latent distribution to embed multiple classes or clusters of data, they often do not correctly handle the structural discontinuity between disparate classes in a latent space.$$$Second, discriminators of GANs easily forget about past generated samples by generators, incurring instability during adversarial training.$$$We argue that these two infamous problems of unsupervised GAN training can be largely alleviated by a learnable memory network to which both generators and discriminators can access.$$$Generators can effectively learn representation of training samples to understand underlying cluster distributions of data, which ease the structure discontinuity problem.$$$At the same time, discriminators can better memorize clusters of previously generated samples, which mitigate the forgetting problem.$$$We propose a novel end-to-end GAN model named memoryGAN, which involves a memory network that is unsupervisedly trainable and integrable to many existing GAN models.$$$With evaluations on multiple datasets such as Fashion-MNIST, CelebA, CIFAR10, and Chairs, we show that our model is probabilistically interpretable, and generates realistic image samples of high visual fidelity.$$$The memoryGAN also achieves the state-of-the-art inception scores over unsupervised GAN models on the CIFAR10 dataset, without any optimization tricks and weaker divergences.",OBJECTIVES BACKGROUND BACKGROUND METHODS METHODS METHODS METHODS RESULTS RESULTS
D00180,"Explaining the unreasonable effectiveness of deep learning has eluded researchers around the globe.$$$Various authors have described multiple metrics to evaluate the capacity of deep architectures.$$$In this paper, we allude to the radius margin bounds described for a support vector machine (SVM) with hinge loss, apply the same to the deep feed-forward architectures and derive the Vapnik-Chervonenkis (VC) bounds which are different from the earlier bounds proposed in terms of number of weights of the network.$$$In doing so, we also relate the effectiveness of techniques like Dropout and Dropconnect in bringing down the capacity of the network.$$$Finally, we describe the effect of maximizing the input as well as the output margin to achieve an input noise-robust deep architecture.",BACKGROUND BACKGROUND OBJECTIVES/METHODS RESULTS CONCLUSIONS
D02580,"Almost all known secret sharing schemes work on numbers.$$$Such methods will have difficulty in sharing graphs since the number of graphs increases exponentially with the number of nodes.$$$We propose a secret sharing scheme for graphs where we use graph intersection for reconstructing the secret which is hidden as a sub graph in the shares.$$$Our method does not rely on heavy computational operations such as modular arithmetic or polynomial interpolation but makes use of very basic operations like assignment and checking for equality, and graph intersection can also be performed visually.$$$In certain cases, the secret could be reconstructed using just pencil and paper by authorised parties but cannot be broken by an adversary even with unbounded computational power.$$$The method achieves perfect secrecy for (2, n) scheme and requires far fewer operations compared to Shamir's algorithm.$$$The proposed method could be used to share objects such as matrices, sets, plain text and even a heterogeneous collection of these.$$$Since we do not require a previously agreed upon encoding scheme, the method is very suitable for sharing heterogeneous collection of objects in a dynamic fashion.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS RESULTS RESULTS RESULTS
D06570,"In this paper, we introduce the notion of Plausible Deniability in an information theoretic framework.$$$We consider a scenario where an entity that eavesdrops through a broadcast channel summons one of the parties in a communication protocol to reveal their message (or signal vector).$$$It is desirable that the summoned party have enough freedom to produce a fake output that is likely plausible given the eavesdropper's observation.$$$We examine three variants of this problem -- Message Deniability, Transmitter Deniability, and Receiver Deniability.$$$In the first setting, the message sender is summoned to produce the sent message.$$$Similarly, in the second and third settings, the transmitter and the receiver are required to produce the transmitted codeword, and the received vector respectively.$$$For each of these settings, we examine the maximum communication rate that allows a given minimum rate of plausible fake outputs.$$$For the Message and Transmitter Deniability problems, we fully characterise the capacity region for general broadcast channels, while for the Receiver Deniability problem, we give an achievable rate region for physically degraded broadcast channels.",OBJECTIVES BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES OBJECTIVES OBJECTIVES RESULTS
D01684,"We propose an improvement of an oceanographic three dimensional variational assimilation scheme (3D-VAR), named OceanVar, by introducing a recursive filter (RF) with the third order of accuracy (3rd-RF), instead of a RF with first order of accuracy (1st-RF), to approximate horizontal Gaussian covariances.$$$An advantage of the proposed scheme is that the CPU's time can be substantially reduced with benefits on the large scale applications.$$$Experiments estimating the impact of 3rd-RF are performed by assimilating oceanographic data in two realistic oceanographic applications.$$$The results evince benefits in terms of assimilation process computational time, accuracy of the Gaussian correlation modeling, and show that the 3rd-RF is a suitable tool for operational data assimilation.",OBJECTIVES RESULTS RESULTS CONCLUSIONS
D01567,"In this paper, we present preliminary results of AFEL-REC, a recommender system for social learning environments.$$$AFEL-REC is build upon a scalable software architecture to provide recommendations of learning resources in near real-time.$$$Furthermore, AFEL-REC can cope with any kind of data that is present in social learning environments such as resource metadata, user interactions or social tags.$$$We provide a preliminary evaluation of three recommendation use cases implemented in AFEL-REC and we find that utilizing social data in form of tags is helpful for not only improving recommendation accuracy but also coverage.$$$This paper should be valuable for both researchers and practitioners interested in providing resource recommendations in social learning environments.",OBJECTIVES METHODS OTHERS RESULTS CONCLUSIONS
D06983,"In many cases, government data is still ""locked"" in several ""data silos"", even within the boundaries of a single (inter-)national public organization with disparate and distributed organizational units and departments spread across multiple sites.$$$Opening data and enabling its unified querying from a single site in an efficient and effective way is a semantic application integration and open government data challenge.$$$This paper describes how NARA is using Semantic Web technology to implement an application integration approach within the boundaries of its organization via opening and querying multiple governmental data sources from a single site.$$$The generic approach proposed, namely S3-AI, provides support to answering unified, ontology-mediated, federated queries to data produced and exploited by disparate applications, while these are being located in different organizational sites.$$$S3-AI preserves ownership, autonomy and independency of applications and data.$$$The paper extensively demonstrates S3-AI, using the D2RQ and Fuseki technologies, for addressing the needs of a governmental ""IT helpdesk support"" case.",BACKGROUND OBJECTIVES METHODS METHODS METHODS/RESULTS RESULTS/CONCLUSIONS
D05446,"We are concerned with robust and accurate forecasting of multiphase flow rates in wells and pipelines during oil and gas production.$$$In practice, the possibility to physically measure the rates is often limited; besides, it is desirable to estimate future values of multiphase rates based on the previous behavior of the system.$$$In this work, we demonstrate that a Long Short-Term Memory (LSTM) recurrent artificial network is able not only to accurately estimate the multiphase rates at current time (i.e., act as a virtual flow meter), but also to forecast the rates for a sequence of future time instants.$$$For a synthetic severe slugging case, LSTM forecasts compare favorably with the results of hydrodynamical modeling.$$$LSTM results for a realistic noizy dataset of a variable rate well test show that the model can also successfully forecast multiphase rates for a system with changing flow patterns.",OBJECTIVES BACKGROUND METHODS/RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS
D01109,We generalize previous studies on critical phenomena in communication networks by adding computational capabilities to the nodes to better describe real-world situations such as cloud computing.$$$A set of tasks with random origin and destination with a multi-tier computational structure is distributed on a network modeled as a graph.$$$The execution time (or latency) of each task is statically computed and the sum is used as the energy in a Montecarlo simulation in which the temperature parameter controls the resource allocation optimality.$$$We study the transition to congestion by varying temperature and system load.$$$A method to approximately recover the time-evolution of the system by interpolating the latency probability distributions is presented.$$$This allows us to study the standard transition to the congested phase by varying the task production rate.$$$We are able to reproduce the main known results on network congestion and to gain a deeper insight over the maximum theoretical performance of a system and its sensitivity to routing and load balancing errors.,OBJECTIVES METHODS METHODS METHODS METHODS/RESULTS RESULTS RESULTS
D03128,"Understanding software design practice is critical to understanding modern information systems development.$$$New developments in empirical software engineering, information systems design science and the interdisciplinary design literature combined with recent advances in process theory and testability have created a situation ripe for innovation.$$$Consequently, this paper utilizes these breakthroughs to formulate a process theory of software design practice: Sensemaking-Coevolution-Implementation Theory explains how complex software systems are created by collocated software development teams in organizations.$$$It posits that an independent agent (design team) creates a software system by alternating between three activities: organizing their perceptions about the context, mutually refining their understandings of the context and design space, and manifesting their understanding of the design space in a technological artifact.$$$This theory development paper defines and illustrates Sensemaking-Coevolution-Implementation Theory, grounds its concepts and relationships in existing literature, conceptually evaluates the theory and situates it in the broader context of information systems development.",BACKGROUND BACKGROUND OBJECTIVES/METHODS/RESULTS RESULTS OTHERS
D06528,"The rise of social media provides a great opportunity for people to reach out to their social connections to satisfy their information needs.$$$However, generic social media platforms are not explicitly designed to assist information seeking of users.$$$In this paper, we propose a novel framework to identify the social connections of a user able to satisfy his information needs.$$$The information need of a social media user is subjective and personal, and we investigate the utility of his social context to identify people able to satisfy it.$$$We present questions users post on Twitter as instances of information seeking activities in social media.$$$We infer soft community memberships of the asker and his social connections by integrating network and content information.$$$Drawing concepts from the social foci theory, we identify answerers who share communities with the asker w.r.t. the question.$$$Our experiments demonstrate that the framework is effective in identifying answerers to social media questions.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS RESULTS CONCLUSIONS
D04535,"We present a structural clustering algorithm for large-scale datasets of small labeled graphs, utilizing a frequent subgraph sampling strategy.$$$A set of representatives provides an intuitive description of each cluster, supports the clustering process, and helps to interpret the clustering results.$$$The projection-based nature of the clustering approach allows us to bypass dimensionality and feature extraction problems that arise in the context of graph datasets reduced to pairwise distances or feature vectors.$$$While achieving high quality and (human) interpretable clusterings, the runtime of the algorithm only grows linearly with the number of graphs.$$$Furthermore, the approach is easy to parallelize and therefore suitable for very large datasets.$$$Our extensive experimental evaluation on synthetic and real world datasets demonstrates the superiority of our approach over existing structural and subspace clustering algorithms, both, from a runtime and quality point of view.",BACKGROUND/OBJECTIVES METHODS BACKGROUND/METHODS RESULTS METHODS RESULTS/CONCLUSIONS
D04817,"Solar energy generation requires efficient monitoring and management in moving towards technologies for net-zero energy buildings.$$$This paper presents a dependable control system based on the Internet of Things (IoT) to control and manage the energy flow of renewable energy collected by solar panels within a microgrid.$$$Data for optimal control include not only measurements from local sensors but also meteorological information retrieved in real-time from online sources.$$$For system fault tolerance across the whole distributed control system featuring multiple controllers, dependable controllers are developed to control and optimise the tracking performance of photovoltaic arrays to maximally capture solar radiation and maintain system resilience and reliability in real time despite failures of one or more redundant controllers due to a problem with communication, hardware or cybersecurity.$$$Experimental results have been obtained to evaluate the validity of the proposed approach.",BACKGROUND OBJECTIVES/METHODS METHODS METHODS RESULTS/CONCLUSIONS
D01955,"Prices of NAND flash memories are falling drastically due to market growth and fabrication process mastering while research efforts from a technological point of view in terms of endurance and density are very active.$$$NAND flash memories are becoming the most important storage media in mobile computing and tend to be less confined to this area.$$$The major constraint of such a technology is the limited number of possible erase operations per block which tend to quickly provoke memory wear out.$$$To cope with this issue, state-of-the-art solutions implement wear leveling policies to level the wear out of the memory and so increase its lifetime.$$$These policies are integrated into the Flash Translation Layer (FTL) and greatly contribute in decreasing the write performance.$$$In this paper, we propose to reduce the flash memory wear out problem and improve its performance by absorbing the erase operations throughout a dual cache system replacing FTL wear leveling and garbage collection services.$$$We justify this idea by proposing a first performance evaluation of an exclusively cache based system for embedded flash memories.$$$Unlike wear leveling schemes, the proposed cache solution reduces the total number of erase operations reported on the media by absorbing them in the cache for workloads expressing a minimal global sequential rate.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/METHODS METHODS RESULTS
D00169,"Network quantization is one of network compression techniques to reduce the redundancy of deep neural networks.$$$It reduces the number of distinct network parameter values by quantization in order to save the storage for them.$$$In this paper, we design network quantization schemes that minimize the performance loss due to quantization given a compression ratio constraint.$$$We analyze the quantitative relation of quantization errors to the neural network loss function and identify that the Hessian-weighted distortion measure is locally the right objective function for the optimization of network quantization.$$$As a result, Hessian-weighted k-means clustering is proposed for clustering network parameters to quantize.$$$When optimal variable-length binary codes, e.g., Huffman codes, are employed for further compression, we derive that the network quantization problem can be related to the entropy-constrained scalar quantization (ECSQ) problem in information theory and consequently propose two solutions of ECSQ for network quantization, i.e., uniform quantization and an iterative solution similar to Lloyd's algorithm.$$$Finally, using the simple uniform quantization followed by Huffman coding, we show from our experiments that the compression ratios of 51.25, 22.17 and 40.65 are achievable for LeNet, 32-layer ResNet and AlexNet, respectively.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS RESULTS
D01415,"Understanding and interacting with everyday physical scenes requires rich knowledge about the structure of the world, represented either implicitly in a value or policy function, or explicitly in a transition model.$$$Here we introduce a new class of learnable models--based on graph networks--which implement an inductive bias for object- and relation-centric representations of complex, dynamical systems.$$$Our results show that as a forward model, our approach supports accurate predictions from real and simulated data, and surprisingly strong and efficient generalization, across eight distinct physical systems which we varied parametrically and structurally.$$$We also found that our inference model can perform system identification.$$$Our models are also differentiable, and support online planning via gradient-based trajectory optimization, as well as offline policy optimization.$$$Our framework offers new opportunities for harnessing and exploiting rich knowledge about the world, and takes a key step toward building machines with more human-like representations of the world.",BACKGROUND OBJECTIVES/METHODS METHODS/RESULTS METHODS/RESULTS METHODS/RESULTS CONCLUSIONS
D06319,"CT protocol design and quality control would benefit from automated tools to estimate the quality of generated CT images.$$$These tools could be used to identify erroneous CT acquisitions or refine protocols to achieve certain signal to noise characteristics.$$$This paper investigates blind estimation methods to determine global signal strength and noise levels in chest CT images.$$$Methods: We propose novel performance metrics corresponding to the accuracy of noise and signal estimation.$$$We implement and evaluate the noise estimation performance of six spatial- and frequency- based methods, derived from conventional image filtering algorithms.$$$Algorithms were tested on patient data sets from whole-body repeat CT acquisitions performed with a higher and lower dose technique over the same scan region.$$$Results: The proposed performance metrics can evaluate the relative tradeoff of filter parameters and noise estimation performance.$$$The proposed automated methods tend to underestimate CT image noise at low-flux levels.$$$Initial application of methodology suggests that anisotropic diffusion and Wavelet-transform based filters provide optimal estimates of noise.$$$Furthermore, methodology does not provide accurate estimates of absolute noise levels, but can provide estimates of relative change and/or trends in noise levels.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS RESULTS RESULTS CONCLUSIONS CONCLUSIONS OTHERS
D00707,"A robust and informative local shape descriptor plays an important role in mesh registration.$$$In this regard, spectral descriptors that are based on the spectrum of the Laplace-Beltrami operator have gained a spotlight among the researchers for the last decade due to their desirable properties, such as isometry invariance.$$$Despite such, however, spectral descriptors often fail to give a correct similarity measure for non-isometric cases where the metric distortion between the models is large.$$$Hence, they are in general not suitable for the registration problems, except for the special cases when the models are near-isometry.$$$In this paper, we investigate a way to develop shape descriptors for non-isometric registration tasks by embedding the spectral shape descriptors into a different metric space where the Euclidean distance between the elements directly indicates the geometric dissimilarity.$$$We design and train a Siamese deep neural network to find such an embedding, where the embedded descriptors are promoted to rearrange based on the geometric similarity.$$$We found our approach can significantly enhance the performance of the conventional spectral descriptors for the non-isometric registration tasks, and outperforms recent state-of-the-art method reported in literature.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/METHODS METHODS RESULTS/CONCLUSIONS
D01672,"Microscopic histology image analysis is a cornerstone in early detection of breast cancer.$$$However these images are very large and manual analysis is error prone and very time consuming.$$$Thus automating this process is in high demand.$$$We proposed a hierarchical system of convolutional neural networks (CNN) that classifies automatically patches of these images into four pathologies: normal, benign, in situ carcinoma and invasive carcinoma.$$$We evaluated our system on the BACH challenge dataset of image-wise classification and a small dataset that we used to extend it.$$$Using a train/test split of 75%/25%, we achieved an accuracy rate of 0.99 on the test split for the BACH dataset and 0.96 on that of the extension.$$$On the test of the BACH challenge, we've reached an accuracy of 0.81 which rank us to the 8th out of 51 teams.",BACKGROUND BACKGROUND BACKGROUND/OBJECTIVES OBJECTIVES/METHODS METHODS RESULTS RESULTS/CONCLUSIONS
D04202,"Future wireless systems are expected to provide a wide range of services to more and more users.$$$Advanced scheduling strategies thus arise not only to perform efficient radio resource management, but also to provide fairness among the users.$$$On the other hand, the users' perceived quality, i.e., Quality of Experience (QoE), is becoming one of the main drivers within the schedulers design.$$$In this context, this paper starts by providing a comprehension of what is QoE and an overview of the evolution of wireless scheduling techniques.$$$Afterwards, a survey on the most recent QoE-based scheduling strategies for wireless systems is presented, highlighting the application/service of the different approaches reported in the literature, as well as the parameters that were taken into account for QoE optimization.$$$Therefore, this paper aims at helping readers interested in learning the basic concepts of QoE-oriented wireless resources scheduling, as well as getting in touch with the present time research frontier.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES CONCLUSIONS
D04527,"Object queries are essential in information seeking and decision making in vast areas of applications.$$$However, a query may involve complex conditions on objects and sets, which can be arbitrarily nested and aliased.$$$The objects and sets involved as well as the demand---the given parameter values of interest---can change arbitrarily.$$$How to implement object queries efficiently under all possible updates, and furthermore to provide complexity guarantees?$$$This paper describes an automatic method.$$$The method allows powerful queries to be written completely declaratively.$$$It transforms demand as well as all objects and sets into relations.$$$Most importantly, it defines invariants for not only the query results, but also all auxiliary values about the objects and sets involved, including those for propagating demand, and incrementally maintains all of them.$$$Implementation and experiments with problems from a variety of application areas, including distributed algorithms and probabilistic queries, confirm the analyzed complexities, trade-offs, and significant improvements over prior work.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS METHODS METHODS RESULTS
D05565,"Did the demise of the Soviet Union in 1991 influence the scientific performance of the researchers in Eastern European countries?$$$Did this historical event affect international collaboration by researchers from the Eastern European countries with those of Western countries?$$$Did it also change international collaboration among researchers from the Eastern European countries?$$$Trying to answer these questions, this study aims to shed light on international collaboration by researchers from the Eastern European countries (Russia, Ukraine, Belarus, Moldova, Bulgaria, the Czech Republic, Hungary, Poland, Romania and Slovakia).$$$The number of publications and normalized citation impact values are compared for these countries based on InCites (Thomson Reuters), from 1981 up to 2011.$$$The international collaboration by researchers affiliated to institutions in Eastern European countries at the time points of 1990, 2000 and 2011 was studied with the help of Pajek and VOSviewer software, based on data from the Science Citation Index (Thomson Reuters).$$$Our results show that the breakdown of the communist regime did not lead, on average, to a huge improvement in the publication performance of the Eastern European countries and that the increase in international co-authorship relations by the researchers affiliated to institutions in these countries was smaller than expected.$$$Most of the Eastern European countries are still subject to changes and are still awaiting their boost in scientific development.",BACKGROUND/OBJECTIVES BACKGROUND/OBJECTIVES BACKGROUND/OBJECTIVES OBJECTIVES/METHODS METHODS METHODS RESULTS CONCLUSIONS
D03196,"The power flow equations, which relate power injections and voltage phasors, are at the heart of many electric power system computations.$$$While Newton-based methods typically find the ""high-voltage"" solution to the power flow equations, which is of primary interest, there are potentially many ""low-voltage"" solutions that are useful for certain analyses.$$$This paper addresses the number of solutions to the power flow equations.$$$There exist upper bounds on the number of power flow solutions; however, there is only limited work regarding bounds that are functions of network topology.$$$This paper empirically explores the relationship between the network topology, as characterized by the maximal cliques, and the number of power flow solutions.$$$To facilitate this analysis, we use a numerical polynomial homotopy continuation approach that is guaranteed to find all complex solutions to the power flow equations.$$$The number of solutions obtained from this approach upper bounds the number of real solutions.$$$Testing with many small networks informs the development of upper bounds that are functions of the network topology.$$$Initial results include empirically derived expressions for the maximum number of solutions for certain classes of network topologies.",BACKGROUND BACKGROUND OBJECTIVES BACKGROUND/OBJECTIVES OBJECTIVES/METHODS METHODS OBJECTIVES/METHODS RESULTS RESULTS
D01730,"The aim of this textbook is to provide students with basic knowledge of stochastic models that may apply to telecommunications research areas, such as traffic modelling, resource provisioning and traffic management.$$$These study areas are often collectively called teletraffic.$$$This book assumes prior knowledge of a programming language, mathematics, probability and stochastic processes normally taught in an electrical engineering course.$$$For students who have some but not sufficiently strong background in probability and stochastic processes, we provide, in the first few chapters, background on the relevant concepts in these areas.",BACKGROUND/OBJECTIVES/CONCLUSIONS BACKGROUND BACKGROUND BACKGROUND
D06644,"We propose three private information retrieval (PIR) protocols for distributed storage systems (DSSs) where data is stored using an arbitrary linear code.$$$The first two protocols, named Protocol 1 and Protocol 2, achieve privacy for the scenario with noncolluding nodes.$$$Protocol 1 requires a file size that is exponential in the number of files in the system, while Protocol 2 requires a file size that is independent of the number of files and is hence simpler.$$$We prove that, for certain linear codes, Protocol 1 achieves the maximum distance separable (MDS) PIR capacity, i.e., the maximum PIR rate (the ratio of the amount of retrieved stored data per unit of downloaded data) for a DSS that uses an MDS code to store any given (finite and infinite) number of files, and Protocol 2 achieves the asymptotic MDS-PIR capacity (with infinitely large number of files in the DSS).$$$In particular, we provide a necessary and a sufficient condition for a code to achieve the MDS-PIR capacity with Protocols 1 and 2 and prove that cyclic codes, Reed-Muller (RM) codes, and a class of distance-optimal local reconstruction codes achieve both the finite MDS-PIR capacity (i.e., with any given number of files) and the asymptotic MDS-PIR capacity with Protocols 1 and 2, respectively.$$$Furthermore, we present a third protocol, Protocol 3, for the scenario with multiple colluding nodes, which can be seen as an improvement of a protocol recently introduced by Freij-Hollanti et al..$$$Similar to the noncolluding case, we provide a necessary and a sufficient condition to achieve the maximum possible PIR rate of Protocol 3.$$$Moreover, we provide a particular class of codes that is suitable for this protocol and show that RM codes achieve the maximum possible PIR rate for the protocol.$$$For all three protocols, we present an algorithm to optimize their PIR rates.",OBJECTIVES/CONCLUSIONS RESULTS OTHERS RESULTS RESULTS RESULTS RESULTS RESULTS RESULTS
D00436,"Efficient processing of aggregated range queries on two-dimensional grids is a common requirement in information retrieval and data mining systems, for example in Geographic Information Systems and OLAP cubes.$$$We introduce a technique to represent grids supporting aggregated range queries that requires little space when the data points in the grid are clustered, which is common in practice.$$$We show how this general technique can be used to support two important types of aggregated queries, which are ranked range queries and counting range queries.$$$Our experimental evaluation shows that this technique can speed up aggregated queries up to more than an order of magnitude, with a small space overhead.",BACKGROUND OBJECTIVES OBJECTIVES RESULTS
D03575,"Graphics Processing Units (GPUs) with high computational capabilities used as modern parallel platforms to deal with complex computational problems.$$$We use this platform to solve large-scale linear programing problems by revised simplex algorithm.$$$To implement this algorithm, we propose some new memory management strategies.$$$In addition, to avoid cycling because of degeneracy conditions, we use a tabu rule for entering variable selection in the revised simplex algorithm.$$$To evaluate this algorithm, we consider two sets of benchmark problems and compare the speedup factors for these problems.$$$The comparisons demonstrate that the proposed method is highly effective and solve the problems with the maximum speedup factors 165.2 and 65.46 with respect to the sequential version and Matlab Linprog solver respectively.",METHODS METHODS METHODS METHODS RESULTS CONCLUSIONS
D02760,"The technology related to networking moves wired connection to wireless connection.The basic problem concern in the wireless domain, random packet loss for the end to end connection.$$$In this paper we show the performance and the impact of the packet loss and delay, by the bit error rate throughput etc with respect to the real world scenario vehicular ad hoc network in 3-dimension space (VANET in 3D).$$$Over the years software development has responded to the increasing growth of wireless connectivity in developing network enabled software.$$$In this paper we consider the real world physical problem in three dimensional wireless domain and map the problem to analytical problem .$$$In this paper we simulate that analytic problem with respect to real world scenario by using enhanced antenna position system (EAPS) mounted over the mobile node in 3D space.$$$In this paper we convert the real world problem into lab oriented problem by using the EAPS -system and shown the performance in wireless domain in 3 dimensional space.",BACKGROUND OBJECTIVES/RESULTS BACKGROUND METHODS METHODS METHODS
D00512,"Recent improvements in computing allow for the processing and analysis of very large datasets in a variety of fields.$$$Often the analysis requires the creation of low-rank approximations to the datasets leading to efficient storage.$$$This article presents and analyzes a novel approach for creating nonnegative, structured dictionaries using NMF applied to reordered pixels of single, natural images.$$$We reorder the pixels based on patches and present our approach in general.$$$We investigate our approach when using the Singular Value Decomposition (SVD) and Nonnegative Matrix Factorizations (NMF) as low-rank approximations.$$$Peak Signal-to-Noise Ratio (PSNR) and Mean Structural Similarity Index (MSSIM) are used to evaluate the algorithm.$$$We report that while the SVD provides the best reconstructions, its dictionary of vectors lose both the sign structure of the original image and details of localized image content.$$$In contrast, the dictionaries produced using NMF preserves the sign structure of the original image matrix and offer a nonnegative, parts-based dictionary.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS RESULTS RESULTS
D01579,"Facebook News Feed personalization algorithm has a significant impact, on a daily basis, on the lifestyle, mood and opinion of millions of Internet users.$$$Nonetheless, the behavior of such algorithms usually lacks transparency, motivating measurements, modeling and analysis in order to understand and improve its properties.$$$In this paper, we propose a reproducible methodology encompassing measurements and an analytical model to capture the visibility of publishers over a News Feed.$$$First, measurements are used to parameterize and to validate the expressive power of the proposed model.$$$Then, we conduct a what-if analysis to assess the visibility bias incurred by the users against a baseline derived from the model.$$$Our results indicate that a significant bias exists and it is more prominent at the top position of the News Feed.$$$In addition, we found that the bias is non-negligible even for users that are deliberately set as neutral with respect to their political views.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS RESULTS/CONCLUSIONS RESULTS
D02482,"Tax manipulation comes in a variety of forms with different motivations and of varying complexities.$$$In this paper, we deal with a specific technique used by tax-evaders known as circular trading.$$$In particular, we define algorithms for the detection and analysis of circular trade.$$$To achieve this, we have modelled the whole system as a directed graph with the actors being vertices and the transactions among them as directed edges.$$$We illustrate the results obtained after running the proposed algorithm on the commercial tax dataset of the government of Telangana, India, which contains the transaction details of a set of participants involved in a known circular trade.",BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS
D05966,"Scholars and practitioners across domains are increasingly concerned with algorithmic transparency and opacity, interrogating the values and assumptions embedded in automated, black-boxed systems, particularly in user-generated content platforms.$$$I report from an ethnography of infrastructure in Wikipedia to discuss an often understudied aspect of this topic: the local, contextual, learned expertise involved in participating in a highly automated social-technical environment.$$$Today, the organizational culture of Wikipedia is deeply intertwined with various data-driven algorithmic systems, which Wikipedians rely on to help manage and govern the ""anyone can edit"" encyclopedia at a massive scale.$$$These bots, scripts, tools, plugins, and dashboards make Wikipedia more efficient for those who know how to work with them, but like all organizational culture, newcomers must learn them if they want to fully participate.$$$I illustrate how cultural and organizational expertise is enacted around algorithmic agents by discussing two autoethnographic vignettes, which relate my personal experience as a veteran in Wikipedia.$$$I present thick descriptions of how governance and gatekeeping practices are articulated through and in alignment with these automated infrastructures.$$$Over the past 15 years, Wikipedian veterans and administrators have made specific decisions to support administrative and editorial workflows with automation in particular ways and not others.$$$I use these cases of Wikipedia's bot-supported bureaucracy to discuss several issues in the fields of critical algorithms studies, critical data studies, and fairness, accountability, and transparency in machine learning -- most principally arguing that scholarship and practice must go beyond trying to ""open up the black box"" of such systems and also examine sociocultural processes like newcomer socialization.",BACKGROUND OBJECTIVES/METHODS RESULTS RESULTS METHODS/RESULTS METHODS/RESULTS RESULTS CONCLUSIONS
D06219,"We present Quip, a lossless compression algorithm for next-generation sequencing data in the FASTQ and SAM/BAM formats.$$$In addition to implementing reference-based compression, we have developed, to our knowledge, the first assembly-based compressor, using a novel de novo assembly algorithm.$$$A probabilistic data structure is used to dramatically reduce the memory required by traditional de Bruijn graph assemblers, allowing millions of reads to be assembled very efficiently.$$$Read sequences are then stored as positions within the assembled contigs.$$$This is combined with statistical compression of read identifiers, quality scores, alignment information, and sequences, effectively collapsing very large datasets to less than 15% of their original size with no loss of information.$$$Availability: Quip is freely available under the BSD license from http://cs.washington.edu/homes/dcjones/quip.",OBJECTIVES BACKGROUND/METHODS METHODS/RESULTS METHODS METHODS/RESULTS OTHERS
D00812,"In this paper, we model the document revision detection problem as a minimum cost branching problem that relies on computing document distances.$$$Furthermore, we propose two new document distance measures, word vector-based Dynamic Time Warping (wDTW) and word vector-based Tree Edit Distance (wTED).$$$Our revision detection system is designed for a large scale corpus and implemented in Apache Spark.$$$We demonstrate that our system can more precisely detect revisions than state-of-the-art methods by utilizing the Wikipedia revision dumps https://snap.stanford.edu/data/wiki-meta.html and simulated data sets.",OBJECTIVES/METHODS OBJECTIVES/METHODS OBJECTIVES/METHODS RESULTS
D01278,"Handwritten mathematical expression recognition is a challenging problem due to the complicated two-dimensional structures, ambiguous handwriting input and variant scales of handwritten math symbols.$$$To settle this problem, we utilize the attention based encoder-decoder model that recognizes mathematical expression images from two-dimensional layouts to one-dimensional LaTeX strings.$$$We improve the encoder by employing densely connected convolutional networks as they can strengthen feature extraction and facilitate gradient propagation especially on a small training set.$$$We also present a novel multi-scale attention model which is employed to deal with the recognition of math symbols in different scales and save the fine-grained details that will be dropped by pooling operations.$$$Validated on the CROHME competition task, the proposed method significantly outperforms the state-of-the-art methods with an expression recognition accuracy of 52.8% on CROHME 2014 and 50.1% on CROHME 2016, by only using the official training dataset.",BACKGROUND METHODS METHODS METHODS RESULTS
D04831,"Fairness in algorithmic decision-making processes is attracting increasing concern.$$$When an algorithm is applied to human-related decision-making an estimator solely optimizing its predictive power can learn biases on the existing data, which motivates us the notion of fairness in machine learning. while several different notions are studied in the literature, little studies are done on how these notions affect the individuals.$$$We demonstrate such a comparison between several policies induced by well-known fairness criteria, including the color-blind (CB), the demographic parity (DP), and the equalized odds (EO).$$$We show that the EO is the only criterion among them that removes group-level disparity.$$$Empirical studies on the social welfare and disparity of these policies are conducted.",BACKGROUND BACKGROUND/OBJECTIVES OBJECTIVES/METHODS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS
D04920,"In this paper, the problem of joint caching and resource allocation is investigated for a network of cache-enabled unmanned aerial vehicles (UAVs) that service wireless ground users over the LTE licensed and unlicensed (LTE-U) bands.$$$The considered model focuses on users that can access both licensed and unlicensed bands while receiving contents from either the cache units at the UAVs directly or via content server-UAV-user links.$$$This problem is formulated as an optimization problem which jointly incorporates user association, spectrum allocation, and content caching.$$$To solve this problem, a distributed algorithm based on the machine learning framework of liquid state machine (LSM) is proposed.$$$Using the proposed LSM algorithm, the cloud can predict the users' content request distribution while having only limited information on the network's and users' states.$$$The proposed algorithm also enables the UAVs to autonomously choose the optimal resource allocation strategies that maximize the number of users with stable queues depending on the network states.$$$Based on the users' association and content request distributions, the optimal contents that need to be cached at UAVs as well as the optimal resource allocation are derived.$$$Simulation results using real datasets show that the proposed approach yields up to 33.3% and 50.3% gains, respectively, in terms of the number of users that have stable queues compared to two baseline algorithms: Q-learning with cache and Q-learning without cache.$$$The results also show that LSM significantly improves the convergence time of up to 33.3% compared to conventional learning algorithms such as Q-learning.",OBJECTIVES BACKGROUND OBJECTIVES METHODS METHODS METHODS RESULTS RESULTS RESULTS
D01859,"Bilinear pooling has been recently proposed as a feature encoding layer, which can be used after the convolutional layers of a deep network, to improve performance in multiple vision tasks.$$$Different from conventional global average pooling or fully connected layer, bilinear pooling gathers 2nd order information in a translation invariant fashion.$$$However, a serious drawback of this family of pooling layers is their dimensionality explosion.$$$Approximate pooling methods with compact properties have been explored towards resolving this weakness.$$$Additionally, recent results have shown that significant performance gains can be achieved by adding 1st order information and applying matrix normalization to regularize unstable higher order information.$$$However, combining compact pooling with matrix normalization and other order information has not been explored until now.$$$In this paper, we unify bilinear pooling and the global Gaussian embedding layers through the empirical moment matrix.$$$In addition, we propose a novel sub-matrix square-root layer, which can be used to normalize the output of the convolution layer directly and mitigate the dimensionality problem with off-the-shelf compact pooling methods.$$$Our experiments on three widely used fine-grained classification datasets illustrate that our proposed architecture, MoNet, can achieve similar or better performance than with the state-of-art G2DeNet.$$$Furthermore, when combined with compact pooling technique, MoNet obtains comparable performance with encoded features with 96% less dimensions.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS RESULTS
D01653,"XML data warehouses form an interesting basis for decision-support applications that exploit heterogeneous data from multiple sources.$$$However, XML-native database systems currently bear limited performances and it is necessary to research ways to optimize them.$$$In this paper, we propose a new index that is specifically adapted to the multidimensional architecture of XML warehouses and eliminates join operations, while preserving the information contained in the original warehouse.$$$A theoretical study and experimental results demonstrate the efficiency of our index, even when queries are complex.",BACKGROUND BACKGROUND OBJECTIVES RESULTS
D03338,"In many economic, social and political situations individuals carry out activities in groups (coalitions) rather than alone and on their own.$$$Examples range from households and sport clubs to research networks, political parties and trade unions.$$$The underlying game theoretic framework is known as coalition formation.$$$This survey discusses the notion of core stability in hedonic coalition formation (where each player's happiness only depends on the other members of his coalition but not on how the remaining players outside his coalition are grouped).$$$We present the central concepts and algorithmic approaches in the area, provide many examples, and pose a number of open problems.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS
D03863,"Clinical Named Entity Recognition (CNER) aims to identify and classify clinical terms such as diseases, symptoms, treatments, exams, and body parts in electronic health records, which is a fundamental and crucial task for clinical and translational research.$$$In recent years, deep neural networks have achieved significant success in named entity recognition and many other Natural Language Processing (NLP) tasks.$$$Most of these algorithms are trained end to end, and can automatically learn features from large scale labeled datasets.$$$However, these data-driven methods typically lack the capability of processing rare or unseen entities.$$$Previous statistical methods and feature engineering practice have demonstrated that human knowledge can provide valuable information for handling rare and unseen cases.$$$In this paper, we address the problem by incorporating dictionaries into deep neural networks for the Chinese CNER task.$$$Two different architectures that extend the Bi-directional Long Short-Term Memory (Bi-LSTM) neural network and five different feature representation schemes are proposed to handle the task.$$$Computational results on the CCKS-2017 Task 2 benchmark dataset show that the proposed method achieves the highly competitive performance compared with the state-of-the-art deep learning methods.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/METHODS METHODS RESULTS/CONCLUSIONS
D01091,"Many projects have applied knowledge patterns (KPs) to the retrieval of specialized information.$$$Yet terminologists still rely on manual analysis of concordance lines to extract semantic information, since there are no user-friendly publicly available applications enabling them to find knowledge rich contexts (KRCs).$$$To fill this void, we have created the KP-based EcoLexicon Semantic SketchGrammar (ESSG) in the well-known corpus query system Sketch Engine.$$$For the first time, the ESSG is now publicly available inSketch Engine to query the EcoLexicon English Corpus.$$$Additionally, reusing the ESSG in any English corpus uploaded by the user enables Sketch Engine to extract KRCs codifying generic-specific, part-whole, location, cause and function relations, because most of the KPs are domain-independent.$$$The information is displayed in the form of summary lists (word sketches) containing the pairs of terms linked by a given semantic relation.$$$This paper describes the process of building a KP-based sketch grammar with special focus on the last stage, namely, the evaluation with refinement purposes.$$$We conducted an initial shallow precision and recall evaluation of the 64 English sketch grammar rules created so far for hyponymy, meronymy and causality.$$$Precision was measured based on a random sample of concordances extracted from each word sketch type.$$$Recall was assessed based on a random sample of concordances where known term pairs are found.$$$The results are necessary for the improvement and refinement of the ESSG.$$$The noise of false positives helped to further specify the rules, whereas the silence of false negatives allows us to find useful new patterns.",BACKGROUND BACKGROUND OBJECTIVES RESULTS RESULTS RESULTS METHODS METHODS METHODS METHODS METHODS METHODS
D02418,"In recent years, fuzz testing has proven itself to be one of the most effective techniques for finding correctness bugs and security vulnerabilities in practice.$$$One particular fuzz testing tool, American Fuzzy Lop or AFL, has become popular thanks to its ease-of-use and bug-finding power.$$$However, AFL remains limited in the depth of program coverage it achieves, in particular because it does not consider which parts of program inputs should not be mutated in order to maintain deep program coverage.$$$We propose an approach, FairFuzz, that helps alleviate this limitation in two key steps.$$$First, FairFuzz automatically prioritizes inputs exercising rare parts of the program under test.$$$Second, it automatically adjusts the mutation of inputs so that the mutated inputs are more likely to exercise these same rare parts of the program.$$$We conduct evaluation on real-world programs against state-of-the-art versions of AFL, thoroughly repeating experiments to get good measures of variability.$$$We find that on certain benchmarks FairFuzz shows significant coverage increases after 24 hours compared to state-of-the-art versions of AFL, while on others it achieves high program coverage at a significantly faster rate.",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES/METHODS METHODS METHODS RESULTS RESULTS/CONCLUSIONS
D03647,"Computer aided diagnostic (CAD) system is crucial for modern med-ical imaging.$$$But almost all CAD systems operate on reconstructed images, which were optimized for radiologists.$$$Computer vision can capture features that is subtle to human observers, so it is desirable to design a CAD system op-erating on the raw data.$$$In this paper, we proposed a deep-neural-network-based detection system for lung nodule detection in computed tomography (CT).$$$A primal-dual-type deep reconstruction network was applied first to convert the raw data to the image space, followed by a 3-dimensional convolutional neural network (3D-CNN) for the nodule detection.$$$For efficient network training, the deep reconstruction network and the CNN detector was trained sequentially first, then followed by one epoch of end-to-end fine tuning.$$$The method was evaluated on the Lung Image Database Consortium image collection (LIDC-IDRI) with simulated forward projections.$$$With 144 multi-slice fanbeam pro-jections, the proposed end-to-end detector could achieve comparable sensitivity with the reference detector, which was trained and applied on the fully-sampled image data.$$$It also demonstrated superior detection performance compared to detectors trained on the reconstructed images.$$$The proposed method is general and could be expanded to most detection tasks in medical imaging.",BACKGROUND BACKGROUND BACKGROUND/OBJECTIVES METHODS METHODS METHODS METHODS METHODS/RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS CONCLUSIONS
D03976,"This paper presents a solution based on dual quaternion algebra to the general problem of pose (i.e., position and orientation) consensus for systems composed of multiple rigid-bodies.$$$The dual quaternion algebra is used to model the agents' poses and also in the distributed control laws, making the proposed technique easily applicable to formation control of general robotic systems.$$$The proposed pose consensus protocol has guaranteed convergence when the interaction among the agents is represented by directed graphs with directed spanning trees, which is a more general result when compared to the literature on formation control.$$$In order to illustrate the proposed pose consensus protocol and its extension to the problem of formation control, we present a numerical simulation with a large number of free-flying agents and also an application of cooperative manipulation by using real mobile manipulators.",BACKGROUND/OBJECTIVES METHODS RESULTS/CONCLUSIONS METHODS/RESULTS
D04255,Machine learning is used to compute achievable information rates (AIRs) for a simplified fiber channel.$$$The approach jointly optimizes the input distribution (constellation shaping) and the auxiliary channel distribution to compute AIRs without explicit channel knowledge in an end-to-end fashion.,OBJECTIVES/METHODS CONCLUSIONS
D01444,"Human-swarm interaction (HSI) involves a number of human factors impacting human behaviour throughout the interaction.$$$As the technologies used within HSI advance, it is more tempting to increase the level of swarm autonomy within the interaction to reduce the workload on humans.$$$Yet, the prospective negative effects of high levels of autonomy on human situational awareness can hinder this process.$$$Flexible autonomy aims at trading-off these effects by changing the level of autonomy within the interaction when required; with mixed-initiatives combining human preferences and automation's recommendations to select an appropriate level of autonomy at a certain point of time.$$$However, the effective implementation of mixed-initiative systems raises fundamental questions on how to combine human preferences and automation recommendations, how to realise the selected level of autonomy, and what the future impacts on the cognitive states of a human are.$$$We explore open challenges that hamper the process of developing effective flexible autonomy.$$$We then highlight the potential benefits of using system modelling techniques in HSI by illustrating how they provide HSI designers with an opportunity to evaluate different strategies for assessing the state of the mission and for adapting the level of autonomy within the interaction to maximise mission success metrics.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES
D01633,"This paper proposes a new communication protocol for a cooperative non-orthogonal multiple access (NOMA) system.$$$In this system, based on users' channel conditions, each two NOMA users are paired to reduce system complexity.$$$In this concern, the user with a better channel condition decodes and then forwards messages received from the source to the user with a worse channel condition.$$$In particular, the direct link between the paired users is assumed to be unavailable due to the weak transmission conditions.$$$To overcome this issue, we propose a new cooperative NOMA protocol in which an amplify-and-forward (AF) relay is employed to help the user-to-user communications.$$$To evaluate the proposed protocol, the exact closed-form expressions of outage probability (OP) at the two paired users are derived.$$$Based on the analysis of the OP, we further examine the system throughput in a delay-sensitive transmission mode.$$$Finally, our analytical results verified by Monte-Carlo simulation show that the proposed protocol is efficient in enhancing the performance of NOMA system when the user-to-user communications is limited.",OBJECTIVES BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS CONCLUSIONS
D01376,"Training of neural networks for automated diagnosis of pigmented skin lesions is hampered by the small size and lack of diversity of available datasets of dermatoscopic images.$$$We tackle this problem by releasing the HAM10000 (""Human Against Machine with 10000 training images"") dataset.$$$We collected dermatoscopic images from different populations acquired and stored by different modalities.$$$Given this diversity we had to apply different acquisition and cleaning methods and developed semi-automatic workflows utilizing specifically trained neural networks.$$$The final dataset consists of 10015 dermatoscopic images which are released as a training set for academic machine learning purposes and are publicly available through the ISIC archive.$$$This benchmark dataset can be used for machine learning and for comparisons with human experts.$$$Cases include a representative collection of all important diagnostic categories in the realm of pigmented lesions.$$$More than 50% of lesions have been confirmed by pathology, while the ground truth for the rest of the cases was either follow-up, expert consensus, or confirmation by in-vivo confocal microscopy.",BACKGROUND OBJECTIVES METHODS RESULTS RESULTS CONCLUSIONS RESULTS RESULTS
D06056,"Message digest algorithms are one of the underlying building blocks of blockchain platforms such as Ethereum.$$$This paper analyses situations in which the message digest collision resistance property can be exploited by attackers.$$$Two mitigations for possible attacks are described: longer message digest sizes make attacks more difficult; and, including timeliness properties limits the amount of time an attacker has to determine a hash collision.",BACKGROUND OBJECTIVES CONCLUSIONS
D06875,"This paper deals with a very renowned website (that is Book-Crossing) from two angles: The first angle focuses on the direct relations between users and books.$$$Many things can be inferred from this part of analysis such as who is more interested in book reading than others and why?$$$Which books are most popular and which users are most active and why?$$$The task requires the use of certain social network analysis measures (e.g. degree centrality).$$$What does it mean when two users like the same book?$$$Is it the same when other two users have one thousand books in common?$$$Who is more likely to be a friend of whom and why?$$$Are there specific people in the community who are more qualified to establish large circles of social relations?$$$These questions (and of course others) were answered through the other part of the analysis, which will take us to probe the potential social relations between users in this community.$$$Although these relationships do not exist explicitly, they can be inferred with the help of affiliation network analysis and techniques such as m-slice.",OBJECTIVES BACKGROUND BACKGROUND METHODS OBJECTIVES/METHODS RESULTS RESULTS RESULTS CONCLUSIONS RESULTS
D06329,"A novel variational autoencoder is developed to model images, as well as associated labels or captions.$$$The Deep Generative Deconvolutional Network (DGDN) is used as a decoder of the latent image features, and a deep Convolutional Neural Network (CNN) is used as an image encoder; the CNN is used to approximate a distribution for the latent DGDN features/code.$$$The latent code is also linked to generative models for labels (Bayesian support vector machine) or captions (recurrent neural network).$$$When predicting a label/caption for a new image at test, averaging is performed across the distribution of latent codes; this is computationally efficient as a consequence of the learned CNN-based encoder.$$$Since the framework is capable of modeling the image in the presence/absence of associated labels/captions, a new semi-supervised setting is manifested for CNN learning with images; the framework even allows unsupervised CNN learning, based on images alone.",OBJECTIVES METHODS METHODS METHODS METHODS
D00081,"Solar forecasting accuracy is affected by weather conditions, and weather awareness forecasting models are expected to improve the performance.$$$However, it may not be available and reliable to classify different forecasting tasks by using only meteorological weather categorization.$$$In this paper, an unsupervised clustering-based (UC-based) solar forecasting methodology is developed for short-term (1-hour-ahead) global horizontal irradiance (GHI) forecasting.$$$This methodology consists of three parts: GHI time series unsupervised clustering, pattern recognition, and UC-based forecasting.$$$The daily GHI time series is first clustered by an Optimized Cross-validated ClUsteRing (OCCUR) method, which determines the optimal number of clusters and best clustering results.$$$Then, support vector machine pattern recognition (SVM-PR) is adopted to recognize the category of a certain day using the first few hours' data in the forecasting stage.$$$GHI forecasts are generated by the most suitable models in different clusters, which are built by a two-layer Machine learning based Multi-Model (M3) forecasting framework.$$$The developed UC-based methodology is validated by using 1-year of data with six solar features.$$$Numerical results show that (i) UC-based models outperform non-UC (all-in-one) models with the same M3 architecture by approximately 20%; (ii) M3-based models also outperform the single-algorithm machine learning (SAML) models by approximately 20%.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS METHODS RESULTS RESULTS/CONCLUSIONS
D03096,"Accounting for undecided and uncertain voters is a challenging issue for predicting election results from public opinion polls.$$$Undecided voters typify the uncertainty of swing voters in polls but are often ignored or allocated to each candidate in a simple, deterministic manner.$$$Historically this may have been adequate because the undecided were comparatively small enough to assume that they do not affect the relative proportions of the decided voters.$$$However, in the presence of high numbers of undecided voters, these static rules may in fact bias election predictions from election poll authors and meta-poll analysts.$$$In this paper, we examine the effect of undecided voters in the 2016 US presidential election to the previous three presidential elections.$$$We show there were a relatively high number of undecided voters over the campaign and on election day, and that the allocation of undecided voters in this election was not consistent with two-party proportional (or even) allocations.$$$We find evidence that static allocation regimes are inadequate for election prediction models and that probabilistic allocations may be superior.$$$We also estimate the bias attributable to polling agencies, often referred to as ""house effects"".",BACKGROUND/OBJECTIVES BACKGROUND BACKGROUND BACKGROUND OBJECTIVES RESULTS RESULTS/CONCLUSIONS RESULTS
D06486,"End-to-end task-oriented dialog systems usually suffer from the challenge of incorporating knowledge bases.$$$In this paper, we propose a novel yet simple end-to-end differentiable model called memory-to-sequence (Mem2Seq) to address this issue.$$$Mem2Seq is the first neural generative model that combines the multi-hop attention over memories with the idea of pointer network.$$$We empirically show how Mem2Seq controls each generation step, and how its multi-hop attention mechanism helps in learning correlations between memories.$$$In addition, our model is quite general without complicated task-specific designs.$$$As a result, we show that Mem2Seq can be trained faster and attain the state-of-the-art performance on three different task-oriented dialog datasets.",BACKGROUND OBJECTIVES METHODS METHODS METHODS RESULTS/CONCLUSIONS
D04984,"With the popularity of mobile devices and the development of geo-positioning technology, location-based services (LBS) attract much attention and top-k spatial keyword queries become increasingly complex.$$$It is common to see that clients issue a query to find a restaurant serving pizza and steak, low in price and noise level particularly.$$$However, most of prior works focused only on the spatial keyword while ignoring these independent numerical attributes.$$$In this paper we demonstrate, for the first time, the Attributes-Aware Spatial Keyword Query (ASKQ), and devise a two-layer hybrid index structure called Quad-cluster Dual-filtering R-Tree (QDR-Tree).$$$In the keyword cluster layer, a Quad-Cluster Tree (QC-Tree) is built based on the hierarchical clustering algorithm using kernel k-means to classify keywords.$$$In the spatial layer, for each leaf node of the QC-Tree, we attach a Dual-Filtering R-Tree (DR-Tree) with two filtering algorithms, namely, keyword bitmap-based and attributes skyline-based filtering.$$$Accordingly, efficient query processing algorithms are proposed.$$$Through theoretical analysis, we have verified the optimization both in processing time and space consumption.$$$Finally, massive experiments with real-data demonstrate the efficiency and effectiveness of QDR-Tree.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS METHODS/RESULTS RESULTS/CONCLUSIONS
D01818,"This paper considers the problem of single-server single-message private information retrieval with coded side information (PIR-CSI).$$$In this problem, there is a server storing a database, and a user which knows a linear combination of a subset of messages in the database as a side information.$$$The number of messages contributing to the side information is known to the server, but the indices and the coefficients of these messages are unknown to the server.$$$The user wishes to download a message from the server privately, i.e., without revealing which message it is requesting, while minimizing the download cost.$$$In this work, we consider two different settings for the PIR-CSI problem depending on the demanded message being or not being one of the messages contributing to the side information.$$$For each setting, we prove an upper bound on the maximum download rate as a function of the size of the database and the size of the side information, and propose a protocol that achieves the rate upper-bound.",OBJECTIVES BACKGROUND BACKGROUND BACKGROUND BACKGROUND RESULTS
D00096,"High-order parametric models that include terms for feature interactions are applied to various data mining tasks, where ground truth depends on interactions of features.$$$However, with sparse data, the high- dimensional parameters for feature interactions often face three issues: expensive computation, difficulty in parameter estimation and lack of structure.$$$Previous work has proposed approaches which can partially re- solve the three issues.$$$In particular, models with factorized parameters (e.g.$$$Factorization Machines) and sparse learning algorithms (e.g.$$$FTRL-Proximal) can tackle the first two issues but fail to address the third.$$$Regarding to unstructured parameters, constraints or complicated regularization terms are applied such that hierarchical structures can be imposed.$$$However, these methods make the optimization problem more challenging.$$$In this work, we propose Strongly Hierarchical Factorization Machines and ANOVA kernel regression where all the three issues can be addressed without making the optimization problem more difficult.$$$Experimental results show the proposed models significantly outperform the state-of-the-art in two data mining tasks: cold-start user response time prediction and stock volatility prediction.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES RESULTS
D04348,"Virtual network services that span multiple data centers are important to support emerging data-intensive applications in fields such as bioinformatics and retail analytics.$$$Successful virtual network service composition and maintenance requires flexible and scalable 'constrained shortest path management' both in the management plane for virtual network embedding (VNE) or network function virtualization service chaining (NFV-SC), as well as in the data plane for traffic engineering (TE).$$$In this paper, we show analytically and empirically that leveraging constrained shortest paths within recent VNE, NFV-SC and TE algorithms can lead to network utilization gains (of up to 50%) and higher energy efficiency.$$$The management of complex VNE, NFV-SC and TE algorithms can be, however, intractable for large scale substrate networks due to the NP-hardness of the constrained shortest path problem.$$$To address such scalability challenges, we propose a novel, exact constrained shortest path algorithm viz., 'Neighborhoods Method' (NM).$$$Our NM uses novel search space reduction techniques and has a theoretical quadratic speed-up making it practically faster (by an order of magnitude) than recent branch-and-bound exhaustive search solutions.$$$Finally, we detail our NM-based SDN controller implementation in a real-world testbed to further validate practical NM benefits for virtual network services.",BACKGROUND BACKGROUND/OBJECTIVES OBJECTIVES/METHODS/RESULTS/CONCLUSIONS BACKGROUND OBJECTIVES/METHODS RESULTS RESULTS
D06390,"The Dubins Traveling Salesman Problem (DTSP) has generated significant interest over the last decade due to its occurrence in several civil and military surveillance applications.$$$Currently, there is no algorithm that can find an optimal solution to the problem.$$$In addition, relaxing the motion constraints and solving the resulting Euclidean TSP (ETSP) provides the only lower bound available for the problem.$$$However, in many problem instances, the lower bound computed by solving the ETSP is far below the cost of the feasible solutions obtained by some well-known algorithms for the DTSP.$$$This article addresses this fundamental issue and presents the first systematic procedure for developing tight lower bounds for the DTSP.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES
D01892,"Minimizing job scheduling time is a fundamental issue in data center networks that has been extensively studied in recent years.$$$The incoming jobs require different CPU and memory units, and span different number of time slots.$$$The traditional solution is to design efficient heuristic algorithms with performance guarantee under certain assumptions.$$$In this paper, we improve a recently proposed job scheduling algorithm using deep reinforcement learning and extend it to multiple server clusters.$$$Our study reveals that deep reinforcement learning method has the potential to outperform traditional resource allocation algorithms in a variety of complicated environments.",BACKGROUND BACKGROUND BACKGROUND/OBJECTIVES RESULTS CONCLUSIONS
D06244,"Image cropping aims at improving the aesthetic quality of images by adjusting their composition.$$$Most weakly supervised cropping methods (without bounding box supervision) rely on the sliding window mechanism.$$$The sliding window mechanism requires fixed aspect ratios and limits the cropping region with arbitrary size.$$$Moreover, the sliding window method usually produces tens of thousands of windows on the input image which is very time-consuming.$$$Motivated by these challenges, we firstly formulate the aesthetic image cropping as a sequential decision-making process and propose a weakly supervised Aesthetics Aware Reinforcement Learning (A2-RL) framework to address this problem.$$$Particularly, the proposed method develops an aesthetics aware reward function which especially benefits image cropping.$$$Similar to human's decision making, we use a comprehensive state representation including both the current observation and the historical experience.$$$We train the agent using the actor-critic architecture in an end-to-end manner.$$$The agent is evaluated on several popular unseen cropping datasets.$$$Experiment results show that our method achieves the state-of-the-art performance with much fewer candidate windows and much less time compared with previous weakly supervised methods.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/METHODS METHODS METHODS METHODS RESULTS RESULTS/CONCLUSIONS
D04424,"Reinforcement learning has significant applications for multi-agent systems, especially in unknown dynamic environments.$$$However, most multi-agent reinforcement learning (MARL) algorithms suffer from such problems as exponential computation complexity in the joint state-action space, which makes it difficult to scale up to realistic multi-agent problems.$$$In this paper, a novel algorithm named negotiation-based MARL with sparse interactions (NegoSI) is presented.$$$In contrast to traditional sparse-interaction based MARL algorithms, NegoSI adopts the equilibrium concept and makes it possible for agents to select the non-strict Equilibrium Dominating Strategy Profile (non-strict EDSP) or Meta equilibrium for their joint actions.$$$The presented NegoSI algorithm consists of four parts: the equilibrium-based framework for sparse interactions, the negotiation for the equilibrium set, the minimum variance method for selecting one joint action and the knowledge transfer of local Q-values.$$$In this integrated algorithm, three techniques, i.e., unshared value functions, equilibrium solutions and sparse interactions are adopted to achieve privacy protection, better coordination and lower computational complexity, respectively.$$$To evaluate the performance of the presented NegoSI algorithm, two groups of experiments are carried out regarding three criteria: steps of each episode (SEE), rewards of each episode (REE) and average runtime (AR).$$$The first group of experiments is conducted using six grid world games and shows fast convergence and high scalability of the presented algorithm.$$$Then in the second group of experiments NegoSI is applied to an intelligent warehouse problem and simulated results demonstrate the effectiveness of the presented NegoSI algorithm compared with other state-of-the-art MARL algorithms.",BACKGROUND BACKGROUND OBJECTIVES/METHODS METHODS METHODS METHODS RESULTS RESULTS RESULTS
D06869,"Data-driven analysis of complex networks has been in the focus of research for decades.$$$An important question is to discover the relation between various network characteristics in real-world networks and how these relationships vary across network domains.$$$A related research question is to study how well the network models can capture the observed relations between the graph metrics.$$$In this paper, we apply statistical and machine learning techniques to answer the aforementioned questions.$$$We study 400 real-world networks along with 2400 networks generated by five frequently used network models with previously fitted parameters to make the generated graphs as similar to the real network as possible.$$$We find that the correlation profiles of the structural measures significantly differ across network domains and the domain can be efficiently determined using a small selection of graph metrics.$$$The goodness-of-fit of the network models and the best performing models themselves highly depend on the domains.$$$Using machine learning techniques, it turned out to be relatively easy to decide if a network is real or model-generated.$$$We also investigate what structural properties make it possible to achieve a good accuracy, i.e. what features the network models cannot capture.",BACKGROUND OBJECTIVES OBJECTIVES METHODS METHODS RESULTS RESULTS RESULTS RESULTS
D04114,"The use of explicit object detectors as an intermediate step to image captioning - which used to constitute an essential stage in early work - is often bypassed in the currently dominant end-to-end approaches, where the language model is conditioned directly on a mid-level image embedding.$$$We argue that explicit detections provide rich semantic information, and can thus be used as an interpretable representation to better understand why end-to-end image captioning systems work well.$$$We provide an in-depth analysis of end-to-end image captioning by exploring a variety of cues that can be derived from such object detections.$$$Our study reveals that end-to-end image captioning systems rely on matching image representations to generate captions, and that encoding the frequency, size and position of objects are complementary and all play a role in forming a good image representation.$$$It also reveals that different object categories contribute in different ways towards image captioning.",BACKGROUND OBJECTIVES METHODS RESULTS RESULTS
D01079,"State-of-the-art deep reading comprehension models are dominated by recurrent neural nets.$$$Their sequential nature is a natural fit for language, but it also precludes parallelization within an instances and often becomes the bottleneck for deploying such models to latency critical scenarios.$$$This is particularly problematic for longer texts.$$$Here we present a convolutional architecture as an alternative to these recurrent architectures.$$$Using simple dilated convolutional units in place of recurrent ones, we achieve results comparable to the state of the art on two question answering tasks, while at the same time achieving up to two orders of magnitude speedups for question answering.",BACKGROUND BACKGROUND/OBJECTIVES BACKGROUND/OBJECTIVES METHODS METHODS/RESULTS
D05521,"One approach to achieving artificial general intelligence (AGI) is through the emergence of complex structures and dynamic properties arising from decentralized networks of interacting artificial intelligence (AI) agents.$$$Understanding the principles of consensus in societies and finding ways to make consensus more reliable becomes critically important as connectivity and interaction speed increase in modern distributed systems of hybrid collective intelligences, which include both humans and computer systems.$$$We propose a new form of reputation-based consensus with greater resistance to reputation gaming than current systems have.$$$We discuss options for its implementation, and provide initial practical results.",BACKGROUND BACKGROUND OBJECTIVES/METHODS/RESULTS/CONCLUSIONS OBJECTIVES/RESULTS
D02909,"A new wave of decision-support systems are being built today using AI services that draw insights from data (like text and video) and incorporate them in human-in-the-loop assistance.$$$However, just as we expect humans to be ethical, the same expectation needs to be met by automated systems that increasingly get delegated to act on their behalf.$$$A very important aspect of an ethical behavior is to avoid (intended, perceived, or accidental) bias.$$$Bias occurs when the data distribution is not representative enough of the natural phenomenon one wants to model and reason about.$$$The possibly biased behavior of a service is hard to detect and handle if the AI service is merely being used and not developed from scratch, since the training data set is not available.$$$In this situation, we envisage a 3rd party rating agency that is independent of the API producer or consumer and has its own set of biased and unbiased data, with customizable distributions.$$$We propose a 2-step rating approach that generates bias ratings signifying whether the AI service is unbiased compensating, data-sensitive biased, or biased.$$$The approach also works on composite services.$$$We implement it in the context of text translation and report interesting results.",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES METHODS OBJECTIVES METHODS CONCLUSIONS RESULTS
D05657,"Understanding the world around us and making decisions about the future is a critical component to human intelligence.$$$As autonomous systems continue to develop, their ability to reason about the future will be the key to their success.$$$Semantic anticipation is a relatively under-explored area for which autonomous vehicles could take advantage of (e.g., forecasting pedestrian trajectories).$$$Motivated by the need for real-time prediction in autonomous systems, we propose to decompose the challenging semantic forecasting task into two subtasks: current frame segmentation and future optical flow prediction.$$$Through this decomposition, we built an efficient, effective, low overhead model with three main components: flow prediction network, feature-flow aggregation LSTM, and end-to-end learnable warp layer.$$$Our proposed method achieves state-of-the-art accuracy on short-term and moving objects semantic forecasting while simultaneously reducing model parameters by up to 95% and increasing efficiency by greater than 40x.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/METHODS METHODS RESULTS/CONCLUSIONS
D01375,"The first cluster-based public computing for Monte Carlo simulation in Indonesia is introduced.$$$The system has been developed to enable public to perform Monte Carlo simulation on a parallel computer through an integrated and user friendly dynamic web interface.$$$The beta version, so called publicMC@BATAN, has been released and implemented for internal users at the National Nuclear Energy Agency (BATAN).$$$In this paper the concept and architecture of publicMC@BATAN are presented.",RESULTS CONCLUSIONS OTHERS METHODS
D04029,"The set covering problem (SCP) is one of the representative combinatorial optimization problems, having many practical applications.$$$This paper investigates the development of an algorithm to solve SCP by employing chemical reaction optimization (CRO), a general-purpose metaheuristic.$$$It is tested on a wide range of benchmark instances of SCP.$$$The simulation results indicate that this algorithm gives outstanding performance compared with other heuristics and metaheuristics in solving SCP.",BACKGROUND OBJECTIVES/METHODS OBJECTIVES RESULTS
D03883,"Genetic algorithms are considered as an original way to solve problems, probably because of their generality and of their ""blind"" nature.$$$But GAs are also unusual since the features of many implementations (among all that could be thought of) are principally led by the biological metaphor, while efficiency measurements intervene only afterwards.$$$We propose here to examine the relevance of these biomimetic aspects, by pointing out some fundamental similarities and divergences between GAs and the genome of living beings shaped by natural selection.$$$One of the main differences comes from the fact that GAs rely principally on the so-called implicit parallelism, while giving to the mutation/selection mechanism the second role.$$$Such differences could suggest new ways of employing GAs on complex problems, using complex codings and starting from nearly homogeneous populations.",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES CONCLUSIONS
D03806,"To understand a node's centrality in a multiplex network, its centrality values in all the layers of the network can be aggregated.$$$This requires a normalization of the values, to allow their meaningful comparison and aggregation over networks with different sizes and orders.$$$The concrete choices of such preprocessing steps like normalization and aggregation are almost never discussed in network analytic papers.$$$In this paper, we show that even sticking to the most simple centrality index (the degree) but using different, classic choices of normalization and aggregation strategies, can turn a node from being among the most central to being among the least central.$$$We present our results by using an aggregation operator which scales between different, classic aggregation strategies based on three multiplex networks.$$$We also introduce a new visualization and characterization of a node's sensitivity to the choice of a normalization and aggregation strategy in multiplex networks.$$$The observed high sensitivity of single nodes to the specific choice of aggregation and normalization strategies is of strong importance, especially for all kinds of intelligence-analytic software as it questions the interpretations of the findings.",BACKGROUND BACKGROUND BACKGROUND RESULTS METHODS RESULTS CONCLUSIONS
D01006,"The blooming availability of traces for social, biological, and communication networks opens up unprecedented opportunities in analyzing diffusion processes in networks.$$$However, the sheer sizes of the nowadays networks raise serious challenges in computational efficiency and scalability.$$$In this paper, we propose a new hyper-graph sketching framework for inflence dynamics in networks.$$$The central of our sketching framework, called SKIS, is an efficient importance sampling algorithm that returns only non-singular reverse cascades in the network.$$$Comparing to previously developed sketches like RIS and SKIM, our sketch significantly enhances estimation quality while substantially reducing processing time and memory-footprint.$$$Further, we present general strategies of using SKIS to enhance existing algorithms for influence estimation and influence maximization which are motivated by practical applications like viral marketing.$$$Using SKIS, we design high-quality influence oracle for seed sets with average estimation error up to 10x times smaller than those using RIS and 6x times smaller than SKIM.$$$In addition, our influence maximization using SKIS substantially improves the quality of solutions for greedy algorithms.$$$It achieves up to 10x times speed-up and 4x memory reduction for the fastest RIS-based DSSA algorithm, while maintaining the same theoretical guarantees.",BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS METHODS METHODS/RESULTS RESULTS RESULTS
D00497,"We study the dynamics of a one dimensional quantum spin chain evolving from unentangled or entangled initial state.$$$At a given instant of time a quantum dynamical process (ex. measurement) is performed on a single spin at one end of the chain, decohering the system.$$$Through the further unitary evolution, a signal propagates in the spin chain, which can be detected from a measurement on a different spin at later times.$$$From the dynamical unitary evolution of the decohered state from the epoch time, it is possible to detect the occurrence of the dynamical process.$$$The propagation of the signal for the dynamical process, and the speed of the signal are investigated for various spin models, viz. using the Ising, Heisenberg, and the transverse-XY dynamics.",BACKGROUND OBJECTIVES OBJECTIVES/METHODS OBJECTIVES/METHODS RESULTS
D00409,"Non-rigid structure-from-motion (NRSfM) has so far been mostly studied for recovering 3D structure of a single non-rigid/deforming object.$$$To handle the real world challenging multiple deforming objects scenarios, existing methods either pre-segment different objects in the scene or treat multiple non-rigid objects as a whole to obtain the 3D non-rigid reconstruction.$$$However, these methods fail to exploit the inherent structure in the problem as the solution of segmentation and the solution of reconstruction could not benefit each other.$$$In this paper, we propose a unified framework to jointly segment and reconstruct multiple non-rigid objects.$$$To compactly represent complex multi-body non-rigid scenes, we propose to exploit the structure of the scenes along both temporal direction and spatial direction, thus achieving a spatio-temporal representation.$$$Specifically, we represent the 3D non-rigid deformations as lying in a union of subspaces along the temporal direction and represent the 3D trajectories as lying in the union of subspaces along the spatial direction.$$$This spatio-temporal representation not only provides competitive 3D reconstruction but also outputs robust segmentation of multiple non-rigid objects.$$$The resultant optimization problem is solved efficiently using the Alternating Direction Method of Multipliers (ADMM).$$$Extensive experimental results on both synthetic and real multi-body NRSfM datasets demonstrate the superior performance of our proposed framework compared with the state-of-the-art methods.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS CONCLUSIONS METHODS RESULTS
D01756,"The wide implementation of electronic health record (EHR) systems facilitates the collection of large-scale health data from real clinical settings.$$$Despite the significant increase in adoption of EHR systems, this data remains largely unexplored, but presents a rich data source for knowledge discovery from patient health histories in tasks such as understanding disease correlations and predicting health outcomes.$$$However, the heterogeneity, sparsity, noise, and bias in this data present many complex challenges.$$$This complexity makes it difficult to translate potentially relevant information into machine learning algorithms.$$$In this paper, we propose a computational framework, Patient2Vec, to learn an interpretable deep representation of longitudinal EHR data which is personalized for each patient.$$$To evaluate this approach, we apply it to the prediction of future hospitalizations using real EHR data and compare its predictive performance with baseline methods.$$$Patient2Vec produces a vector space with meaningful structure and it achieves an AUC around 0.799 outperforming baseline methods.$$$In the end, the learned feature importance can be visualized and interpreted at both the individual and population levels to bring clinical insights.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/METHODS METHODS RESULTS CONCLUSIONS
D03902,"Irregular low-density parity check (LDPC) codes are particularly well-suited for transmission schemes that require unequal error protection (UEP) of the transmitted data due to the different connection degrees of its variable nodes.$$$However, this UEP capability is strongly dependent on the connection profile among the protection classes.$$$This paper applies a multi-edge type analysis of LDPC codes for optimizing such connection profile according to the performance requirements of each protection class.$$$This allows the construction of UEP-LDPC codes where the difference between the performance of the protection classes can be adjusted and with an UEP capability that does not vanish as the number of decoding iterations grows.",BACKGROUND BACKGROUND OBJECTIVES/METHODS METHODS/RESULTS/CONCLUSIONS
D03387,"Modern big data frameworks (such as Hadoop and Spark) allow multiple users to do large-scale analysis simultaneously.$$$Typically, users deploy Data-Intensive Workflows (DIWs) for their analytical tasks.$$$These DIWs of different users share many common parts (i.e, 50-80%), which can be materialized to reuse them in future executions.$$$The materialization improves the overall processing time of DIWs and also saves computational resources.$$$Current solutions for materialization store data on Distributed File Systems (DFS) by using a fixed data format.$$$However, a fixed choice might not be the optimal one for every situation.$$$For example, it is well-known that different data fragmentation strategies (i.e., horizontal, vertical or hybrid) behave better or worse according to the access patterns of the subsequent operations.$$$In this paper, we present a cost-based approach which helps deciding the most appropriate storage format in every situation.$$$A generic cost-based storage format selector framework considering the three fragmentation strategies is presented.$$$Then, we use our framework to instantiate cost models for specific Hadoop data formats (namely SequenceFile, Avro and Parquet), and test it with realistic use cases.$$$Our solution gives on average 33% speedup over SequenceFile, 11% speedup over Avro, 32% speedup over Parquet, and overall, it provides upto 25% performance gain.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES OBJECTIVES METHODS METHODS RESULTS RESULTS
D01220,"In this paper, we present a novel deep learning based approach for addressing the problem of interaction recognition from a first person perspective.$$$The proposed approach uses a pair of convolutional neural networks, whose parameters are shared, for extracting frame level features from successive frames of the video.$$$The frame level features are then aggregated using a convolutional long short-term memory.$$$The hidden state of the convolutional long short-term memory, after all the input video frames are processed, is used for classification in to the respective categories.$$$The two branches of the convolutional neural network perform feature encoding on a short time interval whereas the convolutional long short term memory encodes the changes on a longer temporal duration.$$$In our network the spatio-temporal structure of the input is preserved till the very final processing stage.$$$Experimental results show that our method outperforms the state of the art on most recent first person interactions datasets that involve complex ego-motion.$$$In particular, on UTKinect-FirstPerson it competes with methods that use depth image and skeletal joints information along with RGB images, while it surpasses all previous methods that use only RGB images by more than 20% in recognition accuracy.",OBJECTIVES METHODS METHODS METHODS METHODS METHODS RESULTS RESULTS
D02532,"Using time series of US patents per million inhabitants, knowledge-generating cycles can be distinguished.$$$These cycles partly coincide with Kondratieff long waves.$$$The changes in the slopes between them indicate discontinuities in the knowledge-generating paradigms.$$$The knowledge-generating paradigms can be modeled in terms of interacting dimensions (for example, in university-industry-government relations) that set limits to the maximal efficiency of innovation systems.$$$The maximum values of the parameters in the model are of the same order as the regression coefficients of the empirical waves.$$$The mechanism of the increase in the dimensionality is specified as self-organization which leads to the breaking of existing relations into the more diversified structure of a fractal-like network.$$$This breaking can be modeled in analogy to 2D and 3D (Koch) snowflakes.$$$The boost of knowledge generation leads to newly emerging technologies that can be expected to be more diversified and show shorter life cycles than before.$$$Time spans of the knowledge-generating cycles can also be analyzed in terms of Fibonacci numbers.$$$This perspective allows for forecasting expected dates of future possible paradigm changes.$$$In terms of policy implications, this suggests a shift in focus from the manufacturing technologies to developing new organizational technologies and formats of human interactions",BACKGROUND BACKGROUND METHODS METHODS RESULTS METHODS METHODS RESULTS RESULTS RESULTS CONCLUSIONS
D01745,"A simulation model based on parallel systems is established, aiming to explore the relation between the number of submissions and the overall quality of academic journals within a similar discipline under peer review.$$$The model can effectively simulate the submission, review and acceptance behaviors of academic journals, in a distributed manner.$$$According to the simulation experiments, it could possibly happen that the overall standard of academic journals may deteriorate due to excessive submissions.",OBJECTIVES/METHODS RESULTS CONCLUSIONS
D04468,"An event-based state estimation approach for reducing communication in a networked control system is proposed.$$$Multiple distributed sensor-actuator-agents observe a dynamic process and sporadically exchange their measurements and inputs over a bus network.$$$Based on these data, each agent estimates the full state of the dynamic system, which may exhibit arbitrary inter-agent couplings.$$$Local event-based protocols ensure that data is transmitted only when necessary to meet a desired estimation accuracy.$$$This event-based scheme is shown to mimic a centralized Luenberger observer design up to guaranteed bounds, and stability is proven in the sense of bounded estimation errors for bounded disturbances.$$$The stability result extends to the distributed control system that results when the local state estimates are used for distributed feedback control.$$$Simulation results highlight the benefit of the event-based approach over classical periodic ones in reducing communication requirements.",BACKGROUND BACKGROUND BACKGROUND/METHODS METHODS RESULTS RESULTS RESULTS
D04952,"Citizen Broadband Radio Service band (3550 - 3700 GHz) is seen as one of the key frequency bands to enable improvements in performance of wireless broadband and cellular systems.$$$A careful study of interference caused by a secondary cellular communication system coexisting with an incumbent naval radar is required to establish a pragmatic protection distance, which not only protects the incumbent from harmful interference but also increases the spectrum access opportunity for the secondary system.$$$In this context, this paper investigates the co-channel and adjacent channel coexistence of a ship-borne naval radar and a wide-area cellular communication system and presents the analysis of interference caused by downlink transmission in the cellular system on the naval radar for different values of radar protection distance.$$$The results of such analysis suggest that maintaining a protection distance of 30 km from the radar will ensure the required INR protection criterion of -6 dB at the radar receiver with > 0.9 probability, even when the secondary network operates in the same channel as the radar.$$$Novel power control algorithms to assign operating powers to the coexisting cellular devices are also proposed to further reduce the protection distance from radar while still meeting the radar INR protection requirement.",BACKGROUND OBJECTIVES OBJECTIVES RESULTS OBJECTIVES/METHODS
D03427,"Multi-person articulated pose tracking in unconstrained videos is an important while challenging problem.$$$In this paper, going along the road of top-down approaches, we propose a decent and efficient pose tracker based on pose flows.$$$First, we design an online optimization framework to build the association of cross-frame poses and form pose flows (PF-Builder).$$$Second, a novel pose flow non-maximum suppression (PF-NMS) is designed to robustly reduce redundant pose flows and re-link temporal disjoint ones.$$$Extensive experiments show that our method significantly outperforms best-reported results on two standard Pose Tracking datasets by 13 mAP 25 MOTA and 6 mAP 3 MOTA respectively.$$$Moreover, in the case of working on detected poses in individual frames, the extra computation of pose tracker is very minor, guaranteeing online 10FPS tracking.$$$Our source codes are made publicly available(https://github.com/YuliangXiu/PoseFlow).",BACKGROUND METHODS METHODS METHODS RESULTS RESULTS OTHERS
D01298,"Extracting text objects from the PDF images is a challenging problem.$$$The text data present in the PDF images contain certain useful information for automatic annotation, indexing etc.$$$However variations of the text due to differences in text style, font, size, orientation, alignment as well as complex structure make the problem of automatic text extraction extremely difficult and challenging job.$$$This paper presents two techniques under block-based classification.$$$After a brief introduction of the classification methods, two methods were enhanced and results were evaluated.$$$The performance metrics for segmentation and time consumption are tested for both the models.",BACKGROUND OBJECTIVES METHODS OBJECTIVES METHODS CONCLUSIONS
D02679,"The Grey Wolf Optimizer (GWO) is a swarm intelligence meta-heuristic algorithm inspired by the hunting behaviour and social hierarchy of grey wolves in nature.$$$This paper analyses the use of chaos theory in this algorithm to improve its ability to escape local optima by replacing the key parameters by chaotic variables.$$$The optimal choice of chaotic maps is then used to apply the Chaotic Grey Wolf Optimizer (CGWO) to the problem of factoring a large semi prime into its prime factors.$$$Assuming the number of digits of the factors to be equal, this is a computationally difficult task upon which the RSA-cryptosystem relies.$$$This work proposes the use of a new objective function to solve the problem and uses the CGWO to optimize it and compute the factors.$$$It is shown that this function performs better than its predecessor for large semi primes and CGWO is an efficient algorithm to optimize it.",BACKGROUND OBJECTIVES/METHODS METHODS BACKGROUND OBJECTIVES CONCLUSIONS
D02433,"In this paper, we present an improved feedforward sequential memory networks (FSMN) architecture, namely Deep-FSMN (DFSMN), by introducing skip connections between memory blocks in adjacent layers.$$$These skip connections enable the information flow across different layers and thus alleviate the gradient vanishing problem when building very deep structure.$$$As a result, DFSMN significantly benefits from these skip connections and deep structure.$$$We have compared the performance of DFSMN to BLSTM both with and without lower frame rate (LFR) on several large speech recognition tasks, including English and Mandarin.$$$Experimental results shown that DFSMN can consistently outperform BLSTM with dramatic gain, especially trained with LFR using CD-Phone as modeling units.$$$In the 2000 hours Fisher (FSH) task, the proposed DFSMN can achieve a word error rate of 9.4% by purely using the cross-entropy criterion and decoding with a 3-gram language model, which achieves a 1.5% absolute improvement compared to the BLSTM.$$$In a 20000 hours Mandarin recognition task, the LFR trained DFSMN can achieve more than 20% relative improvement compared to the LFR trained BLSTM.$$$Moreover, we can easily design the lookahead filter order of the memory blocks in DFSMN to control the latency for real-time applications.",METHODS METHODS METHODS METHODS CONCLUSIONS RESULTS RESULTS CONCLUSIONS
D02128,"This paper proposes a novel approach for uncertainty quantification in dense Conditional Random Fields (CRFs).$$$The presented approach, called Perturb-and-MPM, enables efficient, approximate sampling from dense multi-label CRFs via random perturbations.$$$An analytic error analysis was performed which identified the main cause of approximation error as well as showed that the error is bounded.$$$Spatial uncertainty maps can be derived from the Perturb-and-MPM model, which can be used to visualize uncertainty in image segmentation results.$$$The method is validated on synthetic and clinical Magnetic Resonance Imaging data.$$$The effectiveness of the approach is demonstrated on the challenging problem of segmenting the tumor core in glioblastoma.$$$We found that areas of high uncertainty correspond well to wrongly segmented image regions.$$$Furthermore, we demonstrate the potential use of uncertainty maps to refine imaging biomarkers in the case of extent of resection and residual tumor volume in brain tumor patients.",OBJECTIVES METHODS METHODS/RESULTS METHODS/RESULTS METHODS METHODS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS
D02358,"A crucial and time-sensitive task when any disaster occurs is to rescue victims and distribute resources to the right groups and locations.$$$This task is challenging in populated urban areas, due to the huge burst of help requests generated in a very short period.$$$To improve the efficiency of the emergency response in the immediate aftermath of a disaster, we propose a heuristic multi-agent reinforcement learning scheduling algorithm, named as ResQ, which can effectively schedule the rapid deployment of volunteers to rescue victims in dynamic settings.$$$The core concept is to quickly identify victims and volunteers from social network data and then schedule rescue parties with an adaptive learning algorithm.$$$This framework performs two key functions: 1) identify trapped victims and rescue volunteers, and 2) optimize the volunteers' rescue strategy in a complex time-sensitive environment.$$$The proposed ResQ algorithm can speed up the training processes through a heuristic function which reduces the state-action space by identifying the set of particular actions over others.$$$Experimental results showed that the proposed heuristic multi-agent reinforcement learning based scheduling outperforms several state-of-art methods, in terms of both reward rate and response times.",BACKGROUND BACKGROUND OBJECTIVES METHODS BACKGROUND METHODS RESULTS/CONCLUSIONS
D04268,"Researchers spend a great deal of time reading research papers.$$$Keshav (2012) provides a three-pass method to researchers to improve their reading skills.$$$This article extends Keshav's method for reading a research compendium.$$$Research compendia are an increasingly used form of publication, which packages not only the research paper's text and figures, but also all data and software for better reproducibility.$$$We introduce the existing conventions for research compendia and suggest how to utilise their shared properties in a structured reading process.$$$Unlike the original, this article is not build upon a long history but intends to provide guidance at the outset of an emerging practice.",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES METHODS/RESULTS CONCLUSIONS
D05062,"A heterogeneous resource, such as a land-estate, is already divided among several agents in an unfair way.$$$It should be re-divided among the agents in a way that balances fairness with ownership rights.$$$We present re-division protocols that attain various trade-off points between fairness and ownership rights, in various settings differing in the geometric constraints on the allotments: (a) no geometric constraints; (b) connectivity --- the cake is a one-dimensional interval and each piece must be a contiguous interval; (c) rectangularity --- the cake is a two-dimensional rectangle or rectilinear polygon and the pieces should be rectangles; (d) convexity --- the cake is a two-dimensional convex polygon and the pieces should be convex.$$$Our re-division protocols have implications on another problem: the price-of-fairness --- the loss of social welfare caused by fairness requirements.$$$Each protocol implies an upper bound on the price-of-fairness with the respective geometric constraints.",OBJECTIVES OBJECTIVES METHODS/RESULTS RESULTS RESULTS
D06765,"Generative models with an encoding component such as autoencoders currently receive great interest.$$$However, training of autoencoders is typically complicated by the need to train a separate encoder and decoder model that have to be enforced to be reciprocal to each other.$$$To overcome this problem, by-design reversible neural networks (RevNets) had been previously used as generative models either directly optimizing the likelihood of the data under the model or using an adversarial approach on the generated data.$$$Here, we instead investigate their performance using an adversary on the latent space in the adversarial autoencoder framework.$$$We investigate the generative performance of RevNets on the CelebA dataset, showing that generative RevNets can generate coherent faces with similar quality as Variational Autoencoders.$$$This first attempt to use RevNets inside the adversarial autoencoder framework slightly underperformed relative to recent advanced generative models using an autoencoder component on CelebA, but this gap may diminish with further optimization of the training setup of generative RevNets.$$$In addition to the experiments on CelebA, we show a proof-of-principle experiment on the MNIST dataset suggesting that adversary-free trained RevNets can discover meaningful latent dimensions without pre-specifying the number of dimensions of the latent sampling distribution.$$$In summary, this study shows that RevNets can be employed in different generative training settings.$$$Source code for this study is at https://github.com/robintibor/generative-reversible",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/METHODS METHODS/RESULTS RESULTS/CONCLUSIONS RESULTS CONCLUSIONS OTHERS
D03419,"In this paper, we present the system we have used for the Implicit WASSA 2018 Implicit Emotion Shared Task.$$$The task is to predict the emotion of a tweet of which the explicit mentions of emotion terms have been removed.$$$The idea is to come up with a model which has the ability to implicitly identify the emotion expressed given the context words.$$$We have used a Gated Recurrent Neural Network (GRU) and a Capsule Network based model for the task.$$$Pre-trained word embeddings have been utilized to incorporate contextual knowledge about words into the model.$$$GRU layer learns latent representations using the input word embeddings.$$$Subsequent Capsule Network layer learns high-level features from that hidden representation.$$$The proposed model managed to achieve a macro-F1 score of 0.692.",OTHERS OBJECTIVES OBJECTIVES METHODS BACKGROUND METHODS METHODS/CONCLUSIONS RESULTS
D05134,"Natural Language Interfaces and tools such as spellcheckers and Web search in one's own language are known to be useful in ICT-mediated communication.$$$Most languages in Southern Africa are under-resourced, however.$$$Therefore, it would be very useful if both the generic and the few language-specific NLP tools could be reused or easily adapted across languages.$$$This depends on the notion, and extent, of similarity between the languages.$$$We assess this from the angle of orthography and corpora.$$$Twelve versions of the Universal Declaration of Human Rights (UDHR) are examined, showing clusters of languages, and which are thus more or less amenable to cross-language adaptation of NLP tools, which do not match with Guthrie zones.$$$To examine the generalisability of these results, we zoom in on isiZulu both quantitatively and qualitatively with four other corpora and texts in different genres.$$$The results show that the UDHR is a typical text document orthographically.$$$The results also provide insight into usability of typical measures such as lexical diversity and genre, and that the same statistic may mean different things in different documents.$$$While NLTK for Python could be used for basic analyses of text, it, and similar NLP tools, will need considerable customization.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES METHODS METHODS RESULTS RESULTS RESULTS
D04435,"There are several distinct failure modes for overoptimization of systems on the basis of metrics.$$$This occurs when a metric which can be used to improve a system is used to an extent that further optimization is ineffective or harmful, and is sometimes termed Goodhart's Law.$$$This class of failure is often poorly understood, partly because terminology for discussing them is ambiguous, and partly because discussion using this ambiguous terminology ignores distinctions between different failure modes of this general type.$$$This paper expands on an earlier discussion by Garrabrant, which notes there are ""(at least) four different mechanisms"" that relate to Goodhart's Law.$$$This paper is intended to explore these mechanisms further, and specify more clearly how they occur.$$$This discussion should be helpful in better understanding these types of failures in economic regulation, in public policy, in machine learning, and in Artificial Intelligence alignment.$$$The importance of Goodhart effects depends on the amount of power directed towards optimizing the proxy, and so the increased optimization power offered by artificial intelligence makes it especially critical for that field.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES RESULTS RESULTS
D03309,"Reliable diagnosis of depressive disorder is essential for both optimal treatment and prevention of fatal outcomes.$$$In this study, we aimed to elucidate the effectiveness of two non-linear measures, Higuchi Fractal Dimension (HFD) and Sample Entropy (SampEn), in detecting depressive disorders when applied on EEG.$$$HFD and SampEn of EEG signals were used as features for seven machine learning algorithms including Multilayer Perceptron, Logistic Regression, Support Vector Machines with the linear and polynomial kernel, Decision Tree, Random Forest, and Naive Bayes classifier, discriminating EEG between healthy control subjects and patients diagnosed with depression.$$$We confirmed earlier observations that both non-linear measures can discriminate EEG signals of patients from healthy control subjects.$$$The results suggest that good classification is possible even with a small number of principal components.$$$Average accuracy among classifiers ranged from 90.24% to 97.56%.$$$Among the two measures, SampEn had better performance.$$$Using HFD and SampEn and a variety of machine learning techniques we can accurately discriminate patients diagnosed with depression vs controls which can serve as a highly sensitive, clinically relevant marker for the diagnosis of depressive disorders.",BACKGROUND OBJECTIVES METHODS RESULTS CONCLUSIONS RESULTS RESULTS CONCLUSIONS
D06466,"The design of robotic systems is largely dictated by our purely human intuition about how we perceive the world.$$$This intuition has been proven incorrect with regard to a number of critical issues, such as visual change blindness.$$$In order to develop truly autonomous robots, we must step away from this intuition and let robotic agents develop their own way of perceiving.$$$The robot should start from scratch and gradually develop perceptual notions, under no prior assumptions, exclusively by looking into its sensorimotor experience and identifying repetitive patterns and invariants.$$$One of the most fundamental perceptual notions, space, cannot be an exception to this requirement.$$$In this paper we look into the prerequisites for the emergence of simplified spatial notions on the basis of a robot's sensorimotor flow.$$$We show that the notion of space as environment-independent cannot be deduced solely from exteroceptive information, which is highly variable and is mainly determined by the contents of the environment.$$$The environment-independent definition of space can be approached by looking into the functions that link the motor commands to changes in exteroceptive inputs.$$$In a sufficiently rich environment, the kernels of these functions correspond uniquely to the spatial configuration of the agent's exteroceptors.$$$We simulate a redundant robotic arm with a retina installed at its end-point and show how this agent can learn the configuration space of its retina.$$$The resulting manifold has the topology of the Cartesian product of a plane and a circle, and corresponds to the planar position and orientation of the retina.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES OBJECTIVES/METHODS OBJECTIVES/METHODS METHODS/RESULTS RESULTS
D01408,"The Internet of Things (IoTs) is an evolving new face of technology that provides state of the art services using ubiquitously connected smart objects.$$$These smart objects are capable of sensing, processing, collaborating, communicating the events and provide services.$$$The IoT is a collection of heterogeneous technologies like Sensor, RFID, Communication and nanotechnology.$$$These technologies enable smart objects to identify objects, collect information about their status,communicating the collected information for taking some desired actions.$$$Widespread adaptations of IoT based devices and services raised the ethical challenges for their users.$$$In this paper we highlight ethical challenges raised by IoT and discuss the solutions and methods for encouraging people to properly use these technologies according to Islamic teachings.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES
D01010,"This paper concerns a deep learning approach to relevance ranking in information retrieval (IR).$$$Existing deep IR models such as DSSM and CDSSM directly apply neural networks to generate ranking scores, without explicit understandings of the relevance.$$$According to the human judgement process, a relevance label is generated by the following three steps: 1) relevant locations are detected, 2) local relevances are determined, 3) local relevances are aggregated to output the relevance label.$$$In this paper we propose a new deep learning architecture, namely DeepRank, to simulate the above human judgment process.$$$Firstly, a detection strategy is designed to extract the relevant contexts.$$$Then, a measure network is applied to determine the local relevances by utilizing a convolutional neural network (CNN) or two-dimensional gated recurrent units (2D-GRU).$$$Finally, an aggregation network with sequential integration and term gating mechanism is used to produce a global relevance score.$$$DeepRank well captures important IR characteristics, including exact/semantic matching signals, proximity heuristics, query term importance, and diverse relevance requirement.$$$Experiments on both benchmark LETOR dataset and a large scale clickthrough data show that DeepRank can significantly outperform learning to ranking methods, and existing deep learning methods.",BACKGROUND BACKGROUND BACKGROUND METHODS METHODS METHODS METHODS OBJECTIVES RESULTS
D06152,"The abundance of poorly optimized mobile applications coupled with their increasing centrality in our digital lives make a framework for mobile app optimization an imperative.$$$While tuning strategies for desktop and server applications have a long history, it is difficult to adapt them for use on mobile phones.$$$Reference inputs which trigger behavior similar to a mobile application's typical are hard to construct.$$$For many classes of applications the very concept of typical behavior is nonexistent, each user interacting with the application in very different ways.$$$In contexts like this, optimization strategies need to evaluate their effectiveness against real user input, but doing so online runs the risk of user dissatisfaction when suboptimal optimizations are evaluated.$$$In this paper we present an iterative compiler which employs a novel capture and replay technique in order to collect real user input and use it later to evaluate different transformations offline.$$$The proposed mechanism identifies and stores only the set of memory pages needed to replay the most heavily used functions of the application.$$$At idle periods, this minimal state is combined with different binaries of the application, each one build with different optimizations enabled.$$$Replaying the targeted functions allows us to evaluate the effectiveness of each set of optimizations for the actual way the user interacts with the application.$$$For the BEEBS benchmark suite, our approach was able to improve performance by up to 57%, while keeping the slowdown experienced by the user on average at 0.8%.$$$By focusing only on heavily used functions, we are able to conserve storage space by between two and three orders of magnitude compared to typical capture and replay implementations.",BACKGROUND BACKGROUND OBJECTIVES BACKGROUND/OBJECTIVES BACKGROUND/OBJECTIVES METHODS METHODS METHODS METHODS RESULTS RESULTS
D04130,"This paper introduces a new constraint domain for reasoning about data with uncertainty.$$$It extends convex modeling with the notion of p-box to gain additional quantifiable information on the data whereabouts.$$$Unlike existing approaches, the p-box envelops an unknown probability instead of approximating its representation.$$$The p-box bounds are uniform cumulative distribution functions (cdf) in order to employ linear computations in the probabilistic domain.$$$The reasoning by means of p-box cdf-intervals is an interval computation which is exerted on the real domain then it is projected onto the cdf domain.$$$This operation conveys additional knowledge represented by the obtained probabilistic bounds.$$$The empirical evaluation of our implementation shows that, with minimal overhead, the output solution set realizes a full enclosure of the data along with tighter bounds on its probabilistic distributions.",OBJECTIVES METHODS METHODS METHODS RESULTS RESULTS CONCLUSIONS
D06769,"We took part in the YouTube-8M Video Understanding Challenge hosted on Kaggle, and achieved the 10th place within less than one month's time.$$$In this paper, we present an extensive analysis and solution to the underlying machine-learning problem based on frame-level data, where major challenges are identified and corresponding preliminary methods are proposed.$$$It's noteworthy that, with merely the proposed strategies and uniformly-averaging multi-crop ensemble was it sufficient for us to reach our ranking.$$$We also report the methods we believe to be promising but didn't have enough time to train to convergence.$$$We hope this paper could serve, to some extent, as a review and guideline of the YouTube-8M multi-label video classification benchmark, inspiring future attempts and research.",BACKGROUND/RESULTS OBJECTIVES/METHODS METHODS METHODS CONCLUSIONS
D04992,"The synchronizing word of deterministic automaton is a word in the alphabet of colors (considered as letters) of its edges that maps the automaton to a single state.$$$A coloring of edges of a directed graph is synchronizing if the coloring turns the graph into deterministic finite automaton possessing a synchronizing word.$$$The road coloring problem is a problem of synchronizing coloring of directed finite strongly connected graph with constant outdegree of all its vertices if the greatest common divisor of lengths of all its cycles is one.$$$The problem was posed by Adler, Goodwyn and Weiss over 30 years ago and evoked a noticeable interest among the specialists in theory of graphs, deterministic automata and symbolic dynamics.$$$The problem is described even in ""Wikipedia"" - the popular Internet Encyclopedia.$$$The positive solution of the road coloring problem is presented.",BACKGROUND BACKGROUND OBJECTIVES BACKGROUND CONCLUSIONS RESULTS
D01738,"Mobility Management (MM) techniques have conventionally been centralized in nature, wherein a single network entity has been responsible for handling the mobility related tasks of the mobile nodes attached to the network.$$$However, an exponential growth in network traffic and the number of users has ushered in the concept of providing Mobility Management as a Service (MMaaS) to the wireless nodes attached to the 5G networks.$$$Allowing for on-demand mobility management solutions will not only provide the network with the flexibility that it needs to accommodate the many different use cases that are to be served by future networks, but it will also provide the network with the scalability that is needed alongside the flexibility to serve future networks.$$$And hence, in this paper, a detailed study of MMaaS has been provided, highlighting its benefits and challenges for 5G networks.$$$Additionally, the very important property of granularity of service which is deeply intertwined with the scalability and flexibility requirements of the future wireless networks, and a consequence of MMaaS, has also been discussed in detail.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS
D00757,"The regression test selection problem--selecting a subset of a test-suite given a change--has been studied widely over the past two decades.$$$However, the problem has seen little attention when constrained to high-criticality developments and where a ""safe"" selection of tests need to be chosen.$$$Further, no practical approaches have been presented for the programming language Ada.$$$In this paper, we introduce an approach to solving the selection problem given a combination of both static and dynamic data for a program and a change-set.$$$We present a change impact analysis for Ada that selects the safe set of tests that need to be re-executed to ensure no regressions.$$$We have implemented the approach in the commercial, unit-testing tool VectorCAST, and validated it on a number of open-source examples.$$$On an example of a fully-functioning Ada implementation of a DNS server (IRONSIDES), the experimental results show a 97% reduction in test-case execution.",BACKGROUND OBJECTIVES OBJECTIVES METHODS METHODS RESULTS RESULTS/CONCLUSIONS
D00713,"Research into the stylistic properties of translations is an issue which has received some attention in computational stylistics.$$$Previous work by Rybicki (2006) on the distinguishing of character idiolects in the work of Polish author Henryk Sienkiewicz and two corresponding English translations using Burrow's Delta method concluded that idiolectal differences could be observed in the source texts and this variation was preserved to a large degree in both translations.$$$This study also found that the two translations were also highly distinguishable from one another.$$$Burrows (2002) examined English translations of Juvenal also using the Delta method, results of this work suggest that some translators are more adept at concealing their own style when translating the works of another author whereas other authors tend to imprint their own style to a greater extent on the work they translate.$$$Our work examines the writing of a single author, Norwegian playwright Henrik Ibsen, and these writings translated into both German and English from Norwegian, in an attempt to investigate the preservation of characterization, defined here as the distinctiveness of textual contributions of characters.",BACKGROUND BACKGROUND CONCLUSIONS BACKGROUND OBJECTIVES
D04225,"Diffusion MRI is the modality of choice to study alterations of white matter.$$$In the past years, various works have used diffusion MRI for automatic classification of Alzheimers disease.$$$However, the performances obtained with different approaches are difficult to compare because of variations in components such as input data, participant selection, image preprocessing, feature extraction, feature selection (FS) and cross-validation (CV) procedure.$$$Moreover, these studies are also difficult to reproduce because these different components are not readily available.$$$In a previous work (Samper-Gonzalez et al.$$$2018), we proposed an open-source framework for the reproducible evaluation of AD classification from T1-weighted (T1w) MRI and PET data.$$$In the present paper, we extend this framework to diffusion MRI data.$$$The framework comprises: tools to automatically convert ADNI data into the BIDS standard, pipelines for image preprocessing and feature extraction, baseline classifiers and a rigorous CV procedure.$$$We demonstrate the use of the framework through assessing the influence of diffusion tensor imaging (DTI) metrics (fractional anisotropy - FA, mean diffusivity - MD), feature types, imaging modalities (diffusion MRI or T1w MRI), data imbalance and FS bias.$$$First, voxel-wise features generally gave better performances than regional features.$$$Secondly, FA and MD provided comparable results for voxel-wise features.$$$Thirdly, T1w MRI performed better than diffusion MRI.$$$Fourthly, we demonstrated that using non-nested validation of FS leads to unreliable and over-optimistic results.$$$All the code is publicly available: general-purpose tools have been integrated into the Clinica software (www.clinica.run) and the paper-specific code is available at: https://gitlab.icm-institute.org/aramislab/AD-ML.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS RESULTS RESULTS RESULTS RESULTS OTHERS
D03753,"Prevailing computational tools available to and used by architecture and engineering professionals purport to gather and present thorough and accurate perspectives of the environmental impacts associated with their contributions to the built environment.$$$The presented research of building modeling and analysis software used by the Architecture, Engineering, Construction, and Operations (AECO) industry reveals that many of the most heavily relied-upon industry tools are isolated in functionality, utilize incomplete models and data, and are disruptive to normative design and building optimization workflows.$$$This paper describes the current models and tools, their primary functions and limitations, and presents our concurrent research to develop more advanced models to assess lifetime building energy consumption alongside operating energy use.$$$A series of case studies describes the current state-of-the-art in tools and building energy analysis followed by the research models and novel design and analysis Tool that the Green Scale Research Group has developed in response.$$$A fundamental goal of this effort is to increase the use and efficacy of building impact studies conducted by architects, engineers, and building owners and operators during the building design process.",OBJECTIVES BACKGROUND RESULTS METHODS OBJECTIVES
D06741,"This article addresses an open problem in the area of cognitive systems and architectures: namely the problem of handling (in terms of processing and reasoning capabilities) complex knowledge structures that can be at least plausibly comparable, both in terms of size and of typology of the encoded information, to the knowledge that humans process daily for executing everyday activities.$$$Handling a huge amount of knowledge, and selectively retrieve it ac- cording to the needs emerging in different situational scenarios, is an important aspect of human intelligence.$$$For this task, in fact, humans adopt a wide range of heuristics (Gigerenzer and Todd) due to their bounded rationality (Simon, 1957).$$$In this perspective, one of the re- quirements that should be considered for the design, the realization and the evaluation of intelligent cognitively inspired systems should be represented by their ability of heuristically identify and retrieve, from the general knowledge stored in their artificial Long Term Memory (LTM), that one which is synthetically and contextually relevant.$$$This require- ment, however, is often neglected.$$$Currently, artificial cognitive systems and architectures are not able, de facto, to deal with complex knowledge structures that can be even slightly comparable to the knowledge heuris- tically managed by humans.$$$In this paper I will argue that this is not only a technological problem but also an epistemological one and I will briefly sketch a proposal for a possible solution.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/METHODS
D00062,"Dou Shou Qi is a game in which two players control a number of pieces, each of them aiming to move one of their pieces onto a given square.$$$We implemented an engine for analyzing the game.$$$Moreover, we created a series of endgame tablebases containing all configurations with up to four pieces.$$$These tablebases are the first steps towards theoretically solving the game.$$$Finally, we constructed decision trees based on the endgame tablebases.$$$In this note we report on some interesting patterns.",BACKGROUND METHODS METHODS/RESULTS RESULTS METHODS/RESULTS RESULTS
D03996,"Enterprise software systems make complex interactions with other services in their environment.$$$Developing and testing for production-like conditions is therefore a challenging task.$$$Prior approaches include emulations of the dependency services using either explicit modelling or record-and-replay approaches.$$$Models require deep knowledge of the target services while record-and-replay is limited in accuracy.$$$We present a new technique that improves the accuracy of record-and-replay approaches, without requiring prior knowledge of the services.$$$The approach uses multiple sequence alignment to derive message prototypes from recorded system interactions and a scheme to match incoming request messages against message prototypes to generate response messages.$$$We introduce a modified Needleman-Wunsch algorithm for distance calculation during message matching, wildcards in message prototypes for high variability sections, and entropy-based weightings in distance calculations for increased accuracy.$$$Combined, our new approach has shown greater than 99% accuracy for four evaluated enterprise system messaging protocols.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS RESULTS
D01371,"Difference sets and their generalisations to difference families arise from the study of designs and many other applications.$$$Here we give a brief survey of some of these applications, noting in particular the diverse definitions of difference families and the variations in priorities in constructions.$$$We propose a definition of disjoint difference families that encompasses these variations and allows a comparison of the similarities and disparities.$$$We then focus on two constructions of disjoint difference families arising from frequency hopping sequences and showed that they are in fact the same.$$$We conclude with a discussion of the notion of equivalence for frequency hopping sequences and for disjoint difference families.",BACKGROUND OBJECTIVES OBJECTIVES OBJECTIVES RESULTS
D06558,"This paper introduces a method for predicting the likely behaviors of continuous nonlinear systems in equilibrium in which the input values can vary.$$$The method uses a parameterized equation model and a lower bound on the input joint density to bound the likelihood that some behavior will occur, such as a state variable being inside a given numeric range.$$$Using a bound on the density instead of the density itself is desirable because often the input density's parameters and shape are not exactly known.$$$The new method is called SAB after its basic operations: split the input value space into smaller regions, and then bound those regions' possible behaviors and the probability of being in them.$$$SAB finds rough bounds at first, and then refines them as more time is given.$$$In contrast to other researchers' methods, SAB can (1) find all the possible system behaviors, and indicate how likely they are, (2) does not approximate the distribution of possible outcomes without some measure of the error magnitude, (3) does not use discretized variable values, which limit the events one can find probability bounds for, (4) can handle density bounds, and (5) can handle such criteria as two state variables both being inside a numeric range.",OBJECTIVES METHODS METHODS METHODS RESULTS CONCLUSIONS
D04857,"Conference publications in computer science (CS) have attracted scholarly attention due to their unique status as a main research outlet unlike other science fields where journals are dominantly used for communicating research findings.$$$One frequent research question has been how different conference and journal publications are, considering a paper as a unit of analysis.$$$This study takes an author-based approach to analyze publishing patterns of 517,763 scholars who have ever published both in CS conferences and journals for the last 57 years, as recorded in DBLP.$$$The analysis shows that the majority of CS scholars tend to make their scholarly debut, publish more papers, and collaborate with more coauthors in conferences than in journals.$$$Importantly, conference papers seem to serve as a distinct channel of scholarly communication, not a mere preceding step to journal publications: coauthors and title words of authors across conferences and journals tend not to overlap much.$$$This study corroborates findings of previous studies on this topic from a distinctive perspective and suggests that conference authorship in CS calls for more special attention from scholars and administrators outside CS who have focused on journal publications to mine authorship data and evaluate scholarly performance.",BACKGROUND OBJECTIVES METHODS RESULTS RESULTS CONCLUSIONS
D02518,"Most existing knowledge graphs (KGs) in academic domains suffer from problems of insufficient multi-relational information, name ambiguity and improper data format for large-scale machine processing.$$$In this paper, we present AceKG, a new large-scale KG in academic domain.$$$AceKG not only provides clean academic information, but also offers a large-scale benchmark dataset for researchers to conduct challenging data mining projects including link prediction, community detection and scholar classification.$$$Specifically, AceKG describes 3.13 billion triples of academic facts based on a consistent ontology, including necessary properties of papers, authors, fields of study, venues and institutes, as well as the relations among them.$$$To enrich the proposed knowledge graph, we also perform entity alignment with existing databases and rule-based inference.$$$Based on AceKG, we conduct experiments of three typical academic data mining tasks and evaluate several state-of- the-art knowledge embedding and network representation learning approaches on the benchmark datasets built from AceKG.$$$Finally, we discuss several promising research directions that benefit from AceKG.",BACKGROUND OBJECTIVES METHODS RESULTS METHODS METHODS RESULTS
D01331,"Deep reinforcement learning, and especially the Asynchronous Advantage Actor-Critic algorithm, has been successfully used to achieve super-human performance in a variety of video games.$$$Starcraft II is a new challenge for the reinforcement learning community with the release of pysc2 learning environment proposed by Google Deepmind and Blizzard Entertainment.$$$Despite being a target for several AI developers, few have achieved human level performance.$$$In this project we explain the complexities of this environment and discuss the results from our experiments on the environment.$$$We have compared various architectures and have proved that transfer learning can be an effective paradigm in reinforcement learning research for complex scenarios requiring skill transfer.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS/RESULTS/CONCLUSIONS
D03721,"The paper considers a bidirectional power flow model of the electric vehicles (EVs) in a charging station.$$$The EVs can inject energies by discharging via a Vehicle-to-Grid (V2G) service which can enhance the profits of the charging station.$$$However, frequent charging and discharging degrade battery life.$$$A proper compensation needs to be paid to the users to participate in the V2G service.$$$We propose a menu-based pricing scheme, where the charging station selects a price for each arriving user for the amount of battery utilization, the total energy, and the time (deadline) that the EV will stay.$$$The user can accept one of the contracts or rejects all depending on their utilities.$$$The charging station can serve users using a combination of the renewable energy and the conventional energy bought from the grid.$$$We show that though there exists a profit maximizing price which maximizes the social welfare, it provides no surplus to the users if the charging station is aware of the utilities of the users.$$$If the charging station is not aware of the exact utilities, the social welfare maximizing price may not maximize the expected profit.$$$In fact, it can give a zero profit.$$$We propose a pricing strategy which provides a guaranteed fixed profit to the charging station and it also maximizes the expected profit for a wide range of utility functions.$$$Our analysis shows that when the harvested renewable energy is small the users have higher incentives for the V2G service.$$$We, numerically, show that the charging station's profit and the user's surplus both increase as V2G service is efficiently utilized by the pricing mechanism.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS BACKGROUND RESULTS RESULTS CONCLUSIONS RESULTS CONCLUSIONS CONCLUSIONS
D06923,"Every year, 3 million newborns die within the first month of life.$$$Birth asphyxia and other breathing-related conditions are a leading cause of mortality during the neonatal phase.$$$Current diagnostic methods are too sophisticated in terms of equipment, required expertise, and general logistics.$$$Consequently, early detection of asphyxia in newborns is very difficult in many parts of the world, especially in resource-poor settings.$$$We are developing a machine learning system, dubbed Ubenwa, which enables diagnosis of asphyxia through automated analysis of the infant cry.$$$Deployed via smartphone and wearable technology, Ubenwa will drastically reduce the time, cost and skill required to make accurate and potentially life-saving diagnoses.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/METHODS METHODS/RESULTS/CONCLUSIONS
D05939,"In this paper, we propose a distributed primal-dual algorithm for computation of a generalized Nash equilibrium (GNE) in noncooperative games over network systems.$$$In the considered game, not only each player's local objective function depends on other players' decisions, but also the feasible decision sets of all the players are coupled together with a globally shared affine inequality constraint.$$$Adopting the variational GNE, that is the solution of a variational inequality, as a refinement of GNE, we introduce a primal-dual algorithm that players can use to seek it in a distributed manner.$$$Each player only needs to know its local objective function, local feasible set, and a local block of the affine constraint.$$$Meanwhile, each player only needs to observe the decisions on which its local objective function explicitly depends through the interference graph and share information related to multipliers with its neighbors through a multiplier graph.$$$Through a primal-dual analysis and an augmentation of variables, we reformulate the problem as finding the zeros of a sum of monotone operators.$$$Our distributed primal-dual algorithm is based on forward-backward operator splitting methods.$$$We prove its convergence to the variational GNE for fixed step-sizes under some mild assumptions.$$$Then a distributed algorithm with inertia is also introduced and analyzed for variational GNE seeking.$$$Finally, numerical simulations for network Cournot competition are given to illustrate the algorithm efficiency and performance.",OBJECTIVES BACKGROUND METHODS BACKGROUND METHODS METHODS METHODS RESULTS RESULTS RESULTS
D03998,"Structured prediction is ubiquitous in applications of machine learning such as knowledge extraction and natural language processing.$$$Structure often can be formulated in terms of logical constraints.$$$We consider the question of how to perform efficient active learning in the presence of logical constraints among variables inferred by different classifiers.$$$We propose several methods and provide theoretical results that demonstrate the inappropriateness of employing uncertainty guided sampling, a commonly used active learning method.$$$Furthermore, experiments on ten different datasets demonstrate that the methods significantly outperform alternatives in practice.$$$The results are of practical significance in situations where labeled data is scarce.",BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS RESULTS
D00843,"Semi-supervised node classification in attributed graphs, i.e., graphs with node features, involves learning to classify unlabeled nodes given a partially labeled graph.$$$Label predictions are made by jointly modeling the node and its' neighborhood features.$$$State-of-the-art models for node classification on such attributed graphs use differentiable recursive functions that enable aggregation and filtering of neighborhood information from multiple hops.$$$In this work, we analyze the representation capacity of these models to regulate information from multiple hops independently.$$$From our analysis, we conclude that these models despite being powerful, have limited representation capacity to capture multi-hop neighborhood information effectively.$$$Further, we also propose a mathematically motivated, yet simple extension to existing graph convolutional networks (GCNs) which has improved representation capacity.$$$We extensively evaluate the proposed model, F-GCN on eight popular datasets from different domains.$$$F-GCN outperforms the state-of-the-art models for semi-supervised learning on six datasets while being extremely competitive on the other two.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES RESULTS/CONCLUSIONS METHODS RESULTS RESULTS/CONCLUSIONS
D03461,"Bilateral filters have wide spread use due to their edge-preserving properties.$$$The common use case is to manually choose a parametric filter type, usually a Gaussian filter.$$$In this paper, we will generalize the parametrization and in particular derive a gradient descent algorithm so the filter parameters can be learned from data.$$$This derivation allows to learn high dimensional linear filters that operate in sparsely populated feature spaces.$$$We build on the permutohedral lattice construction for efficient filtering.$$$The ability to learn more general forms of high-dimensional filters can be used in several diverse applications.$$$First, we demonstrate the use in applications where single filter applications are desired for runtime reasons.$$$Further, we show how this algorithm can be used to learn the pairwise potentials in densely connected conditional random fields and apply these to different image segmentation tasks.$$$Finally, we introduce layers of bilateral filters in CNNs and propose bilateral neural networks for the use of high-dimensional sparse data.$$$This view provides new ways to encode model structure into network architectures.$$$A diverse set of experiments empirically validates the usage of general forms of filters.",BACKGROUND BACKGROUND OBJECTIVES/METHODS OBJECTIVES/METHODS METHODS OBJECTIVES/METHODS METHODS/RESULTS METHODS/RESULTS METHODS/RESULTS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS
D00020,"Publishing articles in high-impact English journals is difficult for scholars around the world, especially for non-native English-speaking scholars (NNESs), most of whom struggle with proficiency in English.$$$In order to uncover the differences in English scientific writing between native English-speaking scholars (NESs) and NNESs, we collected a large-scale data set containing more than 150,000 full-text articles published in PLoS between 2006 and 2015.$$$We divided these articles into three groups according to the ethnic backgrounds of the first and corresponding authors, obtained by Ethnea, and examined the scientific writing styles in English from a two-fold perspective of linguistic complexity: (1) syntactic complexity, including measurements of sentence length and sentence complexity; and (2) lexical complexity, including measurements of lexical diversity, lexical density, and lexical sophistication.$$$The observations suggest marginal differences between groups in syntactical and lexical complexity.",BACKGROUND OBJECTIVES METHODS CONCLUSIONS
D06217,"Mp3 is a very popular audio format and hence it can be a good host for carrying hidden messages.$$$Therefore, different steganography methods have been proposed for mp3 hosts.$$$But, current literature has only focused on steganalysis of mp3stego.$$$In this paper we mention some of the limitations of mp3stego and argue that UnderMp3Cover (Ump3c) does not have those limitations.$$$Ump3c makes subtle changes only to the global gain of bitstream and keeps the rest of bitstream intact.$$$Therefore, its detection is much harder than mp3stego.$$$To address this, joint distributions between global gain and other fields of mp3 bit stream are used.$$$The changes are detected by measuring the mutual information from those joint distributions.$$$Furthermore, we show that different mp3 encoders have dissimilar performances.$$$Consequently, a novel multi-layer architecture for steganalysis of Ump3c is proposed.$$$In this manner, the first layer detects the encoder and the second layer performs the steganalysis job.$$$One of advantages of this architecture is that feature extraction and feature selection can be optimized for each encoder separately.$$$We show this multi-layer architecture outperforms the conventional single-layer methods.$$$Comparing results of the proposed method with other works shows an improvement of 20.4% in the accuracy of steganalysis.",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES OBJECTIVES OBJECTIVES METHODS METHODS METHODS METHODS METHODS METHODS RESULTS RESULTS
D03308,"While neural networks demonstrate stronger capabilities in pattern recognition nowadays, they are also becoming larger and deeper.$$$As a result, the effort needed to train a network also increases dramatically.$$$In many cases, it is more practical to use a neural network intellectual property (IP) that an IP vendor has already trained.$$$As we do not know about the training process, there can be security threats in the neural IP: the IP vendor (attacker) may embed hidden malicious functionality, i.e. neural Trojans, into the neural IP.$$$We show that this is an effective attack and provide three mitigation techniques: input anomaly detection, re-training, and input preprocessing.$$$All the techniques are proven effective.$$$The input anomaly detection approach is able to detect 99.8% of Trojan triggers although with 12.2% false positive.$$$The re-training approach is able to prevent 94.1% of Trojan triggers from triggering the Trojan although it requires that the neural IP be reconfigurable.$$$In the input preprocessing approach, 90.2% of Trojan triggers are rendered ineffective and no assumption about the neural IP is needed.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND/OBJECTIVES METHODS RESULTS RESULTS RESULTS RESULTS
D01222,"The paper makes a thermal predictive analysis of the electric power system security for a day ahead.$$$This predictive analysis is set as a thermal computation of the expected security.$$$This computation is obtained by cointegrating the daily electric power systen load and the weather, by finding the daily electric power system thermodynamics and by introducing tests for this thermodynamics.$$$The predictive analysis made shows the electricity consumers' wisdom.",OBJECTIVES OBJECTIVES METHODS RESULTS
D01104,"Effective emergency and natural disaster management depend on the efficient mission-critical voice and data communication between first responders and victims.$$$Land Mobile Radio System (LMRS) is a legacy narrowband technology used for critical voice communications with limited use for data applications.$$$Recently Long Term Evolution (LTE) emerged as a broadband communication technology that has a potential to transform the capabilities of public safety technologies by providing broadband, ubiquitous, and mission-critical voice and data support.$$$For example, in the United States, FirstNet is building a nationwide coast-to-coast public safety network based of LTE broadband technology.$$$This paper presents a comparative survey of legacy and the LTE-based public safety networks, and discusses the LMRS-LTE convergence as well as mission-critical push-to-talk over LTE.$$$A simulation study of LMRS and LTE band class 14 technologies is provided using the NS-3 open source tool.$$$An experimental study of APCO-25 and LTE band class 14 is also conducted using software-defined radio, to enhance the understanding of the public safety systems.$$$Finally, emerging technologies that may have strong potential for use in public safety networks are reviewed.",BACKGROUND BACKGROUND CONCLUSIONS BACKGROUND/OBJECTIVES OBJECTIVES METHODS/RESULTS METHODS/RESULTS CONCLUSIONS
D04997,"Frequency control rebalances supply and demand while maintaining the network state within operational margins.$$$It is implemented using fast ramping reserves that are expensive and wasteful, and which are expected to grow with the increasing penetration of renewables.$$$The most promising solution to this problem is the use of demand response, i.e. load participation in frequency control.$$$Yet it is still unclear how to efficiently integrate load participation without introducing instabilities and violating operational constraints.$$$In this paper we present a comprehensive load-side frequency control mechanism that can maintain the grid within operational constraints.$$$In particular, our controllers can rebalance supply and demand after disturbances, restore the frequency to its nominal value and preserve inter-area power flows.$$$Furthermore, our controllers are distributed (unlike the currently implemented frequency control), can allocate load updates optimally, and can maintain line flows within thermal limits.$$$We prove that such a distributed load-side control is globally asymptotically stable and robust to unknown load parameters.$$$We illustrate its effectiveness through simulations.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/RESULTS RESULTS RESULTS RESULTS RESULTS
D04020,"Construction of non-isomorphic cospectral graphs is a nontrivial problem in spectral graph theory specially for large graphs.$$$In this paper, we establish that graph theoretical partial transpose of a graph is a potential tool to create non-isomorphic cospectral graphs by considering a graph as a clustered graph.",BACKGROUND METHODS/RESULTS/CONCLUSIONS
D01303,"We present a hierarchical regression framework for estimating hand joint positions from single depth images based on local surface normals.$$$The hierarchical regression follows the tree structured topology of hand from wrist to finger tips.$$$We propose a conditional regression forest, i.e., the Frame Conditioned Regression Forest (FCRF) which uses a new normal difference feature.$$$At each stage of the regression, the frame of reference is established from either the local surface normal or previously estimated hand joints.$$$By making the regression with respect to the local frame, the pose estimation is more robust to rigid transformations.$$$We also introduce a new efficient approximation to estimate surface normals.$$$We verify the effectiveness of our method by conducting experiments on two challenging real-world datasets and show consistent improvements over previous discriminative pose estimation methods.",OBJECTIVES METHODS METHODS METHODS METHODS METHODS RESULTS
D03571,"This letter investigates joint power control and user clustering for downlink non-orthogonal multiple access systems.$$$Our aim is to minimize the total power consumption by taking into account not only the conventional transmission power but also the decoding power of the users.$$$To solve this optimization problem, it is firstly transformed into an equivalent problem with tractable constraints.$$$Then, an efficient algorithm is proposed to tackle the equivalent problem by using the techniques of reweighted 1-norm minimization and majorization-minimization.$$$Numerical results validate the superiority of the proposed algorithm over the conventional algorithms including the popular matching-based algorithm.",BACKGROUND OBJECTIVES METHODS METHODS CONCLUSIONS
D00643,"Stream processing has reached the mainstream in the last years, as a new generation of open source distributed stream processing systems, designed for scaling horizontally on commodity hardware, has brought the capability for processing high volume and high velocity data streams to companies of all sizes.$$$In this work we propose a combination of temporal logic and property-based testing (PBT) for dealing with the challenges of testing programs that employ this programming model.$$$We formalize our approach in a discrete time temporal logic for finite words, with some additions to improve the expressiveness of properties, which includes timeouts for temporal operators and a binding operator for letters.$$$In particular we focus on testing Spark Streaming programs written with the Spark API for the functional language Scala, using the PBT library ScalaCheck.$$$For that we add temporal logic operators to a set of new ScalaCheck generators and properties, as part of our testing library sscheck.$$$Under consideration in Theory and Practice of Logic Programming (TPLP).",BACKGROUND RESULTS/CONCLUSIONS METHODS/CONCLUSIONS BACKGROUND METHODS OTHERS
D06826,"The present survey aims at presenting the current machine learning techniques employed in security games domains.$$$Specifically, we focused on papers and works developed by the Teamcore of University of Southern California, which deepened different directions in this field.$$$After a brief introduction on Stackelberg Security Games (SSGs) and the poaching setting, the rest of the work presents how to model a boundedly rational attacker taking into account her human behavior, then describes how to face the problem of having attacker's payoffs not defined and how to estimate them and, finally, presents how online learning techniques have been exploited to learn a model of the attacker.",OBJECTIVES OTHERS OTHERS
D06161,"Cyberbullying has emerged as an important and growing social problem, wherein people use online social networks and mobile phones to bully victims with offensive text, images, audio and video on a 247 basis.$$$This paper studies negative user behavior in the Ask.fm social network, a popular new site that has led to many cases of cyberbullying, some leading to suicidal behavior.We examine the occurrence of negative words in Ask.fms question+answer profiles along with the social network of likes of questions+answers.$$$We also examine properties of users with cutting behavior in this social network.",BACKGROUND METHODS METHODS
D00853,"As the bioinformatics field grows, it must keep pace not only with new data but with new algorithms.$$$Here we contribute a thorough analysis of 13 state-of-the-art, commonly used machine learning algorithms on a set of 165 publicly available classification problems in order to provide data-driven algorithm recommendations to current researchers.$$$We present a number of statistical and visual comparisons of algorithm performance and quantify the effect of model selection and algorithm tuning for each algorithm and dataset.$$$The analysis culminates in the recommendation of five algorithms with hyperparameters that maximize classifier performance across the tested problems, as well as general guidelines for applying machine learning to supervised classification problems.",BACKGROUND METHODS RESULTS CONCLUSIONS
D05065,"Distribution network reconfiguration (DNR) is a tool used by operators to balance line load flows and mitigate losses.$$$As distributed generation and flexible load adoption increases, the impact of DNR on the security, efficiency, and reliability of the grid will increase as well.$$$Today, heuristic-based actions like branch exchange are routinely taken, with no theoretical guarantee of their optimality.$$$This paper considers loss minimization via DNR, which changes the on/off status of switches in the network.$$$The goal is to ensure a radial final configuration (called a spanning tree in the algorithms literature) that spans all network buses and connects them to the substation (called the root of the tree) through a single path.$$$We prove that the associated combinatorial optimization problem is strongly NP-hard and thus likely cannot be solved efficiently.$$$We formulate the loss minimization problem as a supermodular function minimization under a single matroid basis constraint, and use existing algorithms to propose a polynomial time local search algorithm for the DNR problem at hand and derive performance bounds.$$$We show that our algorithm is equivalent to the extensively used branch exchange algorithm, for which, to the best of our knowledge, we pioneer in proposing a theoretical performance bound.$$$Finally, we use a 33-bus network to compare our algorithm's performance to several algorithms published in the literature.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES METHODS/RESULTS METHODS/RESULTS RESULTS RESULTS
D01164,"Taking advantage of the rolling shutter effect of CMOS cameras in smartphones is a common practice to increase the transfered data rate with visible light communication (VLC) without employing external equipment such as photodiodes.$$$VLC can then be used as replacement of other marker based techniques for object identification for Augmented Reality and Ubiquitous computing applications.$$$However, the rolling shutter effect only allows to transmit data over a single dimension, which considerably limits the available bandwidth.$$$In this article we propose a new method exploiting spacial interference detection to enable parallel transmission and design a protocol that enables easy identification of interferences between two signals.$$$By introducing a second dimension, we are not only able to significantly increase the available bandwidth, but also identify and isolate light sources in close proximity.",BACKGROUND OBJECTIVES BACKGROUND METHODS METHODS
D00801,"Self-powered, energy harvesting small cell base stations (SBS) are expected to be an integral part of next-generation wireless networks.$$$However, due to uncertainties in harvested energy, it is necessary to adopt energy efficient power control schemes to reduce an SBSs' energy consumption and thus ensure quality-of-service (QoS) for users.$$$Such energy-efficient design can also be done via the use of content caching which reduces the usage of the capacity-limited SBS backhaul. of popular content at SBS can also prove beneficial in this regard by reducing the backhaul usage.$$$In this paper, an online energy efficient power control scheme is developed for an energy harvesting SBS equipped with a wireless backhaul and local storage.$$$In our model, energy arrivals are assumed to be Poisson distributed and the popularity distribution of requested content is modeled using Zipf's law.$$$The power control problem is formulated as a (discounted) infinite horizon dynamic programming problem and solved numerically using the value iteration algorithm.$$$Using simulations, we provide valuable insights on the impact of energy harvesting and caching on the energy and sum-throughput performance of the SBS as the network size is varied.$$$Our results also show that the size of cache and energy harvesting equipment at the SBS can be traded off, while still meeting the desired system performance.",BACKGROUND OBJECTIVES BACKGROUND/OBJECTIVES OBJECTIVES METHODS METHODS RESULTS/CONCLUSIONS CONCLUSIONS
D05438,"Optimization problems are considered in the framework of tropical algebra to minimize and maximize a nonlinear objective function defined on vectors over an idempotent semifield, and calculated using multiplicative conjugate transposition.$$$To find the minimum of the function, we first obtain a partial solution, which explicitly represents a subset of solution vectors.$$$We characterize all solutions by a system of simultaneous equation and inequality, and show that the solution set is closed under vector addition and scalar multiplication.$$$A matrix sparsification technique is proposed to extend the partial solution, and then to obtain a complete solution described as a family of subsets.$$$We offer a backtracking procedure that generates all members of the family, and derive an explicit representation for the complete solution.$$$As another result, we deduce a complete solution of the maximization problem, given in a compact vector form by the use of sparsified matrices.$$$The results obtained are illustrated with illuminating examples and graphical representations.$$$We apply the results to solve real-world problems drawn from project (machine) scheduling, and give numerical examples.",BACKGROUND/OBJECTIVES METHODS METHODS/RESULTS METHODS/RESULTS METHODS/RESULTS RESULTS/CONCLUSIONS RESULTS RESULTS/CONCLUSIONS
D03803,"The Wang tiling is a classical problem in combinatorics.$$$A major theoretical question is to find a (small) set of tiles which tiles the plane only aperiodically.$$$In this case, resulting tilings are rather restrictive.$$$On the other hand, Wang tiles are used as a tool to generate textures and patterns in computer graphics.$$$In these applications, a set of tiles is normally chosen so that it tiles the plane or its sub-regions easily in many different ways.$$$With computer graphics applications in mind, we introduce a class of such tileset, which we call sequentially permissive tilesets, and consider tiling problems with constrained boundary.$$$We apply our methodology to a special set of Wang tiles, called Brick Wang tiles, introduced by Derouet-Jourdan et al. in 2015 to model wall patterns.$$$We generalise their result by providing a linear algorithm to decide and solve the tiling problem for arbitrary planar regions with holes.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS
D04354,"Topological data analysis (TDA) has emerged as one of the most promising techniques to reconstruct the unknown shapes of high-dimensional spaces from observed data samples.$$$TDA, thus, yields key shape descriptors in the form of persistent topological features that can be used for any supervised or unsupervised learning task, including multi-way classification.$$$Sparse sampling, on the other hand, provides a highly efficient technique to reconstruct signals in the spatial-temporal domain from just a few carefully-chosen samples.$$$Here, we present a new method, referred to as the Sparse-TDA algorithm, that combines favorable aspects of the two techniques.$$$This combination is realized by selecting an optimal set of sparse pixel samples from the persistent features generated by a vector-based TDA algorithm.$$$These sparse samples are selected from a low-rank matrix representation of persistent features using QR pivoting.$$$We show that the Sparse-TDA method demonstrates promising performance on three benchmark problems related to human posture recognition and image texture classification.",BACKGROUND BACKGROUND BACKGROUND METHODS METHODS METHODS RESULTS
D04982,"Image quality assessment (IQA) is traditionally classified into full-reference (FR) IQA and no-reference (NR) IQA according to whether the original image is required.$$$Although NR-IQA is widely used in practical applications, room for improvement still remains because of the lack of the reference image.$$$Inspired by the fact that in many applications, such as parameter selection, a series of distorted images are available, the authors propose a novel comparison-based image quality assessment (C-IQA) method.$$$The new comparison-based framework parallels FR-IQA by requiring two input images, and resembles NR-IQA by not using the original image.$$$As a result, the new comparison-based approach has more application scenarios than FR-IQA does, and takes greater advantage of the accessible information than the traditional single-input NR-IQA does.$$$Further, C-IQA is compared with other state-of-the-art NR-IQA methods on two widely used IQA databases.$$$Experimental results show that C-IQA outperforms the other NR-IQA methods for parameter selection, and the parameter trimming framework combined with C-IQA saves the computation of iterative image reconstruction up to 80%.",BACKGROUND BACKGROUND OBJECTIVES METHODS CONCLUSIONS RESULTS RESULTS
D01374,"This paper presents preliminary results of our work with a major financial company, where we try to use methods of plan recognition in order to investigate the interactions of a costumer with the company's online interface.$$$In this paper, we present the first steps of integrating a plan recognition algorithm in a real-world application for detecting and analyzing the interactions of a costumer.$$$It uses a novel approach for plan recognition from bare-bone UI data, which reasons about the plan library at the lowest recognition level in order to define the relevancy of actions in our domain, and then uses it to perform plan recognition.$$$We present preliminary results of inference on three different use-cases modeled by domain experts from the company, and show that this approach manages to decrease the overload of information required from an analyst to evaluate a costumer's session - whether this is a malicious or benign session, whether the intended tasks were completed, and if not - what actions are expected next.",OBJECTIVES OBJECTIVES/METHODS METHODS RESULTS
D00938,"We consider a new Steiner tree problem, called vertex-cover-weighted Steiner tree problem.$$$This problem defines the weight of a Steiner tree as the minimum weight of vertex covers in the tree, and seeks a minimum-weight Steiner tree in a given vertex-weighted undirected graph.$$$Since it is included by the Steiner tree activation problem, the problem admits an O(log n)-approximation algorithm in general graphs with n vertices.$$$This approximation factor is tight up to a constant because it is NP-hard to achieve an o(log n)-approximation for the vertex-cover-weighted Steiner tree problem on general graphs even if the given vertex weights are uniform and a spanning tree is required instead of a Steiner tree.$$$In this paper, we present constant-factor approximation algorithms for the problem with unit disk graphs and with graphs excluding a fixed minor.$$$For the latter graph class, our algorithm can be also applied for the Steiner tree activation problem.",OBJECTIVES BACKGROUND BACKGROUND BACKGROUND RESULTS RESULTS
D02240,"Context: Existing knowledge in agile software development suggests that individual competency (e.g. skills) is a critical success factor for agile projects.$$$While assuming that technical skills are important for every kind of software development project, many researchers suggest that non-technical individual skills are especially important in agile software development.$$$Objective: In this paper, we investigate whether non-technical individual skills can predict the use of agile practices.$$$Method: Through creating a set of multiple linear regression models using a total of 113 participants from agile teams in six software development organizations from The Netherlands and Brazil, we analyzed the predictive power of non-technical individual skills in relation to agile practices.$$$Results: The results show that there is surprisingly low power in using non-technical individual skills to predict (i.e. explain variance in) the mature use of agile practices in software development.$$$Conclusions: Therefore, we conclude that looking at non-technical individual skills is not the optimal level of analysis when trying to understand, and explain, the mature use of agile practices in the software development context.$$$We argue that it is more important to focus on the non-technical skills as a team-level capacity instead of assuring that all individuals possess such skills when understanding the use of the agile practices.",BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS CONCLUSIONS CONCLUSIONS
D00248,"Grid computing is a type of distributed computing which allows sharing of computer resources through Internet.$$$It not only allows us to share files but also most of the software and hardware resources.$$$An efficient resource discovery mechanism is the fundamental requirements for grid computing systems, as it supports resource management and scheduling of applications.$$$Among various discovery mechanisms,Peer-to-Peer (P2P) technology witnessed rapid development and the key component for this success is efficient lookup applications of P2P.$$$Chord is a P2P structural model widely used as a routing protocol to find resources in grid environment.$$$Plenty of ideas are implemented by researchers to improve the lookup performance of chord protocol in Grid environment.$$$In this paper, we discuss the recent researches made on Chord Structured P2P protocol and present our proposed methods in which we use the address of Recently Visited Node (RVN) and fuzzy technique to easily locate the grid resources by reducing message complexity and time complexity.",BACKGROUND BACKGROUND BACKGROUND CONCLUSIONS BACKGROUND OBJECTIVES OBJECTIVES
D01098,"We introduce an exact reformulation of a broad class of neighborhood filters, among which the bilateral filters, in terms of two functional rearrangements: the decreasing and the relative rearrangements.$$$Independently of the image spatial dimension (one-dimensional signal, image, volume of images, etc.$$$), we reformulate these filters as integral operators defined in a one-dimensional space corresponding to the level sets measures.$$$We prove the equivalence between the usual pixel-based version and the rearranged version of the filter.$$$When restricted to the discrete setting, our reformulation of bilateral filters extends previous results for the so-called fast bilateral filtering.$$$We, in addition, prove that the solution of the discrete setting, understood as constant-wise interpolators, converges to the solution of the continuous setting.$$$Finally, we numerically illustrate computational aspects concerning quality approximation and execution time provided by the rearranged formulation.",BACKGROUND METHODS METHODS RESULTS BACKGROUND RESULTS RESULTS
D05604,"Coreference resolution is one of the first stages in deep language understanding and its importance has been well recognized in the natural language processing community.$$$In this paper, we propose a generative, unsupervised ranking model for entity coreference resolution by introducing resolution mode variables.$$$Our unsupervised system achieves 58.44% F1 score of the CoNLL metric on the English data from the CoNLL-2012 shared task (Pradhan et al., 2012), outperforming the Stanford deterministic system (Lee et al., 2013) by 3.01%.",BACKGROUND OBJECTIVES/METHODS RESULTS
D05627,"High altitude platform (HAP) drones can provide broadband wireless connectivity to ground users in rural areas by establishing line-of-sight (LoS) links and exploiting effective beamforming techniques.$$$However, at high altitudes, acquiring the channel state information (CSI) for HAPs, which is a key component to perform beamforming, is challenging.$$$In this paper, by exploiting an interference alignment (IA) technique, a novel method for achieving the maximum sum-rate in HAP-based communications without CSI is proposed.$$$In particular, to realize IA, a multiple-antenna tethered balloon is used as a relay between multiple HAP drones and ground stations (GSs).$$$Here, a multiple-input multiple-output X network system is considered.$$$The capacity of the considered M*N X network with a tethered balloon relay is derived in closed-form.$$$Simulation results corroborate the theoretical findings and show that the proposed approach yields the maximum sum-rate in multiple HAPs-GSs communications in absence of CSI.$$$The results also show the existence of an optimal balloon's altitude for which the sum-rate is maximized.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS RESULTS CONCLUSIONS RESULTS
D05693,"Dynamic ensemble selection (DES) techniques work by estimating the level of competence of each classifier from a pool of classifiers.$$$Only the most competent ones are selected to classify a given test sample.$$$Hence, the key issue in DES is the criterion used to estimate the level of competence of the classifiers in predicting the label of a given test sample.$$$In order to perform a more robust ensemble selection, we proposed the META-DES framework using meta-learning, where multiple criteria are encoded as meta-features and are passed down to a meta-classifier that is trained to estimate the competence level of a given classifier.$$$In this technical report, we present a step-by-step analysis of each phase of the framework during training and test.$$$We show how each set of meta-features is extracted as well as their impact on the estimation of the competence level of the base classifier.$$$Moreover, an analysis of the impact of several factors in the system performance, such as the number of classifiers in the pool, the use of different linear base classifiers, as well as the size of the validation data.$$$We show that using the dynamic selection of linear classifiers through the META-DES framework, we can solve complex non-linear classification problems where other combination techniques such as AdaBoost cannot.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES OBJECTIVES/METHODS RESULTS CONCLUSIONS
D03347,"The quantity of event logs available is increasing rapidly, be they produced by industrial processes, computing systems, or life tracking, for instance.$$$It is thus important to design effective ways to uncover the information they contain.$$$Because event logs often record repetitive phenomena, mining periodic patterns is especially relevant when considering such data.$$$Indeed, capturing such regularities is instrumental in providing condensed representations of the event sequences.$$$We present an approach for mining periodic patterns from event logs while relying on a Minimum Description Length (MDL) criterion to evaluate candidate patterns.$$$Our goal is to extract a set of patterns that suitably characterises the periodic structure present in the data.$$$We evaluate the interest of our approach on several real-world event log datasets.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND METHODS OBJECTIVES METHODS
D05963,"Deep neural network models for Chinese zero pronoun resolution learn semantic information for zero pronoun and candidate antecedents, but tend to be short-sighted---they often make local decisions.$$$They typically predict coreference chains between the zero pronoun and one single candidate antecedent one link at a time, while overlooking their long-term influence on future decisions.$$$Ideally, modeling useful information of preceding potential antecedents is critical when later predicting zero pronoun-candidate antecedent pairs.$$$In this study, we show how to integrate local and global decision-making by exploiting deep reinforcement learning models.$$$With the help of the reinforcement learning agent, our model learns the policy of selecting antecedents in a sequential manner, where useful information provided by earlier predicted antecedents could be utilized for making later coreference decisions.$$$Experimental results on OntoNotes 5.0 dataset show that our technique surpasses the state-of-the-art models.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS RESULTS
D06077,"It is well-known that the precision of data, hyperparameters, and internal representations employed in learning systems directly impacts its energy, throughput, and latency.$$$The precision requirements for the training algorithm are also important for systems that learn on-the-fly.$$$Prior work has shown that the data and hyperparameters can be quantized heavily without incurring much penalty in classification accuracy when compared to floating point implementations.$$$These works suffer from two key limitations.$$$First, they assume uniform precision for the classifier and for the training algorithm and thus miss out on the opportunity to further reduce precision.$$$Second, prior works are empirical studies.$$$In this article, we overcome both these limitations by deriving analytical lower bounds on the precision requirements of the commonly employed stochastic gradient descent (SGD) on-line learning algorithm in the specific context of a support vector machine (SVM).$$$Lower bounds on the data precision are derived in terms of the the desired classification accuracy and precision of the hyperparameters used in the classifier.$$$Additionally, lower bounds on the hyperparameter precision in the SGD training algorithm are obtained.$$$These bounds are validated using both synthetic and the UCI breast cancer dataset.$$$Additionally, the impact of these precisions on the energy consumption of a fixed-point SVM with on-line training is studied.",BACKGROUND BACKGROUND BACKGROUND OTHERS BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS RESULTS RESULTS
D06963,"This is the preprint version of our paper on JOMS.$$$In this paper, two mHealth applications are introduced, which can be employed as the terminals of bigdata based health service to collect information for electronic medical records (EMRs).$$$The first one is a hybrid system for improving the user experience in the hyperbaric oxygen chamber by 3D stereoscopic virtual reality glasses and immersive perception.$$$Several HMDs have been tested and compared.$$$The second application is a voice interactive serious game as a likely solution for providing assistive rehabilitation tool for therapists.$$$The recorder of the voice of patients could be analysed to evaluate the long-time rehabilitation results and further to predict the rehabilitation process.",BACKGROUND OBJECTIVES METHODS METHODS METHODS RESULTS
D01790,"In this letter, we address the symbol synchronization issue in molecular communication via diffusion (MCvD).$$$Symbol synchronization among chemical sensors and nanomachines is one of the critical challenges to manage complex tasks in the nanonetworks with molecular communication (MC).$$$As in diffusion-based MC, most of the molecules arrive at the receptor closer to the start of the symbol duration, the wrong estimation of the start of the symbol interval leads to high symbol detection error.$$$By utilizing two types of molecules with different diffusion coefficients we propose a synchronization technique for MCvD.$$$Moreover, we evaluate the symbol-error-rate performance under the proposed symbol synchronization scheme for equal and non-equal symbol duration MCvD systems.",OBJECTIVES BACKGROUND BACKGROUND METHODS RESULTS
D01554,"This paper proposes a hybrid self-adaptive evolutionary algorithm for graph coloring that is hybridized with the following novel elements: heuristic genotype-phenotype mapping, a swap local search heuristic, and a neutral survivor selection operator.$$$This algorithm was compared with the evolutionary algorithm with the SAW method of Eiben et al., the Tabucol algorithm of Hertz and de Werra, and the hybrid evolutionary algorithm of Galinier and Hao.$$$The performance of these algorithms were tested on a test suite consisting of randomly generated 3-colorable graphs of various structural features, such as graph size, type, edge density, and variability in sizes of color classes.$$$Furthermore, the test graphs were generated including the phase transition where the graphs are hard to color.$$$The purpose of the extensive experimental work was threefold: to investigate the behavior of the tested algorithms in the phase transition, to identify what impact hybridization with the DSatur traditional heuristic has on the evolutionary algorithm, and to show how graph structural features influence the performance of the graph-coloring algorithms.$$$The results indicate that the performance of the hybrid self-adaptive evolutionary algorithm is comparable with, or better than, the performance of the hybrid evolutionary algorithm which is one of the best graph-coloring algorithms today.$$$Moreover, the fact that all the considered algorithms performed poorly on flat graphs confirms that this type of graphs is really the hardest to color.",OBJECTIVES METHODS METHODS METHODS OBJECTIVES RESULTS CONCLUSIONS
D00544,"The distributed computing is done on many systems to solve a large scale problem.$$$The growing of high-speed broadband networks in developed and developing countries, the continual increase in computing power, and the rapid growth of the Internet have changed the way.$$$In it the society manages information and information services.$$$Historically, the state of computing has gone through a series of platform and environmental changes.$$$Distributed computing holds great assurance for using computer systems effectively.$$$As a result, supercomputer sites and data centers have changed from providing high performance floating point computing capabilities to concurrently servicing huge number of requests from billions of users.$$$The distributed computing system uses multiple computers to solve large-scale problems over the Internet.$$$It becomes data-intensive and network-centric.$$$The applications of distributed computing have become increasingly wide-spread.$$$In distributed computing, the main stress is on the large scale resource sharing and always goes for the best performance.$$$In this article, we have reviewed the work done in the area of distributed computing paradigms.$$$The main stress is on the evolving area of cloud computing.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND OTHERS BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND
D04111,"This paper describes a dataset containing small images of text from everyday scenes.$$$The purpose of the dataset is to support the development of new automated systems that can detect and analyze text.$$$Although much research has been devoted to text detection and recognition in scanned documents, relatively little attention has been given to text detection in other types of images, such as photographs that are posted on social-media sites.$$$This new dataset, known as COCO-Text-Patch, contains approximately 354,000 small images that are each labeled as ""text"" or ""non-text"".$$$This dataset particularly addresses the problem of text verification, which is an essential stage in the end-to-end text detection and recognition pipeline.$$$In order to evaluate the utility of this dataset, it has been used to train two deep convolution neural networks to distinguish text from non-text.$$$One network is inspired by the GoogLeNet architecture, and the second one is based on CaffeNet.$$$Accuracy levels of 90.2% and 90.9% were obtained using the two networks, respectively.$$$All of the images, source code, and deep-learning trained models described in this paper will be publicly available",OTHERS CONCLUSIONS BACKGROUND OBJECTIVES OBJECTIVES METHODS METHODS RESULTS CONCLUSIONS
D04958,"Rule-based modelling allows to represent molecular interactions in a compact and natural way.$$$The underlying molecular dynamics, by the laws of stochastic chemical kinetics, behaves as a continuous-time Markov chain.$$$However, this Markov chain enumerates all possible reaction mixtures, rendering the analysis of the chain computationally demanding and often prohibitive in practice.$$$We here describe how it is possible to efficiently find a smaller, aggregate chain, which preserves certain properties of the original one.$$$Formal methods and lumpability notions are used to define algorithms for automated and efficient construction of such smaller chains (without ever constructing the original ones).$$$We here illustrate the method on an example and we discuss the applicability of the method in the context of modelling large signalling pathways.",BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS CONCLUSIONS
D05341,This paper focuses on preserving the privacy of sensitive patterns when inducing decision trees.$$$We adopt a record augmentation approach for hiding sensitive classification rules in binary datasets.$$$Such a hiding methodology is preferred over other heuristic solutions like output perturbation or cryptographic techniques - which restrict the usability of the data - since the raw data itself is readily available for public use.$$$We show some key lemmas which are related to the hiding process and we also demonstrate the methodology with an example and an indicative experiment using a prototype hiding tool.,BACKGROUND/OBJECTIVES METHODS METHODS/RESULTS METHODS/RESULTS
D00473,"The finite element method (FEM) has several computational steps to numerically solve a particular problem, to which many efforts have been directed to accelerate the solution stage of the linear system of equations.$$$However, the finite element matrix construction, which is also time-consuming for unstructured meshes, has been less investigated.$$$The generation of the global finite element matrix is performed in two steps, computing the local matrices by numerical integration and assembling them into a global system, which has traditionally been done in serial computing.$$$This work presents a fast technique to construct the global finite element matrix that arises by solving the Poisson's equation in a three-dimensional domain.$$$The proposed methodology consists in computing the numerical integration, due to its intrinsic parallel opportunities, in the graphics processing unit (GPU) and computing the matrix assembly, due to its intrinsic serial operations, in the central processing unit (CPU).$$$In the numerical integration, only the lower triangular part of each local stiffness matrix is computed thanks to its symmetry, which saves GPU memory and computing time.$$$As a result of symmetry, the global sparse matrix also contains non-zero elements only in its lower triangular part, which reduces the assembly operations and memory usage.$$$This methodology allows generating the global sparse matrix from any unstructured finite element mesh size on GPUs with little memory capacity, only limited by the CPU memory.",BACKGROUND OBJECTIVES METHODS METHODS METHODS METHODS RESULTS CONCLUSIONS
D00229,"To develop a knowledge-aware recommender system, a key data problem is how we can obtain rich and structured knowledge information for recommender system (RS) items.$$$Existing datasets or methods either use side information from original recommender systems (containing very few kinds of useful information) or utilize private knowledge base (KB).$$$In this paper, we present the first public linked KB dataset for recommender systems, named KB4Rec v1.0, which has linked three widely used RS datasets with the popular KB Freebase.$$$Based on our linked dataset, we first preform some interesting qualitative analysis experiments, in which we discuss the effect of two important factors (i.e. popularity and recency) on whether a RS item can be linked to a KB entity.$$$Finally, we present the comparison of several knowledge-aware recommendation algorithms on our linked dataset.",BACKGROUND BACKGROUND OBJECTIVES METHODS OTHERS
D03822,"This paper studies convolutional neural networks (CNN) to learn unsupervised feature representations for 44 different plant species, collected at the Royal Botanic Gardens, Kew, England.$$$To gain intuition on the chosen features from the CNN model (opposed to a 'black box' solution), a visualisation technique based on the deconvolutional networks (DN) is utilized.$$$It is found that venations of different order have been chosen to uniquely represent each of the plant species.$$$Experimental results using these CNN features with different classifiers show consistency and superiority compared to the state-of-the art solutions which rely on hand-crafted features.",BACKGROUND/OBJECTIVES METHODS CONCLUSIONS RESULTS
D05334,"The 2011 Grand Challenge in Service conference aimed to explore, analyse and evaluate complex service systems, utilising a case scenario of delivering on improved perception of safety in the London Borough of Sutton, which provided a common context to link the contributions.$$$The key themes that emerged included value co-creation, systems and networks, ICT and complexity, for which we summarise the contributions.$$$Contributions on value co-creation are based mainly on empirical research and provide a variety of insights including the importance of better understanding collaboration within value co-creation.$$$Contributions on the systems perspective, considered to arise from networks of value co-creation, include efforts to understand the implications of the interactions within service systems, as well as their interactions with social systems, to co-create value.$$$Contributions within the technological sphere, providing ever greater connectivity between entities, focus on the creation of new value constellations and new demand being fulfilled through hybrid offerings of physical assets, information and people.$$$Contributions on complexity, arising from the value co- creation networks of technology enabled services systems, focus on the challenges in understanding, managing and analysing these complex service systems.$$$The theory and applications all show the importance of understanding service for the future.",BACKGROUND/OBJECTIVES METHODS RESULTS RESULTS RESULTS RESULTS RESULTS/CONCLUSIONS
D03529,"Recent technology advancements in the areas of compute, storage and networking, along with the increased demand for organizations to cut costs while remaining responsive to increasing service demands have led to the growth in the adoption of cloud computing services.$$$Cloud services provide the promise of improved agility, resiliency, scalability and a lowered Total Cost of Ownership (TCO).$$$This research introduces a framework for minimizing cost and maximizing resource utilization by using an Integer Linear Programming (ILP) approach to optimize the assignment of workloads to servers on Amazon Web Services (AWS) cloud infrastructure.$$$The model is based on the classical minimum-cost flow model, known as the assignment model.",BACKGROUND BACKGROUND OBJECTIVES/METHODS/RESULTS METHODS
D05071,"In this paper, we propose a convolutional neural network(CNN) with 3-D rank-1 filters which are composed by the outer product of 1-D filters.$$$After being trained, the 3-D rank-1 filters can be decomposed into 1-D filters in the test time for fast inference.$$$The reason that we train 3-D rank-1 filters in the training stage instead of consecutive 1-D filters is that a better gradient flow can be obtained with this setting, which makes the training possible even in the case where the network with consecutive 1-D filters cannot be trained.$$$The 3-D rank-1 filters are updated by both the gradient flow and the outer product of the 1-D filters in every epoch, where the gradient flow tries to obtain a solution which minimizes the loss function, while the outer product operation tries to make the parameters of the filter to live on a rank-1 sub-space.$$$Furthermore, we show that the convolution with the rank-1 filters results in low rank outputs, constraining the final output of the CNN also to live on a low dimensional subspace.",OBJECTIVES RESULTS OBJECTIVES METHODS RESULTS
D03251,"There is no known way of giving a domain-theoretic semantics to higher-order probabilistic languages, in such a way that the involved domains are continuous or quasi-continuous - the latter is required to do any serious mathematics.$$$We argue that the problem naturally disappears for languages with two kinds of types, where one kind is interpreted in a Cartesian-closed category of continuous dcpos, and the other is interpreted in a category that is closed under the probabilistic powerdomain functor.$$$Such a setting is provided by Paul B.$$$Levy's call-by-push-value paradigm.$$$Following this insight, we define a call-by-push-value language, with probabilistic choice sitting inside the value types, and where conversion from a value type to a computation type involves demonic non-determinism.$$$We give both a domain-theoretic semantics and an operational semantics for the resulting language, and we show that they are sound and adequate.$$$With the addition of statistical termination testers and parallel if, we show that the language is even fully abstract - and those two primitives are required for that.",BACKGROUND CONCLUSIONS BACKGROUND BACKGROUND METHODS RESULTS RESULTS
D01806,"We describe the LoopInvGen tool for generating loop invariants that can provably guarantee correctness of a program with respect to a given specification.$$$LoopInvGen is an efficient implementation of the inference technique originally proposed in our earlier work on PIE (https://doi.org/10.1145/2908080.2908099).$$$In contrast to existing techniques, LoopInvGen is not restricted to a fixed set of features -- atomic predicates that are composed together to build complex loop invariants.$$$Instead, we start with no initial features, and use program synthesis techniques to grow the set on demand.$$$This not only enables a less onerous and more expressive approach, but also appears to be significantly faster than the existing tools over the SyGuS-COMP 2017 benchmarks from the INV track.",OBJECTIVES BACKGROUND/OBJECTIVES OBJECTIVES/METHODS METHODS RESULTS/CONCLUSIONS
D06990,"Deep Neural Networks are powerful models that attained remarkable results on a variety of tasks.$$$These models are shown to be extremely efficient when training and test data are drawn from the same distribution.$$$However, it is not clear how a network will act when it is fed with an out-of-distribution example.$$$In this work, we consider the problem of out-of-distribution detection in neural networks.$$$We propose to use multiple semantic dense representations instead of sparse representation as the target label.$$$Specifically, we propose to use several word representations obtained from different corpora or architectures as target labels.$$$We evaluated the proposed model on computer vision, and speech commands detection tasks and compared it to previous methods.$$$Results suggest that our method compares favorably with previous work.$$$Besides, we present the efficiency of our approach for detecting wrongly classified and adversarial examples.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS RESULTS CONCLUSIONS CONCLUSIONS
D04335,"The current work proposes an application of DEA methodology for measurement of technical and allocative efficiency of university research activity.$$$The analysis is based on bibliometric data from the Italian university system for the five year period 2004-2008.$$$Technical and allocative efficiency is measured with input being considered as a university's research staff, classified according to academic rank, and with output considered as the field-standardized impact of the research product realized by these staff.$$$The analysis is applied to all scientific disciplines of the so-called hard sciences, and conducted at subfield level, thus at a greater level of detail than ever before achieved in national-scale research assessments.",OBJECTIVES OTHERS METHODS METHODS
D03150,"Many software development organizations still lack support for obtaining intellectual control over their software development processes and for determining the performance of their processes and the quality of the produced products.$$$Systematic support for detecting and reacting to critical project states in order to achieve planned goals is usually missing.$$$One means to institutionalize measurement on the basis of explicit models is the development and establishment of a so-called Software Project Control Center (SPCC) for systematic quality assurance and management support.$$$An SPCC is comparable to a control room, which is a well known term in the mechanical production domain.$$$Its tasks include collecting, in- terpreting, and visualizing measurement data in order to provide context-, purpose-, and role-oriented information for all stakeholders (e.g., project managers, quality assurance manager, developers) during the execution of a software development project.$$$The article will present an overview of SPCC concepts, a concrete instantiation that supports goal-oriented data visualization (G-SPCC approach), and experiences from practical applications.",BACKGROUND BACKGROUND OBJECTIVES BACKGROUND RESULTS/CONCLUSIONS METHODS
D04657,"Dynamic oracles provide strong supervision for training constituency parsers with exploration, but must be custom defined for a given parser's transition system.$$$We explore using a policy gradient method as a parser-agnostic alternative.$$$In addition to directly optimizing for a tree-level metric such as F1, policy gradient has the potential to reduce exposure bias by allowing exploration during training; moreover, it does not require a dynamic oracle for supervision.$$$On four constituency parsers in three languages, the method substantially outperforms static oracle likelihood training in almost all settings.$$$For parsers where a dynamic oracle is available (including a novel oracle which we define for the transition system of Dyer et al.$$$2016), policy gradient typically recaptures a substantial fraction of the performance gain afforded by the dynamic oracle.",BACKGROUND METHODS OBJECTIVES RESULTS RESULTS RESULTS
D02917,"Although the latent factor model achieves good accuracy in rating prediction, it suffers from many problems including cold-start, non-transparency, and suboptimal results for individual user-item pairs.$$$In this paper, we exploit textual reviews and item images together with ratings to tackle these limitations.$$$Specifically, we first apply a proposed multi-modal aspect-aware topic model (MATM) on text reviews and item images to model users' preferences and items' features from different aspects, and also estimate the aspect importance of a user towards an item.$$$Then the aspect importance is integrated into a novel aspect-aware latent factor model (ALFM), which learns user's and item's latent factors based on ratings.$$$In particular, ALFM introduces a weight matrix to associate those latent factors with the same set of aspects in MATM, such that the latent factors could be used to estimate aspect ratings.$$$Finally, the overall rating is computed via a linear combination of the aspect ratings, which are weighted by the corresponding aspect importance.$$$To this end, our model could alleviate the data sparsity problem and gain good interpretability for recommendation.$$$Besides, every aspect rating is weighted by its aspect importance, which is dependent on the targeted user's preferences and the targeted item's features.$$$Therefore, it is expected that the proposed method can model a user's preferences on an item more accurately for each user-item pair.$$$Comprehensive experimental studies have been conducted on the Yelp 2017 Challenge dataset and Amazon product datasets to demonstrate the effectiveness of our method.",BACKGROUND METHODS METHODS METHODS METHODS METHODS METHODS METHODS METHODS RESULTS
D03627,"It is generally accepted as common wisdom that receiving social feedback is helpful to (i) keep an individual engaged with a community and to (ii) facilitate an individual's positive behavior change.$$$However, quantitative data on the effect of social feedback on continued engagement in an online health community is scarce.$$$In this work we apply Mahalanobis Distance Matching (MDM) to demonstrate the importance of receiving feedback in the ""loseit"" weight loss community on Reddit.$$$Concretely we show that (i) even when correcting for differences in word choice, users receiving more positive feedback on their initial post are more likely to return in the future, and that (ii) there are diminishing returns and social feedback on later posts is less important than for the first post.$$$We also give a description of the type of initial posts that are more likely to attract this valuable social feedback.$$$Though we cannot yet argue about ultimate weight loss success or failure, we believe that understanding the social dynamics underlying online health communities is an important step to devise more effective interventions.",BACKGROUND BACKGROUND OBJECTIVES/METHODS RESULTS RESULTS CONCLUSIONS
D02535,"The paper presents some theoretical and practical considerations regarding the TV information distribution in local (small and medium) networks, using different technologies and architectures.$$$The SMATV concept is chosen to be presented extensively.$$$The most important design formulae are presented with a software package supporting the network planner to design and optimize the network.$$$A case study is realized, using standard components in SMATV, for a 5 floor building.$$$The study proved that it is possible to design and optimize the entire network, without realizing first a costly experimental setup.$$$It is also possible to run different architectures, optimizing also the costs of the final solution of network.",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES/RESULTS CONCLUSIONS CONCLUSIONS
D04321,"For data integration in information ecosystems, semantic heterogeneity is a known difficulty.$$$In this paper, we propose Shadow Theory as the philosophical foundation to address this issue.$$$It is based on the notion of shadows in Plato's Allegory of the Cave.$$$What we can observe are just shadows, and meanings of shadows are mental entities that only exist in viewers' cognitive structures.$$$With enterprise customer data integration example, we proposed six design principles and algebra to support required operations.",BACKGROUND OBJECTIVES BACKGROUND RESULTS METHODS
D03594,"In this paper we present our winning entry at the 2018 ECCV PoseTrack Challenge on 3D human pose estimation.$$$Using a fully-convolutional backbone architecture, we obtain volumetric heatmaps per body joint, which we convert to coordinates using soft-argmax.$$$Absolute person center depth is estimated by a 1D heatmap prediction head.$$$The coordinates are back-projected to 3D camera space, where we minimize the L1 loss.$$$Key to our good results is the training data augmentation with randomly placed occluders from the Pascal VOC dataset.$$$In addition to reaching first place in the Challenge, our method also surpasses the state-of-the-art on the full Human3.6M benchmark among methods that use no additional pose datasets in training.$$$Code for applying synthetic occlusions is availabe at https://github.com/isarandi/synthetic-occlusion.",OBJECTIVES METHODS METHODS METHODS METHODS RESULTS OTHERS
D01671,"Simile is a figure of speech that compares two things through the use of connection words, but where comparison is not intended to be taken literally.$$$They are often used in everyday communication, but they are also a part of linguistic cultural heritage.$$$In this paper we present a methodology for semi-automated collection of similes from the World Wide Web using text mining and machine learning techniques.$$$We expanded an existing corpus by collecting 442 similes from the internet and adding them to the existing corpus collected by Vuk Stefanovic Karadzic that contained 333 similes.$$$We, also, introduce crowdsourcing to the collection of figures of speech, which helped us to build corpus containing 787 unique similes.",BACKGROUND BACKGROUND OBJECTIVES RESULTS RESULTS
D04562,"Probabilistic modeling provides the capability to represent and manipulate uncertainty in data, models, predictions and decisions.$$$We are concerned with the problem of learning probabilistic models of dynamical systems from measured data.$$$Specifically, we consider learning of probabilistic nonlinear state-space models.$$$There is no closed-form solution available for this problem, implying that we are forced to use approximations.$$$In this tutorial we will provide a self-contained introduction to one of the state-of-the-art methods---the particle Metropolis--Hastings algorithm---which has proven to offer a practical approximation.$$$This is a Monte Carlo based method, where the particle filter is used to guide a Markov chain Monte Carlo method through the parameter space.$$$One of the key merits of the particle Metropolis--Hastings algorithm is that it is guaranteed to converge to the ""true solution"" under mild assumptions, despite being based on a particle filter with only a finite number of particles.$$$We will also provide a motivating numerical example illustrating the method using a modeling language tailored for sequential Monte Carlo methods.$$$The intention of modeling languages of this kind is to open up the power of sophisticated Monte Carlo methods---including particle Metropolis--Hastings---to a large group of users without requiring them to know all the underlying mathematical details.",BACKGROUND METHODS METHODS METHODS OBJECTIVES METHODS METHODS RESULTS CONCLUSIONS
D05447,"We consider the computation of Bernoulli, Tangent (zag), and Secant (zig or Euler) numbers.$$$In particular, we give asymptotically fast algorithms for computing the first n such numbers in O(n^2.$$$(log n)^(2+o(1))) bit-operations.$$$We also give very short in-place algorithms for computing the first n Tangent or Secant numbers in O(n^2) integer operations.$$$These algorithms are extremely simple, and fast for moderate values of n. They are faster and use less space than the algorithms of Atkinson (for Tangent and Secant numbers) and Akiyama and Tanigawa (for Bernoulli numbers).",BACKGROUND RESULTS RESULTS RESULTS CONCLUSIONS
D05319,"This paper is concerned with the design of cooperative distributed Model Predictive Control (MPC) for linear systems.$$$Motivated by the special structure of the distributed models in some existing literature, we propose to apply a state transformation to the original system and global cost function.$$$This has major implications on the closed-loop stability analysis and the mechanism of the resultant cooperative framework.$$$It turns out that the proposed framework can be implemented without cooperative iterations being performed in the local optimizations, thus allowing one to compute the local inputs in parallel and independently from each other while requiring only partial plant-wide state information.$$$The proposed framework can also be realized with cooperative iterations, thereby keeping the advantages of the technique in the former reference.$$$Under certain conditions, closed-loop stability for both implementation procedures can be guaranteed a priori by appropriate selections of the original local cost functions.$$$The strengths and benefits of the proposed method are highlighted by means of two numerical examples.",OBJECTIVES METHODS RESULTS RESULTS RESULTS RESULTS OTHERS
D06327,"We address the problem of bootstrapping language acquisition for an artificial system similarly to what is observed in experiments with human infants.$$$Our method works by associating meanings to words in manipulation tasks, as a robot interacts with objects and listens to verbal descriptions of the interactions.$$$The model is based on an affordance network, i.e., a mapping between robot actions, robot perceptions, and the perceived effects of these actions upon objects.$$$We extend the affordance model to incorporate spoken words, which allows us to ground the verbal symbols to the execution of actions and the perception of the environment.$$$The model takes verbal descriptions of a task as the input and uses temporal co-occurrence to create links between speech utterances and the involved objects, actions, and effects.$$$We show that the robot is able form useful word-to-meaning associations, even without considering grammatical structure in the learning process and in the presence of recognition errors.$$$These word-to-meaning associations are embedded in the robot's own understanding of its actions.$$$Thus, they can be directly used to instruct the robot to perform tasks and also allow to incorporate context in the speech recognition task.$$$We believe that the encouraging results with our approach may afford robots with a capacity to acquire language descriptors in their operation's environment as well as to shed some light as to how this challenging process develops with human infants.",OBJECTIVES OBJECTIVES/METHODS METHODS BACKGROUND/METHODS METHODS/RESULTS RESULTS RESULTS RESULTS/CONCLUSIONS CONCLUSIONS
D00032,"One of the most interesting features of Bayesian optimization for direct policy search is that it can leverage priors (e.g., from simulation or from previous tasks) to accelerate learning on a robot.$$$In this paper, we are interested in situations for which several priors exist but we do not know in advance which one fits best the current situation.$$$We tackle this problem by introducing a novel acquisition function, called Most Likely Expected Improvement (MLEI), that combines the likelihood of the priors and the expected improvement.$$$We evaluate this new acquisition function on a transfer learning task for a 5-DOF planar arm and on a possibly damaged, 6-legged robot that has to learn to walk on flat ground and on stairs, with priors corresponding to different stairs and different kinds of damages.$$$Our results show that MLEI effectively identifies and exploits the priors, even when there is no obvious match between the current situations and the priors.",BACKGROUND OBJECTIVES METHODS METHODS RESULTS
D00765,"This paper formulates a time-varying social-welfare maximization problem for distribution grids with distributed energy resources (DERs) and develops online distributed algorithms to identify (and track) its solutions.$$$In the considered setting, network operator and DER-owners pursue given operational and economic objectives, while concurrently ensuring that voltages are within prescribed limits.$$$The proposed algorithm affords an online implementation to enable tracking of the solutions in the presence of time-varying operational conditions and changing optimization objectives.$$$It involves a strategy where the network operator collects voltage measurements throughout the feeder to build incentive signals for the DER-owners in real time; DERs then adjust the generated/consumed powers in order to avoid the violation of the voltage constraints while maximizing given objectives.$$$The stability of the proposed schemes is analytically established and numerically corroborated.",BACKGROUND/OBJECTIVES BACKGROUND/OBJECTIVES METHODS METHODS RESULTS
D00961,"Recurrent neural networks (RNNs) sequentially process data by updating their state with each new data point, and have long been the de facto choice for sequence modeling tasks.$$$However, their inherently sequential computation makes them slow to train.$$$Feed-forward and convolutional architectures have recently been shown to achieve superior results on some sequence modeling tasks such as machine translation, with the added advantage that they concurrently process all inputs in the sequence, leading to easy parallelization and faster training times.$$$Despite these successes, however, popular feed-forward sequence models like the Transformer fail to generalize in many simple tasks that recurrent models handle with ease, e.g.$$$copying strings or even simple logical inference when the string or formula lengths exceed those observed at training time.$$$We propose the Universal Transformer (UT), a parallel-in-time self-attentive recurrent sequence model which can be cast as a generalization of the Transformer model and which addresses these issues.$$$UTs combine the parallelizability and global receptive field of feed-forward sequence models like the Transformer with the recurrent inductive bias of RNNs.$$$We also add a dynamic per-position halting mechanism and find that it improves accuracy on several tasks.$$$In contrast to the standard Transformer, under certain assumptions, UTs can be shown to be Turing-complete.$$$Our experiments show that UTs outperform standard Transformers on a wide range of algorithmic and language understanding tasks, including the challenging LAMBADA language modeling task where UTs achieve a new state of the art, and machine translation where UTs achieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De dataset.",BACKGROUND BACKGROUND/OBJECTIVES BACKGROUND BACKGROUND/OBJECTIVES BACKGROUND/OBJECTIVES OBJECTIVES/METHODS METHODS OBJECTIVES/METHODS CONCLUSIONS RESULTS
D01966,"The combination of aerial survey capabilities of Unmanned Aerial Vehicles with targeted intervention abilities of agricultural Unmanned Ground Vehicles can significantly improve the effectiveness of robotic systems applied to precision agriculture.$$$In this context, building and updating a common map of the field is an essential but challenging task.$$$The maps built using robots of different types show differences in size, resolution and scale, the associated geolocation data may be inaccurate and biased, while the repetitiveness of both visual appearance and geometric structures found within agricultural contexts render classical map merging techniques ineffective.$$$In this paper we propose AgriColMap, a novel map registration pipeline for that leverages a grid-based multi-modal environment representation which includes a vegetation index map and a Digital Surface Model.$$$We cast the data association problem between maps built from UAVs and UGVs as a multi-modal, large displacement dense optical flow estimation.$$$The dominant, coherent flows, selected using a voting scheme, are used as point-to-point correspondences to infer a preliminary non-rigid alignment between the maps.$$$A final refinement is then performed, by exploiting only meaningful parts of the registered maps.$$$We evaluate our system using real world data for 3 fields with different crop species.$$$The results show that our method outperforms several state of the art map registration and matching techniques by a large margin, and has a higher tolerance to large initial misalignments.$$$We release an implementation of the proposed approach along with the acquired datasets with this paper.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS METHODS RESULTS OTHERS
D00539,"This chapter derives the properties of light from the properties of processing, including its ability to be both a wave and a particle, to respond to objects it doesn't physically touch, to take all paths to a destination, to choose a route after it arrives, and to spin both ways at once as it moves.$$$Here a photon is an entity program spreading as a processing wave of instances.$$$It becomes a ""particle"" if any part of it overloads the grid network that runs it, causing the photon program to reboot and restart at a new node.$$$The ""collapse of the wave function"" is how quantum processing creates what we call a physical photon.$$$This informational approach gives insights into issues like the law of least action, entanglement, superposition, counterfactuals, the holographic principle and the measurement problem.$$$The conceptual cost is that physical reality is a quantum processing output, i.e. virtual.",OBJECTIVES CONCLUSIONS CONCLUSIONS CONCLUSIONS CONCLUSIONS CONCLUSIONS
D02891,"Owing to the expeditious growth in the information and communication technologies, smart cities have raised the expectations in terms of efficient functioning and management.$$$One key aspect of residents' daily comfort is assured through affording reliable traffic management and route planning.$$$Comprehensively, the majority of the present trip planning applications and service providers are enabling their trip planning recommendations relying on shortest paths and/or fastest routes.$$$However, such suggestions may discount drivers' preferences with respect to safe and less disturbing trips.$$$Road anomalies such as cracks, potholes, and manholes induce risky driving scenarios and can lead to vehicles damages and costly repairs.$$$Accordingly, in this paper, we propose a crowdsensing based dynamic route planning system.$$$Leveraging both the vehicle motion sensors and the inertial sensors within the smart devices, road surface types and anomalies have been detected and categorized.$$$In addition, the monitored events are geo-referenced utilizing GPS receivers on both vehicles and smart devices.$$$Consequently, road segments assessments are conducted using fuzzy system models based on aspects such as the number of anomalies and their severity levels in each road segment.$$$Afterward, another fuzzy model is adopted to recommend the best trip routes based on the road segments quality in each potential route.$$$Extensive road experiments are held to build and show the potential of the proposed system.",BACKGROUND OBJECTIVES BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS METHODS METHODS
D01085,"The modeling of cascade processes in multi-agent systems in the form of complex networks has in recent years become an important topic of study due to its many applications: the adoption of commercial products, spread of disease, the diffusion of an idea, etc.$$$In this paper, we begin by identifying a desiderata of seven properties that a framework for modeling such processes should satisfy: the ability to represent attributes of both nodes and edges, an explicit representation of time, the ability to represent non-Markovian temporal relationships, representation of uncertain information, the ability to represent competing cascades, allowance of non-monotonic diffusion, and computational tractability.$$$We then present the MANCaLog language, a formalism based on logic programming that satisfies all these desiderata, and focus on algorithms for finding minimal models (from which the outcome of cascades can be obtained) as well as how this formalism can be applied in real world scenarios.$$$We are not aware of any other formalism in the literature that meets all of the above requirements.",BACKGROUND OBJECTIVES METHODS/RESULTS OTHERS
D01600,"Individuals have an intuitive perception of what makes a good coincidence.$$$Though the sensitivity to coincidences has often been presented as resulting from an erroneous assessment of probability, it appears to be a genuine competence, based on non-trivial computations.$$$The model presented here suggests that coincidences occur when subjects perceive complexity drops.$$$Co-occurring events are, together, simpler than if considered separately.$$$This model leads to a possible redefinition of subjective probability.",BACKGROUND BACKGROUND OBJECTIVES RESULTS CONCLUSIONS
D03186,"Stream computation is one of the approaches suitable for FPGA-based custom computing due to its high throughput capability brought by pipelining with regular memory access.$$$To increase performance of iterative stream computation, we can exploit both temporal and spatial parallelism by deepening and duplicating pipelines, respectively.$$$However, the performance is constrained by several factors including available hardware resources on FPGA, an external memory bandwidth, and utilization of pipeline stages, and therefore we need to find the best mix of the different parallelism to achieve the highest performance per power.$$$In this paper, we present a domain-specific language (DSL) based design space exploration for temporally and/or spatially parallel stream computation with FPGA.$$$We define a DSL where we can easily design a hierarchical structure of parallel stream computation with abstract description of computation.$$$For iterative stream computation of fluid dynamics simulation, we design hardware structures with a different mix of the temporal and spatial parallelism.$$$By measuring the performance and the power consumption, we find the best among them.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS
D02576,"The solution of inverse problems in a variational setting finds best estimates of the model parameters by minimizing a cost function that penalizes the mismatch between model outputs and observations.$$$The gradients required by the numerical optimization process are computed using adjoint models.$$$Exponential integrators are a promising family of time discretizations for evolutionary partial differential equations.$$$In order to allow the use of these discretizations in the context of inverse problems adjoints of exponential integrators are required.$$$This work derives the discrete adjoint formulae for a W-type exponential propagation iterative methods of Runge-Kutta type (EPIRK-W).$$$These methods allow arbitrary approximations of the Jacobian while maintaining the overall accuracy of the forward integration.$$$The use of Jacobian approximation matrices that do not depend on the model state avoids the complex calculation of Hessians in the discrete adjoint formulae, and allows efficient adjoint code generation via algorithmic differentiation.$$$We use the discrete EPIRK-W adjoints to solve inverse problems with the Lorenz-96 model and a computational magnetics benchmark test.$$$Numerical results validate our theoretical derivations.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND/OBJECTIVES METHODS/RESULTS METHODS/RESULTS/CONCLUSIONS METHODS/RESULTS/CONCLUSIONS RESULTS CONCLUSIONS
D06519,"It has long been known that certain superquantum nonlocal correlations collapse communication complexity, and it is conjectured that a statement like ""communication complexity is not trivial"" may provide an intuitive information-theoretic axiom for quantum mechanics.$$$With the goal of addressing this conjecture, we take aim at collapsing communication complexity using weaker nonlocal correlations, and present a no-go theorem for a broad class of approaches.$$$To achieve this, we investigate fault-tolerant computation by noisy circuits in a new light.$$$Our main technical result is that, perhaps surprisingly, noiseless XOR gates are not more helpful than noisy ones in read-once formulas that have noisy AND gates for the task of building amplifiers.$$$We also formalize a connection between fault-tolerant computation and amplification, and highlight new directions and open questions in fault-tolerant computation with noisy circuits.$$$Our results inform the relationship between superquantum nonlocality and the collapse of communication complexity.",BACKGROUND OBJECTIVES METHODS RESULTS RESULTS OTHERS
D04033,"Since the advent of deep learning, it has been used to solve various problems using many different architectures.$$$The application of such deep architectures to auditory data is also not uncommon.$$$However, these architectures do not always adequately consider the temporal dependencies in data.$$$We thus propose a new generic architecture called the Deep Belief Network - Bidirectional Long Short-Term Memory (DBN-BLSTM) network that models sequences by keeping track of the temporal information while enabling deep representations in the data.$$$We demonstrate this new architecture by applying it to the task of music generation and obtain state-of-the-art results.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES RESULTS
D02465,"Semi-inner-products in the sense of Lumer are extended to convex functionals.$$$This yields a Hilbert-space like structure to convex functionals in Banach spaces.$$$In particular, a general expression for semi-inner-products with respect to one homogeneous functionals is given.$$$Thus one can use the new operator for the analysis of total variation and higher order functionals like total-generalized-variation (TGV).$$$Having a semi-inner-product, an angle between functions can be defined in a straightforward manner.$$$It is shown that in the one homogeneous case the Bregman distance can be expressed in terms of this newly defined angle.$$$In addition, properties of the semi-inner-product of nonlinear eigenfunctions induced by the functional are derived.$$$We use this construction to state a sufficient condition for a perfect decomposition of two signals and suggest numerical measures which indicate when those conditions are approximately met.",OBJECTIVES OBJECTIVES/RESULTS RESULTS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS RESULTS RESULTS RESULTS
D06598,"The multi-armed bandit (MAB) problem is a sequential allocation task where the goal is to learn a policy that maximizes long term payoff, where only the reward of the executed action is observed; i.e., sequential optimal decisions are made, while simultaneously learning how the world operates.$$$In the stochastic setting, the reward for each action is generated from an unknown distribution.$$$To decide the next optimal action to take, one must compute sufficient statistics of this unknown reward distribution, e.g.$$$upper-confidence bounds (UCB), or expectations in Thompson sampling.$$$Closed-form expressions for these statistics of interest are analytically intractable except for simple cases.$$$We here propose to leverage Monte Carlo estimation and, in particular, the flexibility of (sequential) importance sampling (IS) to allow for accurate estimation of the statistics of interest within the MAB problem.$$$IS methods estimate posterior densities or expectations in probabilistic models that are analytically intractable.$$$We first show how IS can be combined with state-of-the-art MAB algorithms (Thompson sampling and Bayes-UCB) for classic (Bernoulli and contextual linear-Gaussian) bandit problems.$$$Furthermore, we leverage the power of sequential IS to extend the applicability of these algorithms beyond the classic settings, and tackle additional useful cases.$$$Specifically, we study the dynamic linear-Gaussian bandit, and both the static and dynamic logistic cases too.$$$The flexibility of (sequential) importance sampling is shown to be fundamental for obtaining efficient estimates of the key sufficient statistics in these challenging scenarios.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES BACKGROUND METHODS OBJECTIVES RESULTS CONCLUSIONS
D00018,"The success of graph embeddings or node representation learning in a variety of downstream tasks, such as node classification, link prediction, and recommendation systems, has led to their popularity in recent years.$$$Representation learning algorithms aim to preserve local and global network structure by identifying node neighborhood notions.$$$However, many existing algorithms generate embeddings that fail to properly preserve the network structure, or lead to unstable representations due to random processes (e.g., random walks to generate context) and, thus, cannot generate to multi-graph problems.$$$In this paper, we propose RECS, a novel, stable graph embedding algorithmic framework.$$$RECS learns graph representations using connection subgraphs by employing the analogy of graphs with electrical circuits.$$$It preserves both local and global connectivity patterns, and addresses the issue of high-degree nodes.$$$Further, it exploits the strength of weak ties and meta-data that have been neglected by baselines.$$$The experiments show that RECS outperforms state-of-the-art algorithms by up to 36.85% on multi-label classification problem.$$$Further, in contrast to baselines, RECS, being deterministic, is completely stable.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES/METHODS METHODS METHODS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS
D00097,"Information Technology (IT) significantly impacts the environment throughout its life cycle.$$$Most enterprises have not paid enough attention to this until recently.$$$IT's environmental impact can be significantly reduced by behavioral changes, as well as technology changes.$$$Given the relative energy and materials inefficiency of most IT infrastructures today, many green IT initiatives can be easily tackled at no incremental cost.$$$The Green Grid - a non-profit trade organization of IT professionals is such an initiative, formed to initiate the issues of power and cooling in data centers, scattered world-wide.$$$The Green Grid seeks to define best practices for optimizing the efficient consumption of power at IT equipment and facility levels, as well as the manner in which cooling is delivered at these levels hence, providing promising attitude in bringing down the environmental hazards, as well as proceeding to the new era of green computing.$$$In this paper we review the various analytical aspects of The Green Grid upon the data centers and found green facts.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND METHODS/RESULTS
D06514,"Measuring science is based on comparing articles to similar others.$$$However, keyword-based groups of thematically similar articles are dominantly small.$$$These small sizes keep the statistical errors of comparisons high.$$$With the growing availability of bibliographic data such statistical errors can be reduced by merging methods of thematic grouping, citation networks and keyword co-usage.",BACKGROUND BACKGROUND METHODS RESULTS/CONCLUSIONS
D04349,"Regular languages (RL) are the simplest family in Chomsky's hierarchy.$$$Thanks to their simplicity they enjoy various nice algebraic and logic properties that have been successfully exploited in many application fields.$$$Practically all of their related problems are decidable, so that they support automatic verification algorithms.$$$Also, they can be recognized in real-time.$$$Context-free languages (CFL) are another major family well-suited to formalize programming, natural, and many other classes of languages; their increased generative power w.r.t.$$$RL, however, causes the loss of several closure properties and of the decidability of important problems; furthermore they need complex parsing algorithms.$$$Thus, various subclasses thereof have been defined with different goals, spanning from efficient, deterministic parsing to closure properties, logic characterization and automatic verification techniques.$$$Among CFL subclasses, so-called structured ones, i.e., those where the typical tree-structure is visible in the sentences, exhibit many of the algebraic and logic properties of RL, whereas deterministic CFL have been thoroughly exploited in compiler construction and other application fields.$$$After surveying and comparing the main properties of those various language families, we go back to operator precedence languages (OPL), an old family through which R. Floyd pioneered deterministic parsing, and we show that they offer unexpected properties in two fields so far investigated in totally independent ways: they enable parsing parallelization in a more effective way than traditional sequential parsers, and exhibit the same algebraic and logic properties so far obtained only for less expressive language families.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND/OBJECTIVES BACKGROUND BACKGROUND OBJECTIVES/RESULTS/CONCLUSIONS
D01927,"OpenStreetMap offers a valuable source of worldwide geospatial data useful to urban researchers.$$$This study uses the OSMnx software to automatically download and analyze 27,000 US street networks from OpenStreetMap at metropolitan, municipal, and neighborhood scales - namely, every US city and town, census urbanized area, and Zillow-defined neighborhood.$$$It presents empirical findings on US urban form and street network characteristics, emphasizing measures relevant to graph theory, transportation, urban design, and morphology such as structure, connectedness, density, centrality, and resilience.$$$In the past, street network data acquisition and processing have been challenging and ad hoc.$$$This study illustrates the use of OSMnx and OpenStreetMap to consistently conduct street network analysis with extremely large sample sizes, with clearly defined network definitions and extents for reproducibility, and using nonplanar, directed graphs.$$$These street networks and measures data have been shared in a public repository for other researchers to use.",BACKGROUND OBJECTIVES/METHODS RESULTS BACKGROUND RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS
D00157,"Parikh matrices have been extensively investigated due to their usefulness in studying subword occurrences in words.$$$Due to the dependency of Parikh matrices on the ordering of the alphabet, strong M-equivalence was proposed as an order-independent alternative to M-equivalence in studying words possessing the same Parikh matrix.$$$This paper introduces and studies the notions of strong 2.t and strong 3.t transformations in determining when two ternary words are strongly M-equivalent.$$$The irreducibility of strong 2.t transformations are then scrutinized, exemplified by a structural characterization of irreducible strong 2.2 transformations.$$$The common limitation of these transformations in characterizing strong M-equivalence is then addressed.",BACKGROUND BACKGROUND OBJECTIVES/METHODS/RESULTS RESULTS RESULTS
D03102,"Engineering software systems is a multidisciplinary activity, whereby a number of artifacts must be created - and maintained - synchronously.$$$In this paper we investigate whether production code and the accompanying tests co-evolve by exploring a project's versioning system, code coverage reports and size-metrics.$$$Three open source case studies teach us that testing activities usually start later on during the lifetime and are more ""phased"", although we did not observe increasing testing activity before releases.$$$Furthermore, we note large differences in the levels of test coverage given the proportion of test code.",BACKGROUND OBJECTIVES METHODS/CONCLUSIONS CONCLUSIONS
D02485,"Despite the performance advantages of modern sampling-based motion planners, solving high dimensional planning problems in near real-time remains a challenge.$$$Applications include hyper-redundant manipulators, snake-like and humanoid robots.$$$Based on the intuition that many of these problem instances do not require the robots to exercise every degree of freedom independently, we introduce an enhancement to popular sampling-based planning algorithms aimed at circumventing the exponential dependence on dimensionality.$$$We propose beginning the search in a lower dimensional subspace of the configuration space in the hopes that a simple solution will be found quickly.$$$After a certain number of samples are generated, if no solution is found, we increase the dimension of the search subspace by one and continue sampling in the higher dimensional subspace.$$$In the worst case, the search subspace expands to include the full configuration space - making the completeness properties identical to the underlying sampling-based planer.$$$Our experiments comparing the enhanced and traditional version of RRT, RRT-Connect, and BidirectionalT-RRT on both a planar hyper-redundant manipulator and the Baxter humanoid robot indicate that a solution is typically found much faster using this approach and the run time appears to be less sensitive to the dimension of the full configuration space.$$$We explore important implementation issues in the sampling process and discuss its limitations.",BACKGROUND OBJECTIVES OBJECTIVES METHODS METHODS METHODS RESULTS METHODS
D01923,"In contrast to the prevalent assumption of rich multipath in information theoretic analysis of wireless channels, physical channels exhibit sparse multipath, especially at large bandwidths.$$$We propose a model for sparse multipath fading channels and present results on the impact of sparsity on non-coherent capacity and reliability in the wideband regime.$$$A key implication of sparsity is that the statistically independent degrees of freedom in the channel, that represent the delay-Doppler diversity afforded by multipath, scale at a sub-linear rate with the signal space dimension (time-bandwidth product).$$$Our analysis is based on a training-based communication scheme that uses short-time Fourier (STF) signaling waveforms.$$$Sparsity in delay-Doppler manifests itself as time-frequency coherence in the STF domain.$$$From a capacity perspective, sparse channels are asymptotically coherent: the gap between coherent and non-coherent extremes vanishes in the limit of large signal space dimension without the need for peaky signaling.$$$From a reliability viewpoint, there is a fundamental tradeoff between channel diversity and learnability that can be optimized to maximize the error exponent at any rate by appropriately choosing the signaling duration as a function of bandwidth.",BACKGROUND OBJECTIVES METHODS METHODS METHODS RESULTS RESULTS
D05552,"This paper provides a methodology to study the PHY layer vulnerability of wireless protocols in hostile radio environments.$$$Our approach is based on testing the vulnerabilities of a system by analyzing the individual subsystems.$$$By targeting an individual subsystem or a combination of subsystems at a time, we can infer the weakest part and revise it to improve the overall system performance.$$$We apply our methodology to 4G LTE downlink by considering each control channel as a subsystem.$$$We also develop open-source software enabling research and education using software-defined radios.$$$We present experimental results with open-source LTE systems and shows how the different subsystems behave under targeted interference.$$$The analysis for the LTE downlink shows that the synchronization signals (PSS/SSS) are very resilient to interference, whereas the downlink pilots or Cell-Specific Reference signals (CRS) are the most susceptible to a synchronized protocol-aware interferer.$$$We also analyze the severity of control channel attacks for different LTE configurations.$$$Our methodology and tools allow rapid evaluation of the PHY layer reliability in harsh signaling environments, which is an asset to improve current standards and develop new robust wireless protocols.",OBJECTIVES METHODS METHODS METHODS RESULTS RESULTS CONCLUSIONS RESULTS CONCLUSIONS
D00320,"This paper presents a novel mechanism to endogenously determine the fair division of a state into electoral districts in a two-party setting.$$$No geometric constraints are imposed on voter distributions or district shapes; instead, it is assumed that any partition of the population into districts of equal population is feasible.$$$One party divides the map, then the other party chooses a minimum threshold level of support needed to win a district.$$$Districts in which neither party meets this threshold are awarded randomly.$$$Despite the inherent asymmetry, the equilibria of this mechanism always yield fair outcomes, up to integer rounding.",CONCLUSIONS METHODS METHODS METHODS RESULTS
D06349,"Instance-level human parsing towards real-world human analysis scenarios is still under-explored due to the absence of sufficient data resources and technical difficulty in parsing multiple instances in a single pass.$$$Several related works all follow the ""parsing-by-detection"" pipeline that heavily relies on separately trained detection models to localize instances and then performs human parsing for each instance sequentially.$$$Nonetheless, two discrepant optimization targets of detection and parsing lead to suboptimal representation learning and error accumulation for final results.$$$In this work, we make the first attempt to explore a detection-free Part Grouping Network (PGN) for efficiently parsing multiple people in an image in a single pass.$$$Our PGN reformulates instance-level human parsing as two twinned sub-tasks that can be jointly learned and mutually refined via a unified network: 1) semantic part segmentation for assigning each pixel as a human part (e.g., face, arms); 2) instance-aware edge detection to group semantic parts into distinct person instances.$$$Thus the shared intermediate representation would be endowed with capabilities in both characterizing fine-grained parts and inferring instance belongings of each part.$$$Finally, a simple instance partition process is employed to get final results during inference.$$$We conducted experiments on PASCAL-Person-Part dataset and our PGN outperforms all state-of-the-art methods.$$$Furthermore, we show its superiority on a newly collected multi-person parsing dataset (CIHP) including 38,280 diverse images, which is the largest dataset so far and can facilitate more advanced human analysis.$$$The CIHP benchmark and our source code are available at http://sysu-hcp.net/lip/.",BACKGROUND OBJECTIVES OBJECTIVES METHODS METHODS METHODS METHODS RESULTS RESULTS CONCLUSIONS
D06505,"An off-line handwritten alphabetical character recognition system using multilayer feed forward neural network is described in the paper.$$$A new method, called, diagonal based feature extraction is introduced for extracting the features of the handwritten alphabets.$$$Fifty data sets, each containing 26 alphabets written by various people, are used for training the neural network and 570 different handwritten alphabetical characters are used for testing.$$$The proposed recognition system performs quite well yielding higher levels of recognition accuracy compared to the systems employing the conventional horizontal and vertical methods of feature extraction.$$$This system will be suitable for converting handwritten documents into structural text form and recognizing handwritten names.",OBJECTIVES/METHODS OBJECTIVES/METHODS METHODS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS
D05279,"Feature selection has been studied widely in the literature.$$$However, the efficacy of the selection criteria for low sample size applications is neglected in most cases.$$$Most of the existing feature selection criteria are based on the sample similarity.$$$However, the distance measures become insignificant for high dimensional low sample size (HDLSS) data.$$$Moreover, the variance of a feature with a few samples is pointless unless it represents the data distribution efficiently.$$$Instead of looking at the samples in groups, we evaluate their efficiency based on pairwise fashion.$$$In our investigation, we noticed that considering a pair of samples at a time and selecting the features that bring them closer or put them far away is a better choice for feature selection.$$$Experimental results on benchmark data sets demonstrate the effectiveness of the proposed method with low sample size, which outperforms many other state-of-the-art feature selection methods.",BACKGROUND BACKGROUND/OBJECTIVES BACKGROUND BACKGROUND BACKGROUND METHODS METHODS RESULTS/CONCLUSIONS
D01035,"Being able to automatically and quickly understand the user context during a session is a main issue for recommender systems.$$$As a first step toward achieving that goal, we propose a model that observes in real time the diversity brought by each item relatively to a short sequence of consultations, corresponding to the recent user history.$$$Our model has a complexity in constant time, and is generic since it can apply to any type of items within an online service (e.g. profiles, products, music tracks) and any application domain (e-commerce, social network, music streaming), as long as we have partial item descriptions.$$$The observation of the diversity level over time allows us to detect implicit changes.$$$In the long term, we plan to characterize the context, i.e. to find common features among a contiguous sub-sequence of items between two changes of context determined by our model.$$$This will allow us to make context-aware and privacy-preserving recommendations, to explain them to users.$$$As this is an ongoing research, the first step consists here in studying the robustness of our model while detecting changes of context.$$$In order to do so, we use a music corpus of 100 users and more than 210,000 consultations (number of songs played in the global history).$$$We validate the relevancy of our detections by finding connections between changes of context and events, such as ends of session.$$$Of course, these events are a subset of the possible changes of context, since there might be several contexts within a session.$$$We altered the quality of our corpus in several manners, so as to test the performances of our model when confronted with sparsity and different types of items.$$$The results show that our model is robust and constitutes a promising approach.",OBJECTIVES METHODS RESULTS RESULTS CONCLUSIONS CONCLUSIONS METHODS METHODS METHODS METHODS METHODS RESULTS
D00322,"Cause-effect relations are an important part of human knowledge.$$$In real life, humans often reason about complex causes linked to complex effects.$$$By comparison, existing formalisms for representing knowledge about causal relations are quite limited in the kind of specifications of causes and effects they allow.$$$In this paper, we present the new language C-Log, which offers a significantly more expressive representation of effects, including such features as the creation of new objects.$$$We show how C-Log integrates with first-order logic, resulting in the language FO(C).$$$We also compare FO(C) with several related languages and paradigms, including inductive definitions, disjunctive logic programming, business rules and extensions of Datalog.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/RESULTS RESULTS METHODS
D01316,"Software-Defined Networks have seen an increasing in their deployment because they offer better network manageability compared to traditional networks.$$$Despite their immense success and popularity, various security issues in SDN remain open problems for research.$$$Particularly, the problem of securing the controllers in distributed environment is still short of any solutions.$$$This paper proposes a scheme to identify any rogue/malicious controller(s) in a distributed environment.$$$Our scheme is based on trust and reputation system which is centrally managed.$$$As such, our scheme identifies any controllers acting maliciously by comparing the state of installed flows/policies with policies that should be installed.$$$Controllers rate each other on this basis and report the results to a central entity, which reports it to the network administrator.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS RESULTS/CONCLUSIONS
D00426,"We study verifiable sufficient conditions and computable performance bounds for sparse recovery algorithms such as the Basis Pursuit, the Dantzig selector and the Lasso estimator, in terms of a newly defined family of quality measures for the measurement matrices.$$$With high probability, the developed measures for subgaussian random matrices are bounded away from zero as long as the number of measurements is reasonably large.$$$Comparing to the restricted isotropic constant based performance analysis, the arguments in this paper are much more concise and the obtained bounds are tighter.$$$Numerical experiments are presented to illustrate our theoretical results.",METHODS/RESULTS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS METHODS/RESULTS
D01887,"Coverage and connectivity both are important in wireless sensor network (WSN).$$$Coverage means how well an area of interest is being monitored by the deployed network.$$$It depends on sensing model that has been used to design the network model.$$$Connectivity ensures the establishment of a wireless link between two nodes.$$$A link model studies the connectivity between two nodes.$$$The probability of establishing a wireless link between two nodes is a probabilistic phenomenon.$$$The connectivity between two nodes plays an important role in the determination of network connectivity.$$$In this paper, we investigate the impact of sensing model of nodes on the network coverage.$$$Also, we investigate the dependency of the connectivity and coverage on the shadow fading parameters.$$$It has been observed that shadowing effect reduces the network coverage while it enhances connectivity in a multi-hop wireless network.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS/CONCLUSIONS
D05261,"Over the past decade, the study of extrasolar planets has evolved rapidly from plain detection and identification to comprehensive categorization and characterization of exoplanet systems and their atmospheres.$$$Atmospheric retrieval, the inverse modeling technique used to determine an exoplanetary atmosphere's temperature structure and composition from an observed spectrum, is both time-consuming and compute-intensive, requiring complex algorithms that compare thousands to millions of atmospheric models to the observational data to find the most probable values and associated uncertainties for each model parameter.$$$For rocky, terrestrial planets, the retrieved atmospheric composition can give insight into the surface fluxes of gaseous species necessary to maintain the stability of that atmosphere, which may in turn provide insight into the geological and/or biological processes active on the planet.$$$These atmospheres contain many molecules, some of them biosignatures, spectral fingerprints indicative of biological activity, which will become observable with the next generation of telescopes.$$$Runtimes of traditional retrieval models scale with the number of model parameters, so as more molecular species are considered, runtimes can become prohibitively long.$$$Recent advances in machine learning (ML) and computer vision offer new ways to reduce the time to perform a retrieval by orders of magnitude, given a sufficient data set to train with.$$$Here we present an ML-based retrieval framework called Intelligent exoplaNet Atmospheric RetrievAl (INARA) that consists of a Bayesian deep learning model for retrieval and a data set of 3,000,000 synthetic rocky exoplanetary spectra generated using the NASA Planetary Spectrum Generator.$$$Our work represents the first ML retrieval model for rocky, terrestrial exoplanets and the first synthetic data set of terrestrial spectra generated at this scale.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND METHODS/RESULTS CONCLUSIONS
D04615,"In the never-ending quest for tools that enable an ISP to smooth troubleshooting and improve awareness of network behavior, very much effort has been devoted in the collection of data by active and passive measurement at the data plane and at the control plane level.$$$Exploitation of collected data has been mostly focused on anomaly detection and on root-cause analysis.$$$Our objective is somewhat in the middle.$$$We consider traceroutes collected by a network of probes and aim at introducing a practically applicable methodology to quickly spot measurements that are related to high-impact events happened in the network.$$$Such filtering process eases further in- depth human-based analysis, for example with visual tools which are effective only when handling a limited amount of data.$$$We introduce the empathy relation between traceroutes as the cornerstone of our formal characterization of the traceroutes related to a network event.$$$Based on this model, we describe an algorithm that finds traceroutes related to high-impact events in an arbitrary set of measurements.$$$Evidence of the effectiveness of our approach is given by experimental results produced on real-world data.",BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS METHODS METHODS RESULTS
D03451,"Applications such as web search and social networking have been moving from centralized to decentralized cloud architectures to improve their scalability.$$$MapReduce, a programming framework for processing large amounts of data using thousands of machines in a single cloud, also needs to be scaled out to multiple clouds to adapt to this evolution.$$$The challenge of building a multi-cloud distributed architecture is substantial.$$$Notwithstanding, the ability to deal with the new types of faults introduced by such setting, such as the outage of a whole datacenter or an arbitrary fault caused by a malicious cloud insider, increases the endeavor considerably.$$$In this paper we propose Medusa, a platform that allows MapReduce computations to scale out to multiple clouds and tolerate several types of faults.$$$Our solution fulfills four objectives.$$$First, it is transparent to the user, who writes her typical MapReduce application without modification.$$$Second, it does not require any modification to the widely used Hadoop framework.$$$Third, the proposed system goes well beyond the fault-tolerance offered by MapReduce to tolerate arbitrary faults, cloud outages, and even malicious faults caused by corrupt cloud insiders.$$$Fourth, it achieves this increased level of fault tolerance at reasonable cost.$$$We performed an extensive experimental evaluation in the ExoGENI testbed, demonstrating that our solution significantly reduces execution time when compared to traditional methods that achieve the same level of resilience.",BACKGROUND BACKGROUND BACKGROUND/OBJECTIVES BACKGROUND OBJECTIVES OBJECTIVES OBJECTIVES OBJECTIVES OBJECTIVES OBJECTIVES RESULTS
D04697,"Ant Colony System (ACS) is a distributed (agent- based) algorithm which has been widely studied on the Symmetric Travelling Salesman Problem (TSP).$$$The optimum parameters for this algorithm have to be found by trial and error.$$$We use a Particle Swarm Optimization algorithm (PSO) to optimize the ACS parameters working in a designed subset of TSP instances.$$$First goal is to perform the hybrid PSO-ACS algorithm on a single instance to find the optimum parameters and optimum solutions for the instance.$$$Second goal is to analyze those sets of optimum parameters, in relation to instance characteristics.$$$Computational results have shown good quality solutions for single instances though with high computational times, and that there may be sets of parameters that work optimally for a majority of instances.",BACKGROUND BACKGROUND OBJECTIVES METHODS/RESULTS METHODS/RESULTS CONCLUSIONS
D03169,"The variational autoencoder (VAE) is a popular probabilistic generative model.$$$However, one shortcoming of VAEs is that the latent variables cannot be discrete, which makes it difficult to generate data from different modes of a distribution.$$$Here, we propose an extension of the VAE framework that incorporates a classifier to infer the discrete class of the modeled data.$$$To model sequential data, we can combine our Classifying VAE with a recurrent neural network such as an LSTM.$$$We apply this model to algorithmic music generation, where our model learns to generate musical sequences in different keys.$$$Most previous work in this area avoids modeling key by transposing data into only one or two keys, as opposed to the 10+ different keys in the original music.$$$We show that our Classifying VAE and Classifying VAE+LSTM models outperform the corresponding non-classifying models in generating musical samples that stay in key.$$$This benefit is especially apparent when trained on untransposed music data in the original keys.",BACKGROUND OBJECTIVES OBJECTIVES METHODS METHODS BACKGROUND RESULTS/CONCLUSIONS CONCLUSIONS
D01468,"In order to convey the most content in their limited space, advertisements embed references to outside knowledge via symbolism.$$$For example, a motorcycle stands for adventure (a positive property the ad wants associated with the product being sold), and a gun stands for danger (a negative property to dissuade viewers from undesirable behaviors).$$$We show how to use symbolic references to better understand the meaning of an ad.$$$We further show how anchoring ad understanding in general-purpose object recognition and image captioning improves results.$$$We formulate the ad understanding task as matching the ad image to human-generated statements that describe the action that the ad prompts, and the rationale it provides for taking this action.$$$Our proposed method outperforms the state of the art on this task, and on an alternative formulation of question-answering on ads.$$$We show additional applications of our learned representations for matching ads to slogans, and clustering ads according to their topic, without extra training.",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES METHODS RESULTS RESULTS
D00448,"Most brain-computer interfaces (BCIs) based on functional near-infrared spectroscopy (fNIRS) require that users perform mental tasks such as motor imagery, mental arithmetic, or music imagery to convey a message or to answer simple yes or no questions.$$$These cognitive tasks usually have no direct association with the communicative intent, which makes them difficult for users to perform.$$$In this paper, a 3-class intuitive BCI is presented which enables users to directly answer yes or no questions by covertly rehearsing the word 'yes' or 'no' for 15 s. The BCI also admits an equivalent duration of unconstrained rest which constitutes the third discernable task.$$$Twelve participants each completed one offline block and six online blocks over the course of 2 sessions.$$$The mean value of the change in oxygenated hemoglobin concentration during a trial was calculated for each channel and used to train a regularized linear discriminant analysis (RLDA) classifier.$$$By the final online block, 9 out of 12 participants were performing above chance (p<0.001), with a 3-class accuracy of 83.8+9.4%.$$$Even when considering all participants, the average online 3-class accuracy over the last 3 blocks was 64.1+20.6%, with only 3 participants scoring below chance (p<0.001).$$$For most participants, channels in the left temporal and temporoparietal cortex provided the most discriminative information.$$$To our knowledge, this is the first report of an online fNIRS 3-class imagined speech BCI.$$$Our findings suggest that imagined speech can be used as a reliable activation task for selected users for the development of more intuitive BCIs for communication.",BACKGROUND BACKGROUND OBJECTIVES/METHODS METHODS METHODS RESULTS RESULTS RESULTS RESULTS/CONCLUSIONS CONCLUSIONS
D01871,"High performance grid computing is a key enabler of large scale collaborative computational science.$$$With the promise of exascale computing, high performance grid systems are expected to incur electricity bills that grow super-linearly over time.$$$In order to achieve cost effectiveness in these systems, it is essential for the scheduling algorithms to exploit electricity price variations, both in space and time, that are prevalent in the dynamic electricity price markets.$$$In this paper, we present a metascheduling algorithm to optimize the placement of jobs in a compute grid which consumes electricity from the day-ahead wholesale market.$$$We formulate the scheduling problem as a Minimum Cost Maximum Flow problem and leverage queue waiting time and electricity price predictions to accurately estimate the cost of job execution at a system.$$$Using trace based simulation with real and synthetic workload traces, and real electricity price data sets, we demonstrate our approach on two currently operational grids, XSEDE and NorduGrid.$$$Our experimental setup collectively constitute more than 433K processors spread across 58 compute systems in 17 geographically distributed locations.$$$Experiments show that our approach simultaneously optimizes the total electricity cost and the average response time of the grid, without being unfair to users of the local batch systems.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS RESULTS
D06513,"In last decade, data analytics have rapidly progressed from traditional disk-based processing to modern in-memory processing.$$$However, little effort has been devoted at enhancing performance at micro-architecture level.$$$This paper characterizes the performance of in-memory data analytics using Apache Spark framework.$$$We use a single node NUMA machine and identify the bottlenecks hampering the scalability of workloads.$$$We also quantify the inefficiencies at micro-architecture level for various data analysis workloads.$$$Through empirical evaluation, we show that spark workloads do not scale linearly beyond twelve threads, due to work time inflation and thread level load imbalance.$$$Further, at the micro-architecture level, we observe memory bound latency to be the major cause of work time inflation.",BACKGROUND BACKGROUND OBJECTIVES METHODS OBJECTIVES RESULTS RESULTS/CONCLUSIONS
D03264,"3D image processing constitutes nowadays a challenging topic in many scientific fields such as medicine, computational physics and informatics.$$$Therefore, development of suitable tools that guaranty a best treatment is a necessity.$$$Spherical shapes are a big class of 3D images whom processing necessitates adoptable tools.$$$This encourages researchers to develop spherical wavelets and spherical harmonics as special mathematical bases able for 3D spherical shapes.$$$The present work lies in the whole topic of 3D image processing with the special spherical harmonics bases.$$$A spherical harmonics based approach is proposed for the reconstruction of images provided with spherical harmonics Shannon-type entropy to evaluate the order/disorder of the reconstructed image.$$$Efficiency and accuracy of the approach is demonstrated by a simulation study on several spherical models.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS RESULTS/CONCLUSIONS
D03758,"Slow adaption processes, like synaptic and intrinsic plasticity, abound in the brain and shape the landscape for the neural dynamics occurring on substantially faster timescales.$$$At any given time the network is characterized by a set of internal parameters, which are adapting continuously, albeit slowly.$$$This set of parameters defines the number and the location of the respective adiabatic attractors.$$$The slow evolution of network parameters hence induces an evolving attractor landscape, a process which we term attractor metadynamics.$$$We study the nature of the metadynamics of the attractor landscape for several continuous-time autonomous model networks.$$$We find both first- and second-order changes in the location of adiabatic attractors and argue that the study of the continuously evolving attractor landscape constitutes a powerful tool for understanding the overall development of the neural dynamics.",BACKGROUND METHODS METHODS RESULTS OBJECTIVES RESULTS
D01394,"A public decision-making problem consists of a set of issues, each with multiple possible alternatives, and a set of competing agents, each with a preferred alternative for each issue.$$$We study adaptations of market economies to this setting, focusing on binary issues.$$$Issues have prices, and each agent is endowed with artificial currency that she can use to purchase probability for her preferred alternatives (we allow randomized outcomes).$$$We first show that when each issue has a single price that is common to all agents, market equilibria can be arbitrarily bad.$$$This negative result motivates a different approach.$$$We present a novel technique called ""pairwise issue expansion"", which transforms any public decision-making instance into an equivalent Fisher market, the simplest type of private goods market.$$$This is done by expanding each issue into many goods: one for each pair of agents who disagree on that issue.$$$We show that the equilibrium prices in the constructed Fisher market yield a ""pairwise pricing equilibrium"" in the original public decision-making problem which maximizes Nash welfare.$$$More broadly, pairwise issue expansion uncovers a powerful connection between the public decision-making and private goods settings; this immediately yields several interesting results about public decisions markets, and furthers the hope that we will be able to find a simple iterative voting protocol that leads to near-optimum decisions.",BACKGROUND OBJECTIVES METHODS RESULTS METHODS METHODS METHODS RESULTS CONCLUSIONS
D03922,"There is more to images than their objective physical content: for example, advertisements are created to persuade a viewer to take a certain action.$$$We propose the novel problem of automatic advertisement understanding.$$$To enable research on this problem, we create two datasets: an image dataset of 64,832 image ads, and a video dataset of 3,477 ads.$$$Our data contains rich annotations encompassing the topic and sentiment of the ads, questions and answers describing what actions the viewer is prompted to take and the reasoning that the ad presents to persuade the viewer (""What should I do according to this ad, and why should I do it?$$$""), and symbolic references ads make (e.g. a dove symbolizes peace).$$$We also analyze the most common persuasive strategies ads use, and the capabilities that computer vision systems should have to understand these strategies.$$$We present baseline classification results for several prediction tasks, including automatically answering questions about the messages of the ads.",BACKGROUND OBJECTIVES METHODS METHODS METHODS METHODS RESULTS
D06258,"We introduce a new feature map for barcodes that arise in persistent homology computation.$$$The main idea is to first realize each barcode as a path in a convenient vector space, and to then compute its path signature which takes values in the tensor algebra of that vector space.$$$The composition of these two operations - barcode to path, path to tensor series - results in a feature map that has several desirable properties for statistical learning, such as universality and characteristicness, and achieves state-of-the-art results on common classification benchmarks.",OBJECTIVES METHODS RESULTS
D01424,"Phase retrieval refers to recovering a signal from its Fourier magnitude.$$$This problem arises naturally in many scientific applications, such as ultra-short laser pulse characterization and diffraction imaging.$$$Unfortunately, phase retrieval is ill-posed for almost all one-dimensional signals.$$$In order to characterize a laser pulse and overcome the ill-posedness, it is common to use a technique called Frequency-Resolved Optical Gating (FROG).$$$In FROG, the measured data, referred to as FROG trace, is the Fourier magnitude of the product of the underlying signal with several translated versions of itself.$$$The FROG trace results in a system of phaseless quartic Fourier measurements.$$$In this paper, we prove that it suffices to consider only three translations of the signal to determine almost all bandlimited signals, up to trivial ambiguities.$$$In practice, one usually also has access to the signal's Fourier magnitude.$$$We show that in this case only two translations suffice.$$$Our results significantly improve upon earlier work.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND RESULTS BACKGROUND RESULTS CONCLUSIONS
D01534,"We study nonlinear power systems consisting of generators, generator buses, and non-generator buses.$$$First, looking at a generator and its bus' variables jointly, we introduce a synchronization concept for a pair of such joint generators and buses.$$$We show that this concept is related to graph symmetry.$$$Next, we extend, in two ways, the synchronization from a pair to a partition of all generators in the networks and show that they are related to either graph symmetry or equitable partitions.$$$Finally, we show how an exact reduced model can be obtained by aggregating the generators and associated buses in the network when the original system is synchronized with respect to a partition, provided that the initial condition respects the partition.$$$Additionally, the aggregation-based reduced model is again a power system.",OBJECTIVES RESULTS RESULTS RESULTS RESULTS RESULTS
D06576,"Differentiating intrinsic language words from transliterable words is a key step aiding text processing tasks involving different natural languages.$$$We consider the problem of unsupervised separation of transliterable words from native words for text in Malayalam language.$$$Outlining a key observation on the diversity of characters beyond the word stem, we develop an optimization method to score words based on their nativeness.$$$Our method relies on the usage of probability distributions over character n-grams that are refined in step with the nativeness scorings in an iterative optimization formulation.$$$Using an empirical evaluation, we illustrate that our method, DTIM, provides significant improvements in nativeness scoring for Malayalam, establishing DTIM as the preferred method for the task.",BACKGROUND OBJECTIVES METHODS METHODS RESULTS/CONCLUSIONS
D05968,"In this paper we propose a novel texture descriptor called Fractal Weighted Local Binary Pattern (FWLBP).$$$The fractal dimension (FD) measure is relatively invariant to scale-changes, and presents a good correlation with human viewpoint of surface roughness.$$$We have utilized this property to construct a scale-invariant descriptor.$$$Here, the input image is sampled using an augmented form of the local binary pattern (LBP) over three different radii, and then used an indexing operation to assign FD weights to the collected samples.$$$The final histogram of the descriptor has its features calculated using LBP, and its weights computed from the FD image.$$$The proposed descriptor is scale invariant, and is also robust in rotation or reflection, and partially tolerant to noise and illumination changes.$$$In addition, the local fractal dimension is relatively insensitive to the bi-Lipschitz transformations, whereas its extension is adequate to precisely discriminate the fundamental of texture primitives.$$$Experiment results carried out on standard texture databases show that the proposed descriptor achieved better classification rates compared to the state-of-the-art descriptors.",OBJECTIVES BACKGROUND BACKGROUND METHODS METHODS CONCLUSIONS BACKGROUND RESULTS
D06808,"We now advocate a novel physical layer security solution that is unique to our previously proposed GPSM scheme with the aid of the proposed antenna scrambling.$$$The novelty and contribution of our paper lies in three aspects: 1/ principle: we introduce a `security key' generated at Alice that is unknown to both Bob and Eve, where the design goal is that the publicly unknown security key only imposes barrier for Eve.$$$2/ approach: we achieve it by conveying useful information only through the activation of RA indices, which is in turn concealed by the unknown security key in terms of the randomly scrambled symbols used in place of the conventional modulated symbols in GPSM scheme.$$$3/ design: we consider both Circular Antenna Scrambling (CAS) and Gaussian Antenna Scrambling (GAS) in detail and the resultant security capacity of both designs are quantified and compared.",BACKGROUND OBJECTIVES METHODS METHODS
D06899,"Techniques for dense semantic correspondence have provided limited ability to deal with the geometric variations that commonly exist between semantically similar images.$$$While variations due to scale and rotation have been examined, there lack practical solutions for more complex deformations such as affine transformations because of the tremendous size of the associated solution space.$$$To address this problem, we present a discrete-continuous transformation matching (DCTM) framework where dense affine transformation fields are inferred through a discrete label optimization in which the labels are iteratively updated via continuous regularization.$$$In this way, our approach draws solutions from the continuous space of affine transformations in a manner that can be computed efficiently through constant-time edge-aware filtering and a proposed affine-varying CNN-based descriptor.$$$Experimental results show that this model outperforms the state-of-the-art methods for dense semantic correspondence on various benchmarks.",BACKGROUND BACKGROUND OBJECTIVES/METHODS METHODS RESULTS
D04141,"Past decade has seen the development of many shared-memory graph processing frameworks intended to reduce the effort of developing high performance parallel applications.$$$However, many of these frameworks, based on Vertex-centric or Edge-centric paradigms suffer from several issues such as poor cache utilization, irregular memory accesses, heavy use of synchronization primitives or theoretical inefficiency, that deteriorate overall performance and scalability.$$$In this paper, we generalize a recent partition-centric paradigm for PageRank computation to a novel Graph Processing Over Partitions (GPOP) framework that exploits the locality of partitioning to dramatically improve the cache performance of a variety of graph algorithms.$$$It achieves high scalability by enabling completely lock and atomic free computation.$$$Its built-in analytical performance model enables it to use a hybrid of source and partition centric communication modes in a way that ensures work-efficiency each iteration while simultaneously boosting high bandwidth sequential memory accesses.$$$Finally, the GPOP framework is designed with programmability in mind.$$$It completely abstracts away underlying programming model details from the user and provides an easy to program set of APIs with the ability to selectively continue the active vertex set across iterations.$$$We extensively evaluate the performance of GPOP for a variety of graph algorithms, using several large datasets.$$$We observe that GPOP incurs upto 8.6x and 5.2x less L2 cache misses compared to Ligra and GraphMat, respectively.$$$In terms of execution time, GPOP is upto 19x and 6.1x faster than Ligra and GraphMat, respectively.",BACKGROUND BACKGROUND/OBJECTIVES OBJECTIVES/METHODS METHODS METHODS OBJECTIVES METHODS RESULTS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS
D03324,"Hybrid multiple-antenna transceivers, which combine large-dimensional analog pre/postprocessing with lower-dimensional digital processing, are the most promising approach for reducing the hardware cost and training overhead in massive MIMO systems.$$$This paper provides a comprehensive survey of the various incarnations of such structures that have been proposed in the literature.$$$We provide a taxonomy in terms of the required channel state information (CSI), namely whether the processing adapts to the instantaneous or the average (second-order) CSI; while the former provides somewhat better signal-to-noise and interference ratio (SNIR), the latter has much lower overhead for CSI acquisition.$$$We furthermore distinguish hardware structures of different complexities.$$$Finally, we point out the special design aspects for operation at millimeter-wave frequencies.",BACKGROUND OBJECTIVES METHODS/RESULTS RESULTS RESULTS
D02579,"We study projection free methods for constrained geodesically convex optimization.$$$In particular, we propose a Riemannian version of the Frank-Wolfe (RFW) method.$$$We analyze RFW's convergence and provide a global, non-asymptotic sublinear convergence rate.$$$We also present a setting under which RFW can attain a linear rate.$$$Later, we specialize RFW to the manifold of positive definite matrices, where we are motivated by the specific task of computing the geometric mean (also known as Karcher mean or Riemannian centroid).$$$For this task, RFW requires access to a ""linear oracle"" that turns out to be a nonconvex semidefinite program.$$$Remarkably, this nonconvex program is shown to admit a closed form solution, which may be of independent interest too.$$$We complement this result by also studying a nonconvex Euclidean Frank-Wolfe approach, along with its global convergence analysis.$$$Finally, we empirically compare Rfw against recently published methods for the Riemannian centroid and observe strong performance gains.",OBJECTIVES OBJECTIVES RESULTS RESULTS RESULTS METHODS METHODS METHODS METHODS
D06109,"This paper proposes three simple, compact yet effective representations of depth sequences, referred to respectively as Dynamic Depth Images (DDI), Dynamic Depth Normal Images (DDNI) and Dynamic Depth Motion Normal Images (DDMNI), for both isolated and continuous action recognition.$$$These dynamic images are constructed from a segmented sequence of depth maps using hierarchical bidirectional rank pooling to effectively capture the spatial-temporal information.$$$Specifically, DDI exploits the dynamics of postures over time and DDNI and DDMNI exploit the 3D structural information captured by depth maps.$$$Upon the proposed representations, a ConvNet based method is developed for action recognition.$$$The image-based representations enable us to fine-tune the existing Convolutional Neural Network (ConvNet) models trained on image data without training a large number of parameters from scratch.$$$The proposed method achieved the state-of-art results on three large datasets, namely, the Large-scale Continuous Gesture Recognition Dataset (means Jaccard index 0.4109), the Large-scale Isolated Gesture Recognition Dataset (59.21%), and the NTU RGB+D Dataset (87.08% cross-subject and 84.22% cross-view) even though only the depth modality was used.",OBJECTIVES METHODS METHODS METHODS METHODS RESULTS
D05744,"Developing algorithms for solving high-dimensional partial differential equations (PDEs) has been an exceedingly difficult task for a long time, due to the notoriously difficult problem known as the ""curse of dimensionality"".$$$This paper introduces a deep learning-based approach that can handle general high-dimensional parabolic PDEs.$$$To this end, the PDEs are reformulated using backward stochastic differential equations and the gradient of the unknown solution is approximated by neural networks, very much in the spirit of deep reinforcement learning with the gradient acting as the policy function.$$$Numerical results on examples including the nonlinear Black-Scholes equation, the Hamilton-Jacobi-Bellman equation, and the Allen-Cahn equation suggest that the proposed algorithm is quite effective in high dimensions, in terms of both accuracy and cost.$$$This opens up new possibilities in economics, finance, operational research, and physics, by considering all participating agents, assets, resources, or particles together at the same time, instead of making ad hoc assumptions on their inter-relationships.",BACKGROUND OTHERS METHODS RESULTS CONCLUSIONS
D00094,"Recently, there have been increasing demands to construct compact deep architectures to remove unnecessary redundancy and to improve the inference speed.$$$While many recent works focus on reducing the redundancy by eliminating unneeded weight parameters, it is not possible to apply a single deep architecture for multiple devices with different resources.$$$When a new device or circumstantial condition requires a new deep architecture, it is necessary to construct and train a new network from scratch.$$$In this work, we propose a novel deep learning framework, called a nested sparse network, which exploits an n-in-1-type nested structure in a neural network.$$$A nested sparse network consists of multiple levels of networks with a different sparsity ratio associated with each level, and higher level networks share parameters with lower level networks to enable stable nested learning.$$$The proposed framework realizes a resource-aware versatile architecture as the same network can meet diverse resource requirements.$$$Moreover, the proposed nested network can learn different forms of knowledge in its internal networks at different levels, enabling multiple tasks using a single network, such as coarse-to-fine hierarchical classification.$$$In order to train the proposed nested sparse network, we propose efficient weight connection learning and channel and layer scheduling strategies.$$$We evaluate our network in multiple tasks, including adaptive deep compression, knowledge distillation, and learning class hierarchy, and demonstrate that nested sparse networks perform competitively, but more efficiently, compared to existing methods.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/METHODS METHODS METHODS METHODS METHODS RESULTS/CONCLUSIONS
D04783,"When analyzing the genome, researchers have discovered that proteins bind to DNA based on certain patterns of the DNA sequence known as ""motifs"".$$$However, it is difficult to manually construct motifs due to their complexity.$$$Recently, externally learned memory models have proven to be effective methods for reasoning over inputs and supporting sets.$$$In this work, we present memory matching networks (MMN) for classifying DNA sequences as protein binding sites.$$$Our model learns a memory bank of encoded motifs, which are dynamic memory modules, and then matches a new test sequence to each of the motifs to classify the sequence as a binding or nonbinding site.",BACKGROUND BACKGROUND BACKGROUND CONCLUSIONS METHODS
D00577,"Human vision possesses strong invariance in image recognition.$$$The cognitive capability of deep convolutional neural network (DCNN) is close to the human visual level because of hierarchical coding directly from raw image.$$$Owing to its superiority in feature representation, DCNN has exhibited remarkable performance in scene recognition of high-resolution remote sensing (HRRS) images and classification of hyper-spectral remote sensing images.$$$In-depth investigation is still essential for understanding why DCNN can accurately identify diverse ground objects via its effective feature representation.$$$Thus, we train the deep neural network called AlexNet on our large scale remote sensing image recognition benchmark.$$$At the neuron level in each convolution layer, we analyze the general properties of DCNN in HRRS image recognition by use of a framework of visual stimulation-characteristic response combined with feature coding-classification decoding.$$$Specifically, we use histogram statistics, representational dissimilarity matrix, and class activation mapping to observe the selective and invariance representations of DCNN in HRRS image recognition.$$$We argue that selective and invariance representations play important roles in remote sensing images tasks, such as classification, detection, and segment.$$$Also selective and invariance representations are significant to design new DCNN liked models for analyzing and understanding remote sensing images.",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES METHODS METHODS METHODS RESULTS RESULTS
D05720,"We argue that Godel's completeness theorem is equivalent to completability of consistent theories, and Godel's incompleteness theorem is equivalent to the fact that this completion is not constructive, in the sense that there are some consistent and recursively enumerable theories which cannot be extended to any complete and consistent and recursively enumerable theory.$$$Though any consistent and decidable theory can be extended to a complete and consistent and decidable theory.$$$Thus deduction and consistency are not decidable in logic, and an analogue of Rice's Theorem holds for recursively enumerable theories: all the non-trivial properties of such theories are undecidable.",OBJECTIVES BACKGROUND RESULTS
D05346,"Networks observed in real world like social networks, collaboration networks etc., exhibit temporal dynamics, i.e.$$$nodes and edges appear and/or disappear over time.$$$In this paper, we propose a generative, latent space based, statistical model for such networks (called dynamic networks).$$$We consider the case where the number of nodes is fixed, but the presence of edges can vary over time.$$$Our model allows the number of communities in the network to be different at different time steps.$$$We use a neural network based methodology to perform approximate inference in the proposed model and its simplified version.$$$Experiments done on synthetic and real world networks for the task of community detection and link prediction demonstrate the utility and effectiveness of our model as compared to other similar existing approaches.",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES OBJECTIVES METHODS RESULTS
D05775,"We study the query complexity of Weak Parity: the problem of computing the parity of an n-bit input string, where one only has to succeed on a 1/2+eps fraction of input strings, but must do so with high probability on those inputs where one does succeed.$$$It is well-known that n randomized queries and n/2 quantum queries are needed to compute parity on all inputs.$$$But surprisingly, we give a randomized algorithm for Weak Parity that makes only O(n/log^0.246(1/eps)) queries, as well as a quantum algorithm that makes only O(n/sqrt(log(1/eps))) queries.$$$We also prove a lower bound of Omega(n/log(1/eps)) in both cases; and using extremal combinatorics, prove lower bounds of Omega(log n) in the randomized case and Omega(sqrt(log n)) in the quantum case for any eps>0.$$$We show that improving our lower bounds is intimately related to two longstanding open problems about Boolean functions: the Sensitivity Conjecture, and the relationships between query complexity and polynomial degree.",BACKGROUND BACKGROUND RESULTS RESULTS RESULTS
D05662,"There has been a growing interest for Wireless Distributed Computing (WDC), which leverages collaborative computing over multiple wireless devices.$$$WDC enables complex applications that a single device cannot support individually.$$$However, the problem of assigning tasks over multiple devices becomes challenging in the dynamic environments encountered in real-world settings, considering that the resource availability and channel conditions change over time in unpredictable ways due to mobility and other factors.$$$In this paper, we formulate a task assignment problem as an online learning problem using an adversarial multi-armed bandit framework.$$$We propose MABSTA, a novel online learning algorithm that learns the performance of unknown devices and channel qualities continually through exploratory probing and makes task assignment decisions by exploiting the gained knowledge.$$$For maximal adaptability, MABSTA is designed to make no stochastic assumption about the environment.$$$We analyze it mathematically and provide a worst-case performance guarantee for any dynamic environment.$$$We also compare it with the optimal offline policy as well as other baselines via emulations on trace-data obtained from a wireless IoT testbed, and show that it offers competitive and robust performance in all cases.$$$To the best of our knowledge, MABSTA is the first online algorithm in this domain of task assignment problems and provides provable performance guarantee.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS RESULTS RESULTS CONCLUSIONS
D03378,"We have recently seen many successful applications of recurrent neural networks (RNNs) on electronic medical records (EMRs), which contain histories of patients' diagnoses, medications, and other various events, in order to predict the current and future states of patients.$$$Despite the strong performance of RNNs, it is often challenging for users to understand why the model makes a particular prediction.$$$Such black-box nature of RNNs can impede its wide adoption in clinical practice.$$$Furthermore, we have no established methods to interactively leverage users' domain expertise and prior knowledge as inputs for steering the model.$$$Therefore, our design study aims to provide a visual analytics solution to increase interpretability and interactivity of RNNs via a joint effort of medical experts, artificial intelligence scientists, and visual analytics researchers.$$$Following the iterative design process between the experts, we design, implement, and evaluate a visual analytics tool called RetainVis, which couples a newly improved, interpretable and interactive RNN-based model called RetainEX and visualizations for users' exploration of EMR data in the context of prediction tasks.$$$Our study shows the effective use of RetainVis for gaining insights into how individual medical codes contribute to making risk predictions, using EMRs of patients with heart failure and cataract symptoms.$$$Our study also demonstrates how we made substantial changes to the state-of-the-art RNN model called RETAIN in order to make use of temporal information and increase interactivity.$$$This study will provide a useful guideline for researchers that aim to design an interpretable and interactive visual analytics tool for RNNs.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS RESULTS CONCLUSIONS
D00477,"Finding optimal data for inpainting is a key problem in the context of partial differential equation based image compression.$$$The data that yields the most accurate reconstruction is real-valued.$$$Thus, quantisation models are mandatory to allow an efficient encoding.$$$These can also be understood as challenging data clustering problems.$$$Although clustering approaches are well suited for this kind of compression codecs, very few works actually consider them.$$$Each pixel has a global impact on the reconstruction and optimal data locations are strongly correlated with their corresponding colour values.$$$These facts make it hard to predict which feature works best.$$$In this paper we discuss quantisation strategies based on popular methods such as k-means.$$$We are lead to the central question which kind of feature vectors are best suited for image compression.$$$To this end we consider choices such as the pixel values, the histogram or the colour map.$$$Our findings show that the number of colours can be reduced significantly without impacting the reconstruction quality.$$$Surprisingly, these benefits do not directly translate to a good image compression performance.$$$The gains in the compression ratio are lost due to increased storage costs.$$$This suggests that it is integral to evaluate the clustering on both, the reconstruction error and the final file size.",OBJECTIVES BACKGROUND BACKGROUND BACKGROUND METHODS BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES OBJECTIVES RESULTS CONCLUSIONS CONCLUSIONS CONCLUSIONS
D06713,"We present an implementation of a blind source separation algorithm to remove foregrounds off millimeter surveys made by single-channel instruments.$$$In order to make possible such a decomposition over single-wavelength data: we generate levels of artificial redundancy, then perform a blind decomposition, calibrate the resulting maps, and lastly measure physical information.$$$We simulate the reduction pipeline using mock data: atmospheric fluctuations, extended astrophysical foregrounds, and point-like sources, but we apply the same methodology to the AzTEC/ASTE survey of the Great Observatories Origins Deep Survey-South (GOODS-S).$$$In both applications, our technique robustly decomposes redundant maps into their underlying components, reducing flux bias, improving signal-to-noise, and minimizing information loss.$$$In particular, the GOODS-S survey is decomposed into four independent physical components, one of them is the already known map of point sources, two are atmospheric and systematic foregrounds, and the fourth component is an extended emission that can be interpreted as the confusion background of faint sources.",OBJECTIVES/CONCLUSIONS METHODS METHODS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS
D02405,"When scripts in untyped languages grow into large programs, maintaining them becomes difficult.$$$A lack of explicit type annotations in typical scripting languages forces programmers to must (re)discover critical pieces of design information every time they wish to change a program.$$$This analysis step both slows down the maintenance process and may even introduce mistakes due to the violation of undiscovered invariants.$$$This paper presents Typed Scheme, an explicitly typed extension of PLT Scheme, an untyped scripting language.$$$Its type system is based on the novel notion of occurrence typing, which we formalize and mechanically prove sound.$$$The implementation of Typed Scheme additionally borrows elements from a range of approaches, including recursive types, true unions and subtyping, plus polymorphism combined with a modicum of local inference.$$$The formulation of occurrence typing naturally leads to a simple and expressive version of predicates to describe refinement types.$$$A Typed Scheme program can use these refinement types to keep track of arbitrary classes of values via the type system.$$$Further, we show how the Typed Scheme type system, in conjunction with simple recursive types, is able to encode refinements of existing datatypes, thus expressing both proposed variations of refinement types.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS METHODS RESULTS
D04216,"Analysing and explaining relationships between entities in a graph is a fundamental problem associated with many practical applications.$$$For example, a graph of biological pathways can be used for discovering a previously unknown relationship between two proteins.$$$Domain experts, however, may be reluctant to trust such a discovery without a detailed explanation as to why exactly the two proteins are deemed related in the graph.$$$This paper provides an overview of the types of solutions, their associated methods and strategies, that have been proposed for finding entity relatedness explanations in graphs.$$$The first type of solution relies on information inherent to the paths connecting the entities.$$$This type of solution provides entity relatedness explanations in the form of a list of ranked paths.$$$The rank of a path is measured in terms of importance, uniqueness, novelty and informativeness.$$$The second type of solution relies on measures of node relevance.$$$In this case, the relevance of nodes is measured w.r.t. the entities of interest, and relatedness explanations are provided in the form of a subgraph that maximises node relevance scores.$$$This paper uses this classification of approaches to discuss and contrast some of the key concepts that guide different solutions to the problem of entity relatedness explanation in graphs.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS METHODS METHODS OBJECTIVES
D03297,"We investigate the problem of automatically determining what type of shoe left an impression found at a crime scene.$$$This recognition problem is made difficult by the variability in types of crime scene evidence (ranging from traces of dust or oil on hard surfaces to impressions made in soil) and the lack of comprehensive databases of shoe outsole tread patterns.$$$We find that mid-level features extracted by pre-trained convolutional neural nets are surprisingly effective descriptors for this specialized domains.$$$However, the choice of similarity measure for matching exemplars to a query image is essential to good performance.$$$For matching multi-channel deep features, we propose the use of multi-channel normalized cross-correlation and analyze its effectiveness.$$$Our proposed metric significantly improves performance in matching crime scene shoeprints to laboratory test impressions.$$$We also show its effectiveness in other cross-domain image retrieval problems: matching facade images to segmentation labels and aerial photos to map images.$$$Finally, we introduce a discriminatively trained variant and fine-tune our system through our proposed metric, obtaining state-of-the-art performance.",OBJECTIVES BACKGROUND METHODS BACKGROUND/CONCLUSIONS METHODS RESULTS OBJECTIVES/RESULTS METHODS
D04862,"In this paper, we study the implications of the commonplace assumption that most social media studies make with respect to the nature of message shares (such as retweets) as a predominantly positive interaction.$$$By analyzing two large longitudinal Brazilian Twitter datasets containing 5 years of conversations on two polarizing topics - Politics and Sports - we empirically demonstrate that groups holding antagonistic views can actually retweet each other more often than they retweet other groups.$$$We show that assuming retweets as endorsement interactions can lead to misleading conclusions with respect to the level of antagonism among social communities, and that this apparent paradox is explained in part by the use of retweets to quote the original content creator out of the message's original temporal context, for humor and criticism purposes.$$$As a consequence, messages diffused on online media can have their polarity reversed over time, what poses challenges for social and computer scientists aiming to classify and track opinion groups on online media.$$$On the other hand, we found that the time users take to retweet a message after it has been originally posted can be a useful signal to infer antagonism in social platforms, and that surges of out-of-context retweets correlate with sentiment drifts triggered by real-world events.$$$We also discuss how such evidences can be embedded in sentiment analysis models.",OBJECTIVES METHODS/RESULTS CONCLUSIONS RESULTS RESULTS OBJECTIVES
D05639,"We provide two sufficient conditions to guarantee that the round functions of a translation based cipher generate a primitive group.$$$Furthermore, under the same hypotheses, and assuming that a round of the cipher is strongly proper and consists of m-bit S-Boxes, with m = 3; 4 or 5, we prove that such a group is the alternating group.$$$As an immediate consequence, we deduce that the round functions of some lightweight translation based ciphers, such as the PRESENT cipher, generate the alternating group.",RESULTS RESULTS CONCLUSIONS
D03152,"Money laundering is a crime that makes it possible to finance other crimes, for this reason, it is important for criminal organizations and their combat is prioritized by nations around the world.$$$The anti-money laundering process has not evolved as expected because it has prioritized only the signaling of suspicious transactions.$$$The constant increasing in the volume of transactions has overloaded the indispensable human work of final evaluation of the suspicions.$$$This article presents a multiagent system that aims to go beyond the capture of suspicious transactions, seeking to assist the human expert in the analysis of suspicions.$$$The agents created use data mining techniques to create transactional behavioral profiles; apply rules generated in learning process in conjunction with specific rules based on legal aspects and profiles created to capture suspicious transactions; and analyze these suspicious transactions indicating to the human expert those that require more detailed analysis.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS
D04883,"Automated program repair techniques, which target to generating correct patches for real world defects automatically, have gained a lot of attention in the last decade.$$$Many different techniques and tools have been proposed and developed.$$$However, even the most sophisticated program repair techniques can only repair a small portion of defects while producing a lot of incorrect patches.$$$A possible reason for this low performance is that the test suites of real world programs are usually too weak to guarantee the behavior of the program.$$$To understand to what extent defects can be fixed with weak test suites, we analyzed 50 real world defects from Defects4J, in which we found that up to 84% of them could be correctly fixed.$$$This result suggests that there is plenty of space for current automated program repair techniques to improve.$$$Furthermore, we summarized seven fault localization strategies and seven patch generation strategies that were useful in localizing and fixing these defects, and compared those strategies with current repair techniques.$$$The results indicate potential directions to improve automatic program repair in the future research.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/METHODS/RESULTS CONCLUSIONS RESULTS OBJECTIVES
D06080,"The conventional model for online planning under uncertainty assumes that an agent can stop and plan without incurring costs for the time spent planning.$$$However, planning time is not free in most real-world settings.$$$For example, an autonomous drone is subject to nature's forces, like gravity, even while it thinks, and must either pay a price for counteracting these forces to stay in place, or grapple with the state change caused by acquiescing to them.$$$Policy optimization in these settings requires metareasoning---a process that trades off the cost of planning and the potential policy improvement that can be achieved.$$$We formalize and analyze the metareasoning problem for Markov Decision Processes (MDPs).$$$Our work subsumes previously studied special cases of metareasoning and shows that in the general case, metareasoning is at most polynomially harder than solving MDPs with any given algorithm that disregards the cost of thinking.$$$For reasons we discuss, optimal general metareasoning turns out to be impractical, motivating approximations.$$$We present approximate metareasoning procedures which rely on special properties of the BRTDP planning algorithm and explore the effectiveness of our methods on a variety of problems.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND/OBJECTIVES OBJECTIVES/METHODS METHODS/RESULTS RESULTS METHODS/RESULTS
D02524,"Current research environments are witnessing high enormities of presentations occurring in different sessions at academic conferences.$$$This situation makes it difficult for researchers (especially juniors) to attend the right presentation session(s) for effective collaboration.$$$In this paper, we propose an innovative venue recommendation algorithm to enhance smart conference participation.$$$Our proposed algorithm, Social Aware Recommendation of Venues and Environments (SARVE), computes the Pearson Correlation and social characteristic information of conference participants.$$$SARVE further incorporates the current context of both the smart conference community and participants in order to model a recommendation process using distributed community detection.$$$Through the integration of the above computations and techniques, we are able to recommend presentation sessions of active participant presenters that may be of high interest to a particular participant.$$$We evaluate SARVE using a real world dataset.$$$Our experimental results demonstrate that SARVE outperforms other state-of-the-art methods.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS
D04669,"Modeling should play a central role in K-12 STEM education, where it could make classes much more engaging.$$$A model underlies every scientific theory, and models are central to all the STEM disciplines (Science, Technology, Engineering, Math).$$$This paper describes executable concept modeling of STEM concepts using immutable objects and pure functions in Python.$$$I present examples in math, physics, chemistry, and engineering, built using a proof-of-concept tool called PySTEMM .$$$The approach applies to all STEM areas and supports learning with pictures, narrative, animation, and graph plots.$$$Models can extend each other, simplifying getting started.$$$The functional-programming style reduces incidental complexity and code debugging.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS BACKGROUND METHODS
D06369,"This paper proposes a method based on signal injection to obtain the saturated current-flux relations of a PMSM from locked-rotor experiments.$$$With respect to the classical method based on time integration, it has the main advantage of being completely independent of the stator resistance; moreover, it is less sensitive to voltage biases due to the power inverter, as the injected signal may be fairly large.",RESULTS BACKGROUND
D00488,"Serious scientific games are games whose purpose is not only fun.$$$In the field of science, the serious goals include crucial activities for scientists: outreach, teaching and research.$$$The number of serious games is increasing rapidly, in particular citizen science games, games that allow people to produce and/or analyze scientific data.$$$Interestingly, it is possible to build a set of rules providing a guideline to create or improve serious games.$$$We present arguments gathered from our own experience ( Phylo , DocMolecules , HiRE-RNA contest and Pangu) as well as examples from the growing literature on scientific serious games.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES RESULTS
D05828,"Detection of protein-protein interactions (PPIs) plays a vital role in molecular biology.$$$Particularly, infections are caused by the interactions of host and pathogen proteins.$$$It is important to identify host-pathogen interactions (HPIs) to discover new drugs to counter infectious diseases.$$$Conventional wet lab PPI prediction techniques have limitations in terms of large scale application and budget.$$$Hence, computational approaches are developed to predict PPIs.$$$This study aims to develop large margin machine learning models to predict interspecies PPIs with a special interest in host-pathogen protein interactions (HPIs).$$$Especially, we focus on seeking answers to three queries that arise while developing an HPI predictor.$$$1) How should we select negative samples?$$$2) What should be the size of negative samples as compared to the positive samples?$$$3) What type of margin violation penalty should be used to train the predictor?$$$We compare two available methods for negative sampling.$$$Moreover, we propose a new method of assigning weights to each training example in weighted SVM depending on the distance of the negative examples from the positive examples.$$$We have also developed a web server for our HPI predictor called HoPItor (Host Pathogen Interaction predicTOR) that can predict interactions between human and viral proteins.$$$This webserver can be accessed at the URL: http://faculty.pieas.edu.pk/fayyaz/software.html#HoPItor.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES OBJECTIVES OBJECTIVES OBJECTIVES METHODS METHODS/RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS OTHERS
D00593,"We present an improved model and theory for time-causal and time-recursive spatio-temporal receptive fields, based on a combination of Gaussian receptive fields over the spatial domain and first-order integrators or equivalently truncated exponential filters coupled in cascade over the temporal domain.$$$Compared to previous spatio-temporal scale-space formulations in terms of non-enhancement of local extrema or scale invariance, these receptive fields are based on different scale-space axiomatics over time by ensuring non-creation of new local extrema or zero-crossings with increasing temporal scale.$$$Specifically, extensions are presented about (i) parameterizing the intermediate temporal scale levels, (ii) analysing the resulting temporal dynamics, (iii) transferring the theory to a discrete implementation, (iv) computing scale-normalized spatio-temporal derivative expressions for spatio-temporal feature detection and (v) computational modelling of receptive fields in the lateral geniculate nucleus (LGN) and the primary visual cortex (V1) in biological vision.$$$We show that by distributing the intermediate temporal scale levels according to a logarithmic distribution, we obtain much faster temporal response properties (shorter temporal delays) compared to a uniform distribution.$$$Specifically, these kernels converge very rapidly to a limit kernel possessing true self-similar scale-invariant properties over temporal scales, thereby allowing for true scale invariance over variations in the temporal scale, although the underlying temporal scale-space representation is based on a discretized temporal scale parameter.$$$We show how scale-normalized temporal derivatives can be defined for these time-causal scale-space kernels and how the composed theory can be used for computing basic types of scale-normalized spatio-temporal derivative expressions in a computationally efficient manner.",OBJECTIVES/RESULTS BACKGROUND/METHODS OBJECTIVES/RESULTS RESULTS RESULTS OBJECTIVES/RESULTS
D02570,"Recently ensemble selection for consensus clustering has emerged as a research problem in Machine Intelligence.$$$Normally consensus clustering algorithms take into account the entire ensemble of clustering, where there is a tendency of generating a very large size ensemble before computing its consensus.$$$One can avoid considering the entire ensemble and can judiciously select few partitions in the ensemble without compromising on the quality of the consensus.$$$This may result in an efficient consensus computation technique and may save unnecessary computational overheads.$$$The ensemble selection problem addresses this issue of consensus clustering.$$$In this paper, we propose an efficient method of ensemble selection for a large ensemble.$$$We prioritize the partitions in the ensemble based on diversity and frequency.$$$Our method selects top K of the partitions in order of priority, where K is decided by the user.$$$We observe that considering jointly the diversity and frequency helps in identifying few representative partitions whose consensus is qualitatively better than the consensus of the entire ensemble.$$$Experimental analysis on a large number of datasets shows our method gives better results than earlier ensemble selection methods.",BACKGROUND BACKGROUND OBJECTIVES/METHODS OBJECTIVES/METHODS BACKGROUND/OBJECTIVES METHODS METHODS METHODS METHODS/RESULTS/CONCLUSIONS RESULTS
D01057,"Recurrent neural network models with an attention mechanism have proven to be extremely effective on a wide variety of sequence-to-sequence problems.$$$However, the fact that soft attention mechanisms perform a pass over the entire input sequence when producing each element in the output sequence precludes their use in online settings and results in a quadratic time complexity.$$$Based on the insight that the alignment between input and output sequence elements is monotonic in many problems of interest, we propose an end-to-end differentiable method for learning monotonic alignments which, at test time, enables computing attention online and in linear time.$$$We validate our approach on sentence summarization, machine translation, and online speech recognition problems and achieve results competitive with existing sequence-to-sequence models.",BACKGROUND BACKGROUND METHODS RESULTS
D01434,"We present FoamGrid, a new implementation of the DUNE grid interface.$$$FoamGrid implements one- and two-dimensional grids in a physical space of arbitrary dimension, which allows for grids for curved domains.$$$Even more, the grids are not expected to have a manifold structure, i.e., more than two elements can share a common facet.$$$This makes FoamGrid the grid data structure of choice for simulating structures such as foams, discrete fracture networks, or network flow problems.$$$FoamGrid implements adaptive non-conforming refinement with element parametrizations.$$$As an additional feature it allows removal and addition of elements in an existing grid, which makes FoamGrid suitable for network growth problems.$$$We show how to use FoamGrid, with particular attention to the extensions of the grid interface needed to handle non-manifold topology and grid growth.$$$Three numerical examples demonstrate the possibilities offered by FoamGrid.",BACKGROUND RESULTS RESULTS CONCLUSIONS RESULTS RESULTS OTHERS METHODS
D04395,"Prior social contagion models consider the spread of either one contagion at a time on interdependent networks or multiple contagions on single layer networks or under assumptions of competition.$$$We propose a new threshold model for the diffusion of multiple contagions.$$$Individuals are placed on a multiplex network with a periodic lattice and a random-regular-graph layer.$$$On these population structures, we study the interface between two key aspects of the diffusion process: the level of synergy between two contagions, and the rate at which individuals become dormant after adoption.$$$Dormancy is defined as a looser form of immunity that models the ability to spread without resistance.$$$Monte Carlo simulations reveal lower synergy makes contagions more susceptible to percolation, especially those that diffuse on lattices.$$$Faster diffusion of one contagion with dormancy probabilistically blocks the diffusion of the other, in a way similar to ring vaccination.$$$We show that within a band of synergy, contagions on the lattices undergo bimodal or trimodal branching if they are the slower diffusing contagion.",BACKGROUND OBJECTIVES METHODS METHODS METHODS RESULTS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS
D00335,"The quality of the data in spreadsheets is less discussed than the structural integrity of the formulas.$$$Yet it is an area of great interest to the owners and users of the spreadsheet.$$$This paper provides an overview of Information Quality (IQ) and Data Quality (DQ) with specific reference to how data is sourced, structured, and presented in spreadsheets.",BACKGROUND BACKGROUND/OBJECTIVES OBJECTIVES/RESULTS
D01919,"It's useful to automatically transform an image from its original form to some synthetic form (style, partial contents, etc.$$$), while keeping the original structure or semantics.$$$We define this requirement as the ""image-to-image translation"" problem, and propose a general approach to achieve it, based on deep convolutional and conditional generative adversarial networks (GANs), which has gained a phenomenal success to learn mapping images from noise input since 2014.$$$In this work, we develop a two step (unsupervised) learning method to translate images between different domains by using unlabeled images without specifying any correspondence between them, so that to avoid the cost of acquiring labeled data.$$$Compared with prior works, we demonstrated the capacity of generality in our model, by which variance of translations can be conduct by a single type of model.$$$Such capability is desirable in applications like bidirectional translation",BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS CONCLUSIONS
D03054,"Many machine learning image classifiers are vulnerable to adversarial attacks, inputs with perturbations designed to intentionally trigger misclassification.$$$Current adversarial methods directly alter pixel colors and evaluate against pixel norm-balls: pixel perturbations smaller than a specified magnitude, according to a measurement norm.$$$This evaluation, however, has limited practical utility since perturbations in the pixel space do not correspond to underlying real-world phenomena of image formation that lead to them and has no security motivation attached.$$$Pixels in natural images are measurements of light that has interacted with the geometry of a physical scene.$$$As such, we propose the direct perturbation of physical parameters that underly image formation: lighting and geometry.$$$As such, we propose a novel evaluation measure, parametric norm-balls, by directly perturbing physical parameters that underly image formation.$$$One enabling contribution we present is a physically-based differentiable renderer that allows us to propagate pixel gradients to the parametric space of lighting and geometry.$$$Our approach enables physically-based adversarial attacks, and our differentiable renderer leverages models from the interactive rendering literature to balance the performance and accuracy trade-offs necessary for a memory-efficient and scalable adversarial data augmentation workflow.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES METHODS RESULTS/CONCLUSIONS
D06456,"A latent-variable model is introduced for text matching, inferring sentence representations by jointly optimizing generative and discriminative objectives.$$$To alleviate typical optimization challenges in latent-variable models for text, we employ deconvolutional networks as the sequence decoder (generator), providing learned latent codes with more semantic information and better generalization.$$$Our model, trained in an unsupervised manner, yields stronger empirical predictive performance than a decoder based on Long Short-Term Memory (LSTM), with less parameters and considerably faster training.$$$Further, we apply it to text sequence-matching problems.$$$The proposed model significantly outperforms several strong sentence-encoding baselines, especially in the semi-supervised setting.",METHODS OBJECTIVES RESULTS RESULTS RESULTS
D00024,"This paper explores the potential of extreme learning machine based supervised classification algorithm for land cover classification.$$$In comparison to a backpropagation neural network, which requires setting of several user-defined parameters and may produce local minima, extreme learning machine require setting of one parameter and produce a unique solution.$$$ETM+ multispectral data set (England) was used to judge the suitability of extreme learning machine for remote sensing classifications.$$$A back propagation neural network was used to compare its performance in term of classification accuracy and computational cost.$$$Results suggest that the extreme learning machine perform equally well to back propagation neural network in term of classification accuracy with this data set.$$$The computational cost using extreme learning machine is very small in comparison to back propagation neural network.",BACKGROUND BACKGROUND/OBJECTIVES/METHODS OBJECTIVES OBJECTIVES/METHODS RESULTS CONCLUSIONS
D02190,"We make available to the community a new dataset to support action-recognition research.$$$This dataset is different from prior datasets in several key ways.$$$It is significantly larger.$$$It contains streaming video with long segments containing multiple action occurrences that often overlap in space and/or time.$$$All actions were filmed in the same collection of backgrounds so that background gives little clue as to action class.$$$We had five humans replicate the annotation of temporal extent of action occurrences labeled with their class and measured a surprisingly low level of intercoder agreement.$$$A baseline experiment shows that recent state-of-the-art methods perform poorly on this dataset.$$$This suggests that this will be a challenging dataset to foster advances in action-recognition research.$$$This manuscript serves to describe the novel content and characteristics of the LCA dataset, present the design decisions made when filming the dataset, and document the novel methods employed to annotate the dataset.",RESULTS OTHERS RESULTS RESULTS RESULTS METHODS RESULTS CONCLUSIONS OBJECTIVES
D01513,"Deformable image registration is a fundamental task in medical image analysis, aiming to establish a dense and non-linear correspondence between a pair of images.$$$Previous deep-learning studies usually employ supervised neural networks to directly learn the spatial transformation from one image to another, requiring task-specific ground-truth registration for model training.$$$Due to the difficulty in collecting precise ground-truth registration, implementation of these supervised methods is practically challenging.$$$Although several unsupervised networks have been recently developed, these methods usually ignore the inherent inverse-consistent property (essential for diffeomorphic mapping) of transformations between a pair of images.$$$Also, existing approaches usually encourage the to-be-estimated transformation to be locally smooth via a smoothness constraint only, which could not completely avoid folding in the resulting transformation.$$$To this end, we propose an Inverse-Consistent deep Network (ICNet) for unsupervised deformable image registration.$$$Specifically, we develop an inverse-consistent constraint to encourage that a pair of images are symmetrically deformed toward one another, until both warped images are matched.$$$Besides using the conventional smoothness constraint, we also propose an anti-folding constraint to further avoid folding in the transformation.$$$The proposed method does not require any supervision information, while encouraging the diffeomoprhic property of the transformation via the proposed inverse-consistent and anti-folding constraints.$$$We evaluate our method on T1-weighted brain magnetic resonance imaging (MRI) scans for tissue segmentation and anatomical landmark detection, with results demonstrating the superior performance of our ICNet over several state-of-the-art approaches for deformable image registration.$$$Our code will be made publicly available.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS RESULTS/CONCLUSIONS OTHERS
D02043,"The auroras on Jupiter and Saturn can be studied with a high sensitivity and resolution by the Hubble Space Telescope (HST) ultraviolet (UV) and far-ultraviolet (FUV) Space Telescope spectrograph (STIS) and Advanced Camera for Surveys (ACS) instruments.$$$We present results of automatic detection and segmentation of Jupiter's auroral emissions as observed by HST ACS instrument with VOronoi Image SEgmentation (VOISE).$$$VOISE is a dynamic algorithm for partitioning the underlying pixel grid of an image into regions according to a prescribed homogeneity criterion.$$$The algorithm consists of an iterative procedure that dynamically constructs a tessellation of the image plane based on a Voronoi Diagram, until the intensity of the underlying image within each region is classified as homogeneous.$$$The computed tessellations allow the extraction of quantitative information about the auroral features such as mean intensity, latitudinal and longitudinal extents and length scales.$$$These outputs thus represent a more automated and objective method of characterising auroral emissions than manual inspection.",BACKGROUND/OBJECTIVES OBJECTIVES/METHODS/RESULTS OBJECTIVES/METHODS/RESULTS OBJECTIVES/METHODS/RESULTS OBJECTIVES/RESULTS RESULTS/CONCLUSIONS
D06128,"Spectrum sensing is the challenge for cognitive radio design and implementation, which allows the secondary user to access the primary bands without interference with primary users.$$$Cognitive radios should decide on the best spectrum band to meet the Quality of service requirements over all available spectrum bands.$$$This paper investigates the integrated centralized spectrum sensing techniques in multipath fading environment and the performance was analyzed with energy detection and wavelet based sensing techniques for unknown signal.$$$Keywords: Cognitive Radio, Spectrum Sensing, Signal Detection, Primary User, Secondary User",BACKGROUND OBJECTIVES METHODS/RESULTS OTHERS
D05694,"We present three major transitions that occur on the way to the elaborate and diverse societies of the modern era.$$$Our account links the worlds of social animals such as pigtail macaques and monk parakeets to examples from human history, including 18th Century London and the contemporary online phenomenon of Wikipedia.$$$From the first awareness and use of group-level social facts to the emergence of norms and their self-assembly into normative bundles, each transition represents a new relationship between the individual and the group.$$$At the center of this relationship is the use of coarse-grained information gained via lossy compression.$$$The role of top-down causation in the origin of society parallels that conjectured to occur in the origin and evolution of life itself.",OBJECTIVES OBJECTIVES RESULTS RESULTS CONCLUSIONS
D03140,"We propose the neural programmer-interpreter (NPI): a recurrent and compositional neural network that learns to represent and execute programs.$$$NPI has three learnable components: a task-agnostic recurrent core, a persistent key-value program memory, and domain-specific encoders that enable a single NPI to operate in multiple perceptually diverse environments with distinct affordances.$$$By learning to compose lower-level programs to express higher-level programs, NPI reduces sample complexity and increases generalization ability compared to sequence-to-sequence LSTMs.$$$The program memory allows efficient learning of additional tasks by building on existing programs.$$$NPI can also harness the environment (e.g. a scratch pad with read-write pointers) to cache intermediate results of computation, lessening the long-term memory burden on recurrent hidden units.$$$In this work we train the NPI with fully-supervised execution traces; each program has example sequences of calls to the immediate subprograms conditioned on the input.$$$Rather than training on a huge number of relatively weak labels, NPI learns from a small number of rich examples.$$$We demonstrate the capability of our model to learn several types of compositional programs: addition, sorting, and canonicalizing 3D models.$$$Furthermore, a single NPI learns to execute these programs and all 21 associated subprograms.",OBJECTIVES METHODS/RESULTS METHODS/RESULTS METHODS/RESULTS METHODS METHODS METHODS RESULTS RESULTS
D05683,"In this note we aim at putting more emphasis on the fact that trying to solve non-convex optimization problems with coordinate-descent iterative linear matrix inequality algorithms leads to suboptimal solutions, and put forward other optimization methods better equipped to deal with such problems (having theoretical convergence guarantees and/or being more efficient in practice).$$$This fact, already outlined at several places in the literature, still appears to be disregarded by a sizable part of the systems and control community.$$$Thus, main elements on this issue and better optimization alternatives are presented and illustrated by means of an example.",OBJECTIVES/CONCLUSIONS BACKGROUND METHODS
D01282,"Testing in Continuous Integration (CI) involves test case prioritization, selection, and execution at each cycle.$$$Selecting the most promising test cases to detect bugs is hard if there are uncertainties on the impact of committed code changes or, if traceability links between code and tests are not available.$$$This paper introduces Retecs, a new method for automatically learning test case selection and prioritization in CI with the goal to minimize the round-trip time between code commits and developer feedback on failed test cases.$$$The Retecs method uses reinforcement learning to select and prioritize test cases according to their duration, previous last execution and failure history.$$$In a constantly changing environment, where new test cases are created and obsolete test cases are deleted, the Retecs method learns to prioritize error-prone test cases higher under guidance of a reward function and by observing previous CI cycles.$$$By applying Retecs on data extracted from three industrial case studies, we show for the first time that reinforcement learning enables fruitful automatic adaptive test case selection and prioritization in CI and regression testing.",BACKGROUND BACKGROUND METHODS METHODS METHODS RESULTS
D02649,"This paper studies the problem of predicting the coding effort for a subsequent year of development by analysing metrics extracted from project repositories, with an emphasis on projects containing XML code.$$$The study considers thirteen open source projects and applies machine learning algorithms to generate models to predict one-year coding effort, measured in terms of lines of code added, modified and deleted.$$$Both organisational and code metrics associated to revisions are taken into account.$$$The results show that coding effort is highly determined by the expertise of developers while source code metrics have little effect on improving the accuracy of estimations of coding effort.$$$The study also shows that models trained on one project are unreliable at estimating effort in other projects.",OBJECTIVES METHODS METHODS CONCLUSIONS CONCLUSIONS
D04476,"Standard artificial neural networks suffer from the well-known issue of catastrophic forgetting, making continual or lifelong learning problematic.$$$Recently, numerous methods have been proposed for continual learning, but due to differences in evaluation protocols it is difficult to directly compare their performance.$$$To enable more meaningful comparisons, we identified three distinct continual learning scenarios based on whether task identity is known and, if it is not, whether it needs to be inferred.$$$Performing the split and permuted MNIST task protocols according to each of these scenarios, we found that regularization-based approaches (e.g., elastic weight consolidation) failed when task identity needed to be inferred.$$$In contrast, generative replay combined with distillation (i.e., using class probabilities as ""soft targets"") achieved superior performance in all three scenarios.$$$In addition, we reduced the computational cost of generative replay by integrating the generative model into the main model by equipping it with generative feedback connections.$$$This Replay-through-Feedback approach substantially shortened training time with no or negligible loss in performance.$$$We believe this to be an important first step towards making the powerful technique of generative replay scalable to real-world continual learning applications.",BACKGROUND BACKGROUND OBJECTIVES/METHODS/RESULTS METHODS/RESULTS METHODS/RESULTS OBJECTIVES/METHODS/RESULTS RESULTS CONCLUSIONS
D03908,"Tensors are a natural way to express correlations among many physical variables, but storing tensors in a computer naively requires memory which scales exponentially in the rank of the tensor.$$$This is not optimal, as the required memory is actually set not by the rank but by the mutual information amongst the variables in question.$$$Representations such as the tensor tree perform near-optimally when the tree decomposition is chosen to reflect the correlation structure in question, but making such a choice is non-trivial and good heuristics remain highly context-specific.$$$In this work I present two new algorithms for choosing efficient tree decompositions, independent of the physical context of the tensor.$$$The first is a brute-force algorithm which produces optimal decompositions up to truncation error but is generally impractical for high-rank tensors, as the number of possible choices grows exponentially in rank.$$$The second is a greedy algorithm, and while it is not optimal it performs extremely well in numerical experiments while having runtime which makes it practical even for tensors of very high rank.",BACKGROUND BACKGROUND BACKGROUND RESULTS/CONCLUSIONS METHODS/RESULTS METHODS/RESULTS
D01039,"This paper describes a new method of data encoding which may be used in various modern digital, computer and telecommunication systems and devices.$$$The method permits the compression of data for storage or transmission, allowing the exact original data to be reconstructed without any loss of content.$$$The method is characterized by the simplicity of implementation, as well as high speed and compression ratio.$$$The method is based on a unique scheme of binary-ternary prefix-free encoding of characters of the original data.$$$This scheme does not require the transmission of the code tables from encoder to decoder; allows for the linear presentation of the code lists; permits the usage of computable indexes of the prefix codes in a linear list for decoding; makes it possible to estimate the compression ratio prior to encoding; makes the usage of multiplication and division operations, as well as operations with the floating point unnecessary; proves to be effective for static as well as adaptive coding; applicable to character sets of any size; allows for repeated compression to improve the ratio.",METHODS RESULTS RESULTS OBJECTIVES/METHODS/RESULTS METHODS/RESULTS
D03170,"Most of open-source software systems become available on the internet today.$$$Thus, we need automatic methods to label software code.$$$Software code can be labeled with a set of keywords.$$$These keywords in this paper referred as software labels.$$$The goal of this paper is to provide a quick view of the software code vocabulary.$$$This paper proposes an automatic approach to document the object-oriented software by labeling its code.$$$The approach exploits all software identifiers to label software code.$$$The paper presents the results of study conducted on the ArgoUML and drawing shapes case studies.$$$Results showed that all code labels were correctly identified.",BACKGROUND OBJECTIVES OBJECTIVES OBJECTIVES OBJECTIVES METHODS METHODS RESULTS CONCLUSIONS
D06113,"Experimental life sciences like biology or chemistry have seen in the recent decades an explosion of the data available from experiments.$$$Laboratory instruments become more and more complex and report hundreds or thousands measurements for a single experiment and therefore the statistical methods face challenging tasks when dealing with such high dimensional data.$$$However, much of the data is highly redundant and can be efficiently brought down to a much smaller number of variables without a significant loss of information.$$$The mathematical procedures making possible this reduction are called dimensionality reduction techniques; they have widely been developed by fields like Statistics or Machine Learning, and are currently a hot research topic.$$$In this review we categorize the plethora of dimension reduction techniques available and give the mathematical insight behind them.",BACKGROUND OBJECTIVES OBJECTIVES BACKGROUND OBJECTIVES
D06660,"By executing jobs serially rather than in parallel, size-based scheduling policies can shorten time needed to complete jobs; however, major obstacles to their applicability are fairness guarantees and the fact that job sizes are rarely known exactly a-priori.$$$Here, we introduce the Pri family of size-based scheduling policies; Pri simulates any reference scheduler and executes jobs in the order of their simulated completion: we show that these schedulers give strong fairness guarantees, since no job completes later in Pri than in the reference policy.$$$In addition, we introduce PSBS, a practical implementation of such a scheduler: it works online (i.e., without needing knowledge of jobs submitted in the future), it has an efficient O(log n) implementation and it allows setting priorities to jobs.$$$Most importantly, unlike earlier size-based policies, the performance of PSBS degrades gracefully with errors, leading to performances that are close to optimal in a variety of realistic use cases.",BACKGROUND/OBJECTIVES METHODS/RESULTS RESULTS RESULTS/CONCLUSIONS
D06792,"Virtual reality allows to create situations which can be experimented under the control of the user, without risks, in a very flexible way.$$$This allows to develop skills and to have confidence to work in real conditions with real equipment.$$$VR is then widely used as a training and learning tool.$$$More recently, VR has also showed its potential in rehabilitation and therapy fields because it provides users with the ability of repeat their actions several times and to progress at their own pace.$$$In this communication, we present our work in the development of a wheelchair simulator designed to allow children with multiple disabilities to familiarize themselves with the wheelchair.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES
D01245,"Recent breakthroughs in Neural Architectural Search (NAS) have achieved state-of-the-art performance in many tasks such as image classification and language understanding.$$$However, most existing works only optimize for model accuracy and largely ignore other important factors imposed by the underlying hardware and devices, such as latency and energy, when making inference.$$$In this paper, we first introduce the problem of NAS and provide a survey on recent works.$$$Then we deep dive into two recent advancements on extending NAS into multiple-objective frameworks: MONAS and DPP-Net.$$$Both MONAS and DPP-Net are capable of optimizing accuracy and other objectives imposed by devices, searching for neural architectures that can be best deployed on a wide spectrum of devices: from embedded systems and mobile devices to workstations.$$$Experimental results are poised to show that architectures found by MONAS and DPP-Net achieves Pareto optimality w.r.t the given objectives for various devices.",BACKGROUND OBJECTIVES METHODS METHODS RESULTS CONCLUSIONS
D02878,"In the cloud computing environment, cloud virtual machine (VM) will be more and more the number of virtual machine security and management faced giant Challenge.$$$In order to address security issues cloud computing virtualization environment, this paper presents a virtual machine based on efficient and dynamic deployment VM security management model state migration and scheduling, study of which virtual machine security architecture, based on AHP (Analytic Hierarchy Process) virtual machine deployment and scheduling method, based on CUSUM (Cumulative Sum) DDoS attack detection algorithm, and the above-described method for functional testing and validation.",BACKGROUND RESULTS/CONCLUSIONS
D03246,"Social media are becoming an increasingly important source of information about the public mood regarding issues such as elections, Brexit, stock market, etc.$$$In this paper we focus on sentiment classification of Twitter data.$$$Construction of sentiment classifiers is a standard text mining task, but here we address the question of how to properly evaluate them as there is no settled way to do so.$$$Sentiment classes are ordered and unbalanced, and Twitter produces a stream of time-ordered data.$$$The problem we address concerns the procedures used to obtain reliable estimates of performance measures, and whether the temporal ordering of the training and test data matters.$$$We collected a large set of 1.5 million tweets in 13 European languages.$$$We created 138 sentiment models and out-of-sample datasets, which are used as a gold standard for evaluations.$$$The corresponding 138 in-sample datasets are used to empirically compare six different estimation procedures: three variants of cross-validation, and three variants of sequential validation (where test set always follows the training set).$$$We find no significant difference between the best cross-validation and sequential validation.$$$However, we observe that all cross-validation variants tend to overestimate the performance, while the sequential methods tend to underestimate it.$$$Standard cross-validation with random selection of examples is significantly worse than the blocked cross-validation, and should not be used to evaluate classifiers in time-ordered data scenarios.",BACKGROUND OBJECTIVES BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS RESULTS RESULTS CONCLUSIONS
D03543,"The integrated radar and communication system is promising in the next generation wireless communication networks.$$$However, its performance is confined by the limited energy.$$$In order to overcome it, a wireless powered integrated radar and communication system is proposed.$$$An energy minimization problem is formulated subject to constraints on the radar and communication performances.$$$The energy beamforming and radar-communication waveform are jointly optimized to minimize the consumption energy.$$$The challenging non-convex problem is solved by using semidefinite relaxation and auxiliary variable methods.$$$It is proved that the optimal solution can be obtained.$$$Simulation results demonstrate that our proposed optimal design outperforms the benchmark scheme.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS RESULTS RESULTS
D00697,"Convolutional networks (ConvNets) have achieved great successes in various challenging vision tasks.$$$However, the performance of ConvNets would degrade when encountering the domain shift.$$$The domain adaptation is more significant while challenging in the field of biomedical image analysis, where cross-modality data have largely different distributions.$$$Given that annotating the medical data is especially expensive, the supervised transfer learning approaches are not quite optimal.$$$In this paper, we propose an unsupervised domain adaptation framework with adversarial learning for cross-modality biomedical image segmentations.$$$Specifically, our model is based on a dilated fully convolutional network for pixel-wise prediction.$$$Moreover, we build a plug-and-play domain adaptation module (DAM) to map the target input to features which are aligned with source domain feature space.$$$A domain critic module (DCM) is set up for discriminating the feature space of both domains.$$$We optimize the DAM and DCM via an adversarial loss without using any target domain label.$$$Our proposed method is validated by adapting a ConvNet trained with MRI images to unpaired CT data for cardiac structures segmentations, and achieved very promising results.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES METHODS METHODS METHODS METHODS RESULTS/CONCLUSIONS
D05817,"A novel Mathematical Random Number Generator (MRNG) is presented here.$$$In this case, ""mathematical"" refers to the fact that to construct that generator it is not necessary to resort to a physical phenomenon, such as the thermal noise of an electronic device, but rather to a mathematical procedure.$$$The MRNG generates binary strings - in principle, as long as desired - which may be considered genuinely random in the sense that they pass the statistical tests currently accepted to evaluate the randomness of those strings.$$$From those strings, the MRNG also generates random numbers expressed in base 10.$$$An MRNG has been installed as a facility on the following web page: http://www.appliedmathgroup.org.$$$This generator may be used for applications in tasks in: a) computational simulation of probabilistic-type systems, and b) the random selection of samples of different populations.$$$Users interested in applications in cryptography can build another MRNG, but they would have to withhold information - specified in section 5 - from people who are not authorized to decode messages encrypted using that resource.",OTHERS BACKGROUND OBJECTIVES OTHERS RESULTS METHODS METHODS
D03847,"In this paper a Metaheuristic approach for solving the N-Queens Problem is introduced to find the best possible solution in a reasonable amount of time.$$$Genetic Algorithm is used with a novel fitness function as the Metaheuristic.$$$The aim of N-Queens Problem is to place N queens on an N x N chessboard, in a way so that no queen is in conflict with the others.$$$Chromosome representation and genetic operations like Mutation and Crossover are described in detail.$$$Results show that this approach yields promising and satisfactory results in less time compared to that obtained from the previous approaches for several large values of N.",BACKGROUND/METHODS METHODS OBJECTIVES OTHERS RESULTS/CONCLUSIONS
D01836,"As open-ended human-chatbot interaction becomes commonplace, sensitive content detection gains importance.$$$In this work, we propose a two stage semi-supervised approach to bootstrap large-scale data for automatic sensitive language detection from publicly available web resources.$$$We explore various data selection methods including 1) using a blacklist to rank online discussion forums by the level of their sensitiveness followed by randomly sampling utterances and 2) training a weakly supervised model in conjunction with the blacklist for scoring sentences from online discussion forums to curate a dataset.$$$Our data collection strategy is flexible and allows the models to detect implicit sensitive content for which manual annotations may be difficult.$$$We train models using publicly available annotated datasets as well as using the proposed large-scale semi-supervised datasets.$$$We evaluate the performance of all the models on Twitter and Toxic Wikipedia comments testsets as well as on a manually annotated spoken language dataset collected during a large scale chatbot competition.$$$Results show that a model trained on this collected data outperforms the baseline models by a large margin on both in-domain and out-of-domain testsets, achieving an F1 score of 95.5% on an out-of-domain testset compared to a score of 75% for models trained on public datasets.$$$We also showcase that large scale two stage semi-supervision generalizes well across multiple classes of sensitivities such as hate speech, racism, sexual and pornographic content, etc. without even providing explicit labels for these classes, leading to an average recall of 95.5% versus the models trained using annotated public datasets which achieve an average recall of 73.2% across seven sensitive classes on out-of-domain testsets.",BACKGROUND OBJECTIVES METHODS RESULTS/CONCLUSIONS METHODS METHODS RESULTS RESULTS
D05699,"This paper describes LIUM submissions to WMT17 News Translation Task for English-German, English-Turkish, English-Czech and English-Latvian language pairs.$$$We train BPE-based attentive Neural Machine Translation systems with and without factored outputs using the open source nmtpy framework.$$$Competitive scores were obtained by ensembling various systems and exploiting the availability of target monolingual corpora for back-translation.$$$The impact of back-translation quantity and quality is also analyzed for English-Turkish where our post-deadline submission surpassed the best entry by +1.6 BLEU.",BACKGROUND METHODS METHODS/RESULTS/CONCLUSIONS RESULTS
D03298,"In ultra-wideband (UWB) communication systems with impulse radio (IR) modulation, the bandwidth is usually 1GHz or more.$$$To process the received signal digitally, high sampling rate analog-digital-converters (ADC) are required.$$$Due to the high complexity and large power consumption, monobit ADC is appropriate.$$$The optimal monobit receiver has been derived.$$$But it is not efficient to combat intersymbol interference (ISI).$$$Decision feedback equalization (DFE) is an effect way dealing with ISI.$$$In this paper, we proposed a algorithm that combines Viterbi decoding and DFE together for monobit receivers.$$$In this way, we suppress the impact of ISI effectively, thus improving the bit error rate (BER) performance.$$$By state expansion, we achieve better performance.$$$The simulation results show that the algorithm has about 1dB SNR gain compared to separate demodulation and decoding method and 1dB loss compared to the BER performance in the channel without ISI.$$$Compare to the full resolution detection in fading channel without ISI, it has 3dB SNR loss after state expansion.",BACKGROUND OBJECTIVES OBJECTIVES METHODS METHODS METHODS OBJECTIVES OBJECTIVES RESULTS CONCLUSIONS CONCLUSIONS
D04012,"Computing layer similarities is an important way of characterizing multiplex networks because various static properties and dynamic processes depend on the relationships between layers.$$$We provide a taxonomy and experimental evaluation of approaches to compare layers in multiplex networks.$$$Our taxonomy includes, systematizes and extends existing approaches, and is complemented by a set of practical guidelines on how to apply them.",BACKGROUND RESULTS CONCLUSIONS
D04483,"This paper examines the convergence of no-regret learning in games with continuous action sets.$$$For concreteness, we focus on learning via ""dual averaging"", a widely used class of no-regret learning schemes where players take small steps along their individual payoff gradients and then ""mirror"" the output back to their action sets.$$$In terms of feedback, we assume that players can only estimate their payoff gradients up to a zero-mean error with bounded variance.$$$To study the convergence of the induced sequence of play, we introduce the notion of variational stability, and we show that stable equilibria are locally attracting with high probability whereas globally stable equilibria are globally attracting with probability 1.$$$We also discuss some applications to mixed-strategy learning in finite games, and we provide explicit estimates of the method's convergence speed.",OBJECTIVES BACKGROUND BACKGROUND RESULTS RESULTS
D03613,"Recommender systems take inputs from user history, use an internal ranking algorithm to generate results and possibly optimize this ranking based on feedback.$$$However, often the recommender system is unaware of the actual intent of the user and simply provides recommendations dynamically without properly understanding the thought process of the user.$$$An intelligent recommender system is not only useful for the user but also for businesses which want to learn the tendencies of their users.$$$Finding out tendencies or intents of a user is a difficult problem to solve.$$$Keeping this in mind, we sought out to create an intelligent system which will keep track of the user's activity on a web-application as well as determine the intent of the user in each session.$$$We devised a way to encode the user's activity through the sessions.$$$Then, we have represented the information seen by the user in a high dimensional format which is reduced to lower dimensions using tensor factorization techniques.$$$The aspect of intent awareness (or scoring) is dealt with at this stage.$$$Finally, combining the user activity data with the contextual information gives the recommendation score.$$$The final recommendations are then ranked using filtering and collaborative recommendation techniques to show the top-k recommendations to the user.$$$A provision for feedback is also envisioned in the current system which informs the model to update the various weights in the recommender system.$$$Our overall model aims to combine both frequency-based and context-based recommendation systems and quantify the intent of a user to provide better recommendations.$$$We ran experiments on real-world timestamped user activity data, in the setting of recommending reports to the users of a business analytics tool and the results are better than the baselines.$$$We also tuned certain aspects of our model to arrive at optimized results.",BACKGROUND BACKGROUND/OBJECTIVES BACKGROUND/OBJECTIVES BACKGROUND/OBJECTIVES OBJECTIVES METHODS METHODS METHODS METHODS METHODS METHODS OBJECTIVES/METHODS METHODS/RESULTS RESULTS
D04563,"We revisit the problem of asymmetric binary hypothesis testing against a composite alternative hypothesis.$$$We introduce a general framework to treat such problems when the alternative hypothesis adheres to certain axioms.$$$In this case we find the threshold rate, the optimal error and strong converse exponents (at large deviations from the threshold) and the second order asymptotics (at small deviations from the threshold).$$$We apply our results to find operational interpretations of various Renyi information measures.$$$In case the alternative hypothesis is comprised of bipartite product distributions, we find that the optimal error and strong converse exponents are determined by variations of Renyi mutual information.$$$In case the alternative hypothesis consists of tripartite distributions satisfying the Markov property, we find that the optimal exponents are determined by variations of Renyi conditional mutual information.$$$In either case the relevant notion of Renyi mutual information depends on the precise choice of the alternative hypothesis.$$$As such, our work also strengthens the view that different definitions of Renyi mutual information, conditional entropy and conditional mutual information are adequate depending on the context in which the measures are used.",OBJECTIVES RESULTS RESULTS RESULTS RESULTS RESULTS RESULTS CONCLUSIONS
D03579,"Traditional pattern mining algorithms generally suffer from a lack of flexibility.$$$In this paper, we propose a SAT formulation of the problem to successfully mine frequent flexible sequences occurring in transactional datasets.$$$Our SAT-based approach can easily be extended with extra constraints to address a broad range of pattern mining applications.$$$To demonstrate this claim, we formulate and add several constraints, such as gap and span constraints, to our model in order to extract more specific patterns.$$$We also use interactive solving to perform important derived tasks, such as closed pattern mining or maximal pattern mining.$$$Finally, we prove the practical feasibility of our SAT model by running experiments on two real datasets.",BACKGROUND OBJECTIVES OBJECTIVES METHODS METHODS RESULTS/CONCLUSIONS
D05821,This paper describes the monomodal and multimodal Neural Machine Translation systems developed by LIUM and CVC for WMT17 Shared Task on Multimodal Translation.$$$We mainly explored two multimodal architectures where either global visual features or convolutional feature maps are integrated in order to benefit from visual context.$$$Our final systems ranked first for both En-De and En-Fr language pairs according to the automatic evaluation metrics METEOR and BLEU.,OTHERS METHODS RESULTS
D00379,"Numerous fake images spread on social media today and can severely jeopardize the credibility of online content to public.$$$In this paper, we employ deep networks to learn distinct fake image related features.$$$In contrast to authentic images, fake images tend to be eye-catching and visually striking.$$$Compared with traditional visual recognition tasks, it is extremely challenging to understand these psychologically triggered visual patterns in fake images.$$$Traditional general image classification datasets, such as ImageNet set, are designed for feature learning at the object level but are not suitable for learning the hyper-features that would be required by image credibility analysis.$$$In order to overcome the scarcity of training samples of fake images, we first construct a large-scale auxiliary dataset indirectly related to this task.$$$This auxiliary dataset contains 0.6 million weakly-labeled fake and real images collected automatically from social media.$$$Through an AdaBoost-like transfer learning algorithm, we train a CNN model with a few instances in the target training set and 0.6 million images in the collected auxiliary set.$$$This learning algorithm is able to leverage knowledge from the auxiliary set and gradually transfer it to the target task.$$$Experiments on a real-world testing set show that our proposed domain transferred CNN model outperforms several competing baselines.$$$It obtains superiror results over transfer learning methods based on the general ImageNet set.$$$Moreover, case studies show that our proposed method reveals some interesting patterns for distinguishing fake and authentic images.",BACKGROUND OBJECTIVES BACKGROUND/RESULTS BACKGROUND BACKGROUND METHODS RESULTS METHODS METHODS RESULTS RESULTS/CONCLUSIONS CONCLUSIONS
D06773,"We define the task of salient structure (SS) detection to unify the saliency-related tasks like fixation prediction, salient object detection, and other detection of structures of interest.$$$In this study, we propose a unified framework for SS detection by modeling the two-pathway-based guided search strategy of biological vision.$$$Firstly, context-based spatial prior (CBSP) is extracted based on the layout of edges in the given scene along a fast visual pathway, called non-selective pathway.$$$This is a rough and non-selective estimation of the locations where the potential SSs present.$$$Secondly, another flow of local feature extraction is executed in parallel along the selective pathway.$$$Finally, Bayesian inference is used to integrate local cues guided by CBSP, and to predict the exact locations of SSs in the input scene.$$$The proposed model is invariant to size and features of objects.$$$Experimental results on four datasets (two fixation prediction datasets and two salient object datasets) demonstrate that our system achieves competitive performance for SS detection (i.e., both the tasks of fixation prediction and salient object detection) comparing to the state-of-the-art methods.",OBJECTIVES OBJECTIVES METHODS METHODS METHODS METHODS RESULTS RESULTS
D06337,"Deep learning has demonstrated abilities to learn complex structures, but they can be restricted by available data.$$$Recently, Consensus Networks (CNs) were proposed to alleviate data sparsity by utilizing features from multiple modalities, but they too have been limited by the size of labeled data.$$$In this paper, we extend CN to Transductive Consensus Networks (TCNs), suitable for semi-supervised learning.$$$In TCNs, different modalities of input are compressed into latent representations, which we encourage to become indistinguishable during iterative adversarial training.$$$To understand TCNs two mechanisms, consensus and classification, we put forward its three variants in ablation studies on these mechanisms.$$$To further investigate TCN models, we treat the latent representations as probability distributions and measure their similarities as the negative relative Jensen-Shannon divergences.$$$We show that a consensus state beneficial for classification desires a stable but imperfect similarity between the representations.$$$Overall, TCNs outperform or align with the best benchmark algorithms given 20 to 200 labeled samples on the Bank Marketing and the DementiaBank datasets.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS RESULTS CONCLUSIONS
D00200,"The information available to robots in real tasks is widely distributed both in time and space, requiring the agent to search for relevant data.$$$In humans, that face the same problem when sounds, images and smells are presented to their sensors in a daily scene, a natural system is applied: Attention.$$$As vision plays an important role in our routine, most research regarding attention has involved this sensorial system and the same has been replicated to the robotics field.$$$However,most of the robotics tasks nowadays do not rely only in visual data, that are still costly.$$$To allow the use of attentive concepts with other robotics sensors that are usually used in tasks such as navigation, self-localization, searching and mapping, a generic attentional model has been previously proposed.$$$In this work, feature mapping functions were designed to build feature maps to this attentive model from data from range scanner and sonar sensors.$$$Experiments were performed in a high fidelity simulated robotics environment and results have demonstrated the capability of the model on dealing with both salient stimuli and goal-driven attention over multiple features extracted from multiple sensors.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS/RESULTS/CONCLUSIONS
D02684,"Using an information theoretic point of view, we investigate how a dynamics acting on a network can be coarse grained through the use of graph partitions.$$$Specifically, we are interested in how aggregating the state space of a Markov process according to a partition impacts on the thus obtained lower-dimensional dynamics.$$$We highlight that for a dynamics on a particular graph there may be multiple coarse grained descriptions that capture different, incomparable features of the original process.$$$For instance, a coarse graining induced by one partition may be commensurate with a time-scale separation in the dynamics, while another coarse graining may correspond to a different lower-dimensional dynamics that preserves the Markov property of the original process.$$$Taking inspiration from the literature of Computational Mechanics, we find that a convenient tool to summarise and visualise such dynamical properties of a coarse grained model (partition) is the entrogram.$$$The entrogram gathers certain information-theoretic measures, which quantify how information flows across time steps.$$$These information theoretic quantities include the entropy rate, as well as a measure for the memory contained in the process, i.e., how well the dynamics can be approximated by a first order Markov process.$$$We use the entrogram to investigate how specific macro-scale connection patterns in the state-space transition graph of the original dynamics result in desirable properties of coarse grained descriptions.$$$We thereby provide a fresh perspective on the interplay between structure and dynamics in networks, and the process of partitioning from an information theoretic perspective.$$$We focus on networks that may be approximated by both a core-periphery or a clustered organization, and highlight that each of these coarse grained descriptions can capture different aspects of a Markov process acting on the network.",OBJECTIVES OBJECTIVES RESULTS RESULTS METHODS METHODS METHODS RESULTS CONCLUSIONS CONCLUSIONS
D05066,"This paper proposes a joint segmentation and deconvolution Bayesian method for medical ultrasound (US) images.$$$Contrary to piecewise homogeneous images, US images exhibit heavy characteristic speckle patterns correlated with the tissue structures.$$$The generalized Gaussian distribution (GGD) has been shown to be one of the most relevant distributions for characterizing the speckle in US images.$$$Thus, we propose a GGD-Potts model defined by a label map coupling US image segmentation and deconvolution.$$$The Bayesian estimators of the unknown model parameters, including the US image, the label map and all the hyperparameters are difficult to be expressed in closed form.$$$Thus, we investigate a Gibbs sampler to generate samples distributed according to the posterior of interest.$$$These generated samples are finally used to compute the Bayesian estimators of the unknown parameters.$$$The performance of the proposed Bayesian model is compared with existing approaches via several experiments conducted on realistic synthetic data and in vivo US images.",OBJECTIVES BACKGROUND BACKGROUND METHODS METHODS METHODS METHODS RESULTS
D03617,"Multi-tenant cloud networks have various security and monitoring service functions (SFs) that constitute a service function chain (SFC) between two endpoints.$$$SF rule ordering overlaps and policy conflicts can cause increased latency, service disruption and security breaches in cloud networks.$$$Software Defined Network (SDN) based Network Function Virtualization (NFV) has emerged as a solution that allows dynamic SFC composition and traffic steering in a cloud network.$$$We propose an SDN enabled Universal Policy Checking (SUPC) framework, to provide 1) Flow Composition and Ordering by translating various SF rules into the OpenFlow format.$$$This ensures elimination of redundant rules and policy compliance in SFC.$$$2) Flow conflict analysis to identify conflicts in header space and actions between various SF rules.$$$Our results show a significant reduction in SF rules on composition.$$$Additionally, our conflict checking mechanism was able to identify several rule conflicts that pose security, efficiency, and service availability issues in the cloud network.",BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS METHODS/RESULTS RESULTS/CONCLUSIONS CONCLUSIONS
D05362,"We evaluated the effectiveness of an automated bird sound identification system in a situation that emulates a realistic, typical application.$$$We trained classification algorithms on a crowd-sourced collection of bird audio recording data and restricted our training methods to be completely free of manual intervention.$$$The approach is hence directly applicable to the analysis of multiple species collections, with labelling provided by crowd-sourced collection.$$$We evaluated the performance of the bird sound recognition system on a realistic number of candidate classes, corresponding to real conditions.$$$We investigated the use of two canonical classification methods, chosen due to their widespread use and ease of interpretation, namely a k Nearest Neighbour (kNN) classifier with histogram-based features and a Support Vector Machine (SVM) with time-summarisation features.$$$We further investigated the use of a certainty measure, derived from the output probabilities of the classifiers, to enhance the interpretability and reliability of the class decisions.$$$Our results demonstrate that both identification methods achieved similar performance, but we argue that the use of the kNN classifier offers somewhat more flexibility.$$$Furthermore, we show that employing an outcome certainty measure provides a valuable and consistent indicator of the reliability of classification results.$$$Our use of generic training data and our investigation of probabilistic classification methodologies that can flexibly address the variable number of candidate species/classes that are expected to be encountered in the field, directly contribute to the development of a practical bird sound identification system with potentially global application.$$$Further, we show that certainty measures associated with identification outcomes can significantly contribute to the practical usability of the overall system.",BACKGROUND/OBJECTIVES METHODS OBJECTIVES BACKGROUND/OBJECTIVES METHODS METHODS RESULTS RESULTS/CONCLUSIONS BACKGROUND BACKGROUND/RESULTS/CONCLUSIONS
D02120,"The rapidly growing size of RDF graphs in recent years necessitates distributed storage and parallel processing strategies.$$$To obtain efficient query processing using computer clusters a wide variety of different approaches have been proposed.$$$Related to the approach presented in the current paper are systems built on top of Hadoop HDFS, for example using Apache Accumulo or using Apache Spark.$$$We present a new RDF store called PRoST (Partitioned RDF on Spark Tables) based on Apache Spark.$$$PRoST introduces an innovative strategy that combines the Vertical Partitioning approach with the Property Table, two preexisting models for storing RDF datasets.$$$We demonstrate that our proposal outperforms state-of-the-art systems w.r.t. the runtime for a wide range of query types and without any extensive precomputing phase.",BACKGROUND OBJECTIVES METHODS METHODS METHODS RESULTS
D06897,"The safety, mobility, environmental, energy, and economic benefits of transportation systems, which are the focus of recent Connected Vehicles (CVs) programs, are potentially dramatic.$$$However, realization of these benefits largely hinges on the timely integration of the digital technology into the existing transportation infrastructure.$$$CVs must be enabled to broadcast and receive data to and from other CVs (Vehicle-to-Vehicle, or V2V communication), to and from infrastructure (Vehicle-to-Infrastructure, or V2I, communication) and to and from other road users, such as bicyclists or pedestrians (Vehicle-to-Other road users communication).$$$Further, for V2I-focused applications, the infrastructure and the transportation agencies that manage it must be able to collect, process, distribute, and archive these data quickly, reliably, and securely.$$$This paper focuses V2I applications, and studies current digital roadway infrastructure initiatives.$$$It highlights the importance of including digital infrastructure investment alongside investment in more traditional transportation infrastructure to keep up with the auto industrys push towards connecting vehicles to other vehicles.$$$By studying the current CV testbeds and Smart City initiatives, this paper identifies digital infrastructure components being used by public agencies.$$$It also examines public agencies limited budgeting for digital infrastructure, and finds current expenditure is inadequate for realizing the potential benefits of V2I applications.$$$Finally, the paper presents a set of recommendations, based on a review of current practices and future needs, designed to guide agencies responsible for transportation infrastructure.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES METHODS METHODS METHODS
D03382,"As the popularity of electric vehicles increases, the demand for more power can increase more rapidly than our ability to install additional generating capacity.$$$In the long term we expect that the supply and demand will become balanced.$$$However, in the interim the rate at which electric vehicles can be deployed will depend on our ability to charge these vehicles without inconveniencing their owners.$$$In this paper, we investigate using fairness mechanisms to distribute power to electric vehicles on a smart grid.$$$We assume that during peak demand there is insufficient power to charge all the vehicles simultaneously.$$$In each five minute interval of time we select a subset of the vehicles to charge, based upon information about the vehicles.$$$We evaluate the selection mechanisms using published data on the current demand for electric power as a function of time of day, current driving habits for commuting, and the current rates at which electric vehicles can be charged on home outlets.$$$We found that conventional selection strategies, such as first-come-first-served or round robin, may delay a significant fraction of the vehicles by more than two hours, even when the total available power over the course of a day is two or three times the power required by the vehicles.$$$However, a selection mechanism that minimizes the maximum delay can reduce the delays to a few minutes, even when the capacity available for charging electric vehicles exceeds their requirements by as little as 5%.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OTHERS OTHERS METHODS RESULTS RESULTS
D03708,"We show how to extend traditional intrinsic image decompositions to incorporate further layers above albedo and shading.$$$It is hard to obtain data to learn a multi-layer decomposition.$$$Instead, we can learn to decompose an image into layers that are ""like this"" by authoring generative models for each layer using proxy examples that capture the Platonic ideal (Mondrian images for albedo; rendered 3D primitives for shading; material swatches for shading detail).$$$Our method then generates image layers, one from each model, that explain the image.$$$Our approach rests on innovation in generative models for images.$$$We introduce a Convolutional Variational Auto Encoder (conv-VAE), a novel VAE architecture that can reconstruct high fidelity images.$$$The approach is general, and does not require that layers admit a physical interpretation.",RESULTS/CONCLUSIONS BACKGROUND METHODS METHODS METHODS OBJECTIVES/METHODS OBJECTIVES
D01601,"We first explain the notion of secret sharing and also threshold schemes, which can be implemented with the Shamir's secret sharing.$$$Subsequently, we review social secret sharing (NSG'10,NS'10) and its trust function.$$$In a secret sharing scheme, a secret is shared among a group of players who can later recover the secret.$$$We review the construction of a social secret sharing scheme and its application for resource management in cloud, as explained in NS'12.$$$To clarify the social secret sharing scheme, we first review its trust function according to NL'06.$$$In this scheme, a secret is maintained by assigning a trust value to each player based on his behavior, i.e., availability.",BACKGROUND/OBJECTIVES/RESULTS OTHERS OBJECTIVES/METHODS METHODS/RESULTS METHODS BACKGROUND/OBJECTIVES
D05028,"We present an algorithm for creating high resolution anatomically plausible images consistent with acquired clinical brain MRI scans with large inter-slice spacing.$$$Although large data sets of clinical images contain a wealth of information, time constraints during acquisition result in sparse scans that fail to capture much of the anatomy.$$$These characteristics often render computational analysis impractical as many image analysis algorithms tend to fail when applied to such images.$$$Highly specialized algorithms that explicitly handle sparse slice spacing do not generalize well across problem domains.$$$In contrast, we aim to enable application of existing algorithms that were originally developed for high resolution research scans to significantly undersampled scans.$$$We introduce a generative model that captures fine-scale anatomical structure across subjects in clinical image collections and derive an algorithm for filling in the missing data in scans with large inter-slice spacing.$$$Our experimental results demonstrate that the resulting method outperforms state-of-the-art upsampling super-resolution techniques, and promises to facilitate subsequent analysis not previously possible with scans of this quality.$$$Our implementation is freely available at https://github.com/adalca/papago .",METHODS BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS/CONCLUSIONS OTHERS
D02716,"We provide theoretical investigation of curriculum learning in the context of stochastic gradient descent when optimizing the convex linear regression loss.$$$We prove that the rate of convergence of an ideal curriculum learning method is monotonically increasing with the difficulty of the examples.$$$Moreover, among all equally difficult points, convergence is faster when using points which incur higher loss with respect to the current hypothesis.$$$We then analyze curriculum learning in the context of training a CNN.$$$We describe a method which infers the curriculum by way of transfer learning from another network, pre-trained on a different task.$$$While this approach can only approximate the ideal curriculum, we observe empirically similar behavior to the one predicted by the theory, namely, a significant boost in convergence speed at the beginning of training.$$$When the task is made more difficult, improvement in generalization performance is also observed.$$$Finally, curriculum learning exhibits robustness against unfavorable conditions such as excessive regularization.",BACKGROUND/OBJECTIVES RESULTS RESULTS RESULTS METHODS METHODS RESULTS RESULTS
D02803,"We compare the visibility of Latin American and Caribbean (LAC) publications in the Core Collection indexes of the Web of Science (WoS)--Science Citation Index Expanded, Social Sciences Citation Index, and Arts & Humanities Citation Index--and the SciELO Citation Index (SciELO CI) which was integrated into the larger WoS platform in 2014.$$$The purpose of this comparison is to contribute to our understanding of the communication of scientific knowledge produced in Latin America and the Caribbean, and to provide some reflections on the potential benefits of the articulation of regional indexing exercises into WoS for a better understanding of geographic and disciplinary contributions.$$$How is the regional level of SciELO CI related to the global range of WoS?$$$In WoS, LAC authors are integrated at the global level in international networks, while SciELO has provided a platform for interactions among LAC researchers.$$$The articulation of SciELO into WoS may improve the international visibility of the regional journals, but at the cost of independent journal inclusion criteria.",OBJECTIVES OBJECTIVES OBJECTIVES RESULTS CONCLUSIONS
D04882,"While RANSAC-based methods are robust to incorrect image correspondences (outliers), their hypothesis generators are not robust to correct image correspondences (inliers) with positional error (noise).$$$This slows down their convergence because hypotheses drawn from a minimal set of noisy inliers can deviate significantly from the optimal model.$$$This work addresses this problem by introducing ANSAC, a RANSAC-based estimator that accounts for noise by adaptively using more than the minimal number of correspondences required to generate a hypothesis.$$$ANSAC estimates the inlier ratio (the fraction of correct correspondences) of several ranked subsets of candidate correspondences and generates hypotheses from them.$$$Its hypothesis-generation mechanism prioritizes the use of subsets with high inlier ratio to generate high-quality hypotheses.$$$ANSAC uses an early termination criterion that keeps track of the inlier ratio history and terminates when it has not changed significantly for a period of time.$$$The experiments show that ANSAC finds good homography and fundamental matrix estimates in a few iterations, consistently outperforming state-of-the-art methods.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS RESULTS/CONCLUSIONS
D05659,"Motivated by applications in databases, this paper considers various fragments of the calculus of binary relations.$$$The fragments are obtained by leaving out, or keeping in, some of the standard operators, along with some derived operators such as set difference, projection, coprojection, and residuation.$$$For each considered fragment, a characterization is obtained for when two given binary relational structures are indistinguishable by expressions in that fragment.$$$The characterizations are based on appropriately adapted notions of simulation and bisimulation.",OBJECTIVES METHODS RESULTS METHODS
D05195,"The implementation of smart building technology in the form of smart infrastructure applications has great potential to improve sustainability and energy efficiency by leveraging humans-in-the-loop strategy.$$$However, human preference in regard to living conditions is usually unknown and heterogeneous in its manifestation as control inputs to a building.$$$Furthermore, the occupants of a building typically lack the independent motivation necessary to contribute to and play a key role in the control of smart building infrastructure.$$$Moreover, true human actions and their integration with sensing/actuation platforms remains unknown to the decision maker tasked with improving operational efficiency.$$$By modeling user interaction as a sequential discrete game between non-cooperative players, we introduce a gamification approach for supporting user engagement and integration in a human-centric cyber-physical system.$$$We propose the design and implementation of a large-scale network game with the goal of improving the energy efficiency of a building through the utilization of cutting-edge Internet of Things (IoT) sensors and cyber-physical systems sensing/actuation platforms.$$$A benchmark utility learning framework that employs robust estimations for classical discrete choice models provided for the derived high dimensional imbalanced data.$$$To improve forecasting performance, we extend the benchmark utility learning scheme by leveraging Deep Learning end-to-end training with Deep bi-directional Recurrent Neural Networks.$$$We apply the proposed methods to high dimensional data from a social game experiment designed to encourage energy efficient behavior among smart building occupants in Nanyang Technological University (NTU) residential housing.$$$Using occupant-retrieved actions for resources such as lighting and A/C, we simulate the game defined by the estimated utility functions.",OBJECTIVES OTHERS OTHERS BACKGROUND METHODS METHODS METHODS METHODS RESULTS CONCLUSIONS
D02664,"A marginal problem asks whether a given family of marginal distributions for some set of random variables arises from some joint distribution of these variables.$$$Here we point out that the existence of such a joint distribution imposes non-trivial conditions already on the level of Shannon entropies of the given marginals.$$$These entropic inequalities are necessary (but not sufficient) criteria for the existence of a joint distribution.$$$For every marginal problem, a list of such Shannon-type entropic inequalities can be calculated by Fourier-Motzkin elimination, and we offer a software interface to a Fourier-Motzkin solver for doing so.$$$For the case that the hypergraph of given marginals is a cycle graph, we provide a complete analytic solution to the problem of classifying all relevant entropic inequalities, and use this result to bound the decay of correlations in stochastic processes.$$$Furthermore, we show that Shannon-type inequalities for differential entropies are not relevant for continuous-variable marginal problems; non-Shannon-type inequalities are, both in the discrete and in the continuous case.$$$In contrast to other approaches, our general framework easily adapts to situations where one has additional (conditional) independence requirements on the joint distribution, as in the case of graphical models.$$$We end with a list of open problems.$$$A complementary article discusses applications to quantum nonlocality and contextuality.",BACKGROUND RESULTS CONCLUSIONS METHODS RESULTS RESULTS OTHERS OTHERS OTHERS
D06738,"The task of determining labels of all network nodes based on the knowledge about network structure and labels of some training subset of nodes is called the within-network classification.$$$It may happen that none of the labels of the nodes is known and additionally there is no information about number of classes to which nodes can be assigned.$$$In such a case a subset of nodes has to be selected for initial label acquisition.$$$The question that arises is: ""labels of which nodes should be collected and used for learning in order to provide the best classification accuracy for the whole network?"".$$$Active learning and inference is a practical framework to study this problem.$$$A set of methods for active learning and inference for within network classification is proposed and validated.$$$The utility score calculation for each node based on network structure is the first step in the process.$$$The scores enable to rank the nodes.$$$Based on the ranking, a set of nodes, for which the labels are acquired, is selected (e.g. by taking top or bottom N from the ranking).$$$The new measure-neighbour methods proposed in the paper suggest not obtaining labels of nodes from the ranking but rather acquiring labels of their neighbours.$$$The paper examines 29 distinct formulations of utility score and selection methods reporting their impact on the results of two collective classification algorithms: Iterative Classification Algorithm and Loopy Belief Propagation.$$$We advocate that the accuracy of presented methods depends on the structural properties of the examined network.$$$We claim that measure-neighbour methods will work better than the regular methods for networks with higher clustering coefficient and worse than regular methods for networks with low clustering coefficient.$$$According to our hypothesis, based on clustering coefficient we are able to recommend appropriate active learning and inference method.",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES OBJECTIVES METHODS METHODS METHODS METHODS METHODS METHODS RESULTS RESULTS RESULTS
D06760,"LTE-Unlicensed (LTE-U) has recently attracted worldwide interest to meet the explosion in cellular traffic data.$$$By using carrier aggregation (CA), licensed and unlicensed bands are integrated to enhance transmission capacity while maintaining reliable and predictable performance.$$$As there may exist other conventional unlicensed band users, such as Wi-Fi users, LTE-U users have to share the same unlicensed bands with them.$$$Thus, an optimized resource allocation scheme to ensure the fairness between LTE-U users and conventional unlicensed band users is critical for the deployment of LTE-U networks.$$$In this paper, we investigate an energy efficient resource allocation problem in LTE-U coexisting with other wireless networks, which aims at guaranteeing fairness among the users of different radio access networks (RANs).$$$We formulate the problem as a multi-objective optimization problem and propose a semi-distributed matching framework with a partial information-based algorithm to solve it.$$$We demonstrate our contributions with simulations in which various network densities and traffic load levels are considered.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS
D01043,"This paper addresses the problem of localizing an unknown number of targets, all having the same radar signature, by a distributed MIMO radar consisting of single antenna transmitters and receivers that cannot determine directions of departure and arrival.$$$Furthermore, we consider the presence of multipath propagation, and the possible (correlated) blocking of the direct paths (going from the transmitter and reflecting off a target to the receiver).$$$In its most general form, this problem can be cast as a Bayesian estimation problem where every multipath component is accounted for.$$$However, when the environment map is unknown, this problem is ill-posed and hence, a tractable approximation is derived where only direct paths are accounted for.$$$In particular, we take into account the correlated blocking by scatterers in the environment which appears as a prior term in the Bayesian estimation framework.$$$A sub-optimal polynomial-time algorithm to solve the Bayesian multi-target localization problem with correlated blocking is proposed and its performance is evaluated using simulations.$$$We found that when correlated blocking is severe, assuming the blocking events to be independent and having constant probability (as was done in previous papers) resulted in poor detection performance, with false alarms more likely to occur than detections.",BACKGROUND/OBJECTIVES OBJECTIVES METHODS METHODS METHODS RESULTS RESULTS/CONCLUSIONS
D03009,"We consider the problem of minimizing a linear function over an affine section of the cone of positive semidefinite matrices, with the additional constraint that the feasible matrix has prescribed rank.$$$When the rank constraint is active, this is a non-convex optimization problem, otherwise it is a semidefinite program.$$$Both find numerous applications especially in systems control theory and combinatorial optimization, but even in more general contexts such as polynomial optimization or real algebra.$$$While numerical algorithms exist for solving this problem, such as interior-point or Newton-like algorithms, in this paper we propose an approach based on symbolic computation.$$$We design an exact algorithm for solving rank-constrained semidefinite programs, whose complexity is essentially quadratic on natural degree bounds associated to the given optimization problem: for subfamilies of the problem where the size of the feasible matrix is fixed, the complexity is polynomial in the number of variables.$$$The algorithm works under assumptions on the input data: we prove that these assumptions are generically satisfied.$$$We also implement it in Maple and discuss practical experiments.",BACKGROUND BACKGROUND BACKGROUND METHODS OBJECTIVES/CONCLUSIONS OBJECTIVES OBJECTIVES
D06233,"Two-step predictor/corrector methods are provided to solve three classes of problems that present themselves as systems of ordinary differential equations (ODEs).$$$In the first class, velocities are given from which displacements are to be solved.$$$In the second class, velocities and accelerations are given from which displacements are to be solved.$$$And in the third class, accelerations are given from which velocities and displacements are to be solved.$$$Two-step methods are not self starting, so compatible one-step methods are provided to take that first step with.$$$An algorithm is presented for controlling the step size so that the local truncation error does not exceed a specified tolerance.",RESULTS OTHERS OTHERS OTHERS OTHERS OTHERS
D02848,"This paper proposes a novel method for understanding daily hand-object manipulation by developing computer vision-based techniques.$$$Specifically, we focus on recognizing hand grasp types, object attributes and manipulation actions within an unified framework by exploring their contextual relationships.$$$Our hypothesis is that it is necessary to jointly model hands, objects and actions in order to accurately recognize multiple tasks that are correlated to each other in hand-object manipulation.$$$In the proposed model, we explore various semantic relationships between actions, grasp types and object attributes, and show how the context can be used to boost the recognition of each component.$$$We also explore the spatial relationship between the hand and object in order to detect the manipulated object from hand in cluttered environment.$$$Experiment results on all three recognition tasks show that our proposed method outperforms traditional appearance-based methods which are not designed to take into account contextual relationships involved in hand-object manipulation.$$$The visualization and generalizability study of the learned context further supports our hypothesis.",BACKGROUND OBJECTIVES CONCLUSIONS METHODS METHODS RESULTS RESULTS
D03400,"An algorithmic framework to compute sparse or minimal-TV solutions of linear systems is proposed.$$$The framework includes both the Kaczmarz method and the linearized Bregman method as special cases and also several new methods such as a sparse Kaczmarz solver.$$$The algorithmic framework has a variety of applications and is especially useful for problems in which the linear measurements are slow and expensive to obtain.$$$We present examples for online compressed sensing, TV tomographic reconstruction and radio interferometry.",RESULTS RESULTS RESULTS RESULTS
D03568,"Mobile ad-hoc network (MANET) is a dynamic collection of mobile computers without the need for any existing infrastructure.$$$Nodes in a MANET act as hosts and routers.$$$Designing of robust routing algorithms for MANETs is a challenging task.$$$Disjoint multipath routing protocols address this problem and increase the reliability, security and lifetime of network.$$$However, selecting an optimal multipath is an NP-complete problem.$$$In this paper, Hopfield neural network (HNN) which its parameters are optimized by particle swarm optimization (PSO) algorithm is proposed as multipath routing algorithm.$$$Link expiration time (LET) between each two nodes is used as the link reliability estimation metric.$$$This approach can find either node-disjoint or link-disjoint paths in single phase route discovery.$$$Simulation results confirm that PSO-HNN routing algorithm has better performance as compared to backup path set selection algorithm (BPSA) in terms of the path set reliability and number of paths in the set.",BACKGROUND BACKGROUND OBJECTIVES BACKGROUND BACKGROUND METHODS METHODS RESULTS CONCLUSIONS
D04472,"The vision and requirements of the sixth generation (6G) mobile communication systems are expected to adopt free-space optical communication (FSO) and wireless power transfer (WPT).$$$The laser-based WPT or wireless information transfer (WIT) usually faces the challenges of mobility and safety.$$$We present here a mobile and safe resonant beam communication (RBCom) system, which can realize high-rate simultaneous wireless information and power transfer (SWIPT).$$$We propose the analytical model to depict its SWIPT procedure.$$$The numerical results show that RBCom can achieve 7.5 Gbit/s with 200 mW received optical power, which seems to connect the transmitter and the receiver with a mobile ""wireless optical fiber"".",BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS/CONCLUSIONS
D00928,"We unify two prominent lines of work on multi-armed bandits: bandits with knapsacks (BwK) and combinatorial semi-bandits.$$$The former concerns limited ""resources"" consumed by the algorithm, e.g., limited supply in dynamic pricing.$$$The latter allows a huge number of actions but assumes combinatorial structure and additional feedback to make the problem tractable.$$$We define a common generalization, support it with several motivating examples, and design an algorithm for it.$$$Our regret bounds are comparable with those for BwK and combinatorial semi- bandits.",OBJECTIVES BACKGROUND BACKGROUND OBJECTIVES RESULTS
D02743,"It has been well demonstrated that adversarial examples, i.e., natural images with visually imperceptible perturbations added, generally exist for deep networks to fail on image classification.$$$In this paper, we extend adversarial examples to semantic segmentation and object detection which are much more difficult.$$$Our observation is that both segmentation and detection are based on classifying multiple targets on an image (e.g., the basic target is a pixel or a receptive field in segmentation, and an object proposal in detection), which inspires us to optimize a loss function over a set of pixels/proposals for generating adversarial perturbations.$$$Based on this idea, we propose a novel algorithm named Dense Adversary Generation (DAG), which generates a large family of adversarial examples, and applies to a wide range of state-of-the-art deep networks for segmentation and detection.$$$We also find that the adversarial perturbations can be transferred across networks with different training data, based on different architectures, and even for different recognition tasks.$$$In particular, the transferability across networks with the same architecture is more significant than in other cases.$$$Besides, summing up heterogeneous perturbations often leads to better transfer performance, which provides an effective method of black-box adversarial attack.",BACKGROUND OBJECTIVES OBJECTIVES METHODS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS
D03290,"Efficient exploration is an unsolved problem in Reinforcement Learning which is usually addressed by reactively rewarding the agent for fortuitously encountering novel situations.$$$This paper introduces an efficient active exploration algorithm, Model-Based Active eXploration (MAX), which uses an ensemble of forward models to plan to observe novel events.$$$This is carried out by optimizing agent behaviour with respect to a measure of novelty derived from the Bayesian perspective of exploration, which is estimated using the disagreement between the futures predicted by the ensemble members.$$$We show empirically that in semi-random discrete environments where directed exploration is critical to make progress, MAX is at least an order of magnitude more efficient than strong baselines.$$$MAX scales to high-dimensional continuous environments where it builds task-agnostic models that can be used for any downstream task.",BACKGROUND OBJECTIVES/METHODS METHODS RESULTS RESULTS/CONCLUSIONS
D00548,"In the context of personalized medicine, text mining methods pose an interesting option for identifying disease-gene associations, as they can be used to generate novel links between diseases and genes which may complement knowledge from structured databases.$$$The most straightforward approach to extract such links from text is to rely on a simple assumption postulating an association between all genes and diseases that co-occur within the same document.$$$However, this approach (i) tends to yield a number of spurious associations, (ii) does not capture different relevant types of associations, and (iii) is incapable of aggregating knowledge that is spread across documents.$$$Thus, we propose an approach in which disease-gene co-occurrences and gene-gene interactions are represented in an RDF graph.$$$A machine learning-based classifier is trained that incorporates features extracted from the graph to separate disease-gene pairs into valid disease-gene associations and spurious ones.$$$On the manually curated Genetic Testing Registry, our approach yields a 30 points increase in F1 score over a plain co-occurrence baseline.",OBJECTIVES BACKGROUND BACKGROUND/METHODS METHODS METHODS RESULTS
D05704,"Optical backbone networks carry a huge amount of bandwidth and serve as a key enabling technology to provide telecommunication connectivity across the world.$$$Hence, in events of network component (node/link) failures, communication networks may suffer from huge amount of bandwidth loss and service disruptions.$$$Natural disasters such as earthquakes, hurricanes, tornadoes, etc., occur at different places around the world, causing severe communication service disruptions due to network component failures.$$$Most of the previous works on optical network survivability assume that the failures are going to occur in future, and the network is made survivable to ensure connectivity in events of failures.$$$With the advancements in seismology, the predictions of earthquakes are becoming more accurate.$$$Earthquakes have been a major cause of telecommunication service disruption in the past.$$$Hence, the information provided by the meteorological departments and other similar agencies of different countries may be helpful in designing networks that are more robust against earthquakes.$$$In this work, we consider the actual information provided by the Indian meteorological department (IMD) on seismic zones, and earthquakes occurred in the past in India, and propose a scheme to improve the survivability of the existing Indian optical network through minute changes in network topology.$$$Simulations show significant improvement in the network survivability can be achieved using the proposed scheme in events of earthquakes.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/METHODS RESULTS/CONCLUSIONS
D04501,"Deep convolutional neural networks have led to breakthrough results in practical feature extraction applications.$$$The mathematical analysis of these networks was pioneered by Mallat, 2012.$$$Specifically, Mallat considered so-called scattering networks based on identical semi-discrete wavelet frames in each network layer, and proved translation-invariance as well as deformation stability of the resulting feature extractor.$$$The purpose of this paper is to develop Mallat's theory further by allowing for different and, most importantly, general semi-discrete frames (such as, e.g., Gabor frames, wavelets, curvelets, shearlets, ridgelets) in distinct network layers.$$$This allows to extract wider classes of features than point singularities resolved by the wavelet transform.$$$Our generalized feature extractor is proven to be translation-invariant, and we develop deformation stability results for a larger class of deformations than those considered by Mallat.$$$For Mallat's wavelet-based feature extractor, we get rid of a number of technical conditions.$$$The mathematical engine behind our results is continuous frame theory, which allows us to completely detach the invariance and deformation stability proofs from the particular algebraic structure of the underlying frames.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/METHODS RESULTS RESULTS RESULTS METHODS/RESULTS
D03482,"In principle, a network can transfer data at nearly the speed of light.$$$Today's Internet, however, is much slower: our measurements show that latencies are typically more than one, and often more than two orders of magnitude larger than the lower bound implied by the speed of light.$$$Closing this gap would not only add value to today's Internet applications, but might also open the door to exciting new applications.$$$Thus, we propose a grand challenge for the networking research community: building a speed-of-light Internet.$$$Towards addressing this goal, we begin by investigating the causes of latency inflation in the Internet across the network stack.$$$Our analysis reveals that while protocol overheads, which have dominated the community's attention, are indeed important, infrastructural inefficiencies are a significant and under-explored problem.$$$Thus, we propose a radical, yet surprisingly low-cost approach to mitigating latency inflation at the lowest layers and building a nearly speed-of-light Internet infrastructure.",BACKGROUND/OBJECTIVES RESULTS OBJECTIVES OBJECTIVES METHODS CONCLUSIONS METHODS
D03185,"Majority of Artificial Neural Network (ANN) implementations in autonomous systems use a fixed/user-prescribed network topology, leading to sub-optimal performance and low portability.$$$The existing neuro-evolution of augmenting topology or NEAT paradigm offers a powerful alternative by allowing the network topology and the connection weights to be simultaneously optimized through an evolutionary process.$$$However, most NEAT implementations allow the consideration of only a single objective.$$$There also persists the question of how to tractably introduce topological diversification that mitigates overfitting to training scenarios.$$$To address these gaps, this paper develops a multi-objective neuro-evolution algorithm.$$$While adopting the basic elements of NEAT, important modifications are made to the selection, speciation, and mutation processes.$$$With the backdrop of small-robot path-planning applications, an experience-gain criterion is derived to encapsulate the amount of diverse local environment encountered by the system.$$$This criterion facilitates the evolution of genes that support exploration, thereby seeking to generalize from a smaller set of mission scenarios than possible with performance maximization alone.$$$The effectiveness of the single-objective (optimizing performance) and the multi-objective (optimizing performance and experience-gain) neuro-evolution approaches are evaluated on two different small-robot cases, with ANNs obtained by the multi-objective optimization observed to provide superior performance in unseen scenarios.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES/METHODS METHODS METHODS METHODS RESULTS/CONCLUSIONS
D01766,"Synchronizing sequences have been proposed in the late 60's to solve testing problems on systems modeled by finite state machines.$$$Such sequences lead a system, seen as a black box, from an unknown current state to a known final one.$$$This paper presents a first investigation of the computation of synchronizing sequences for systems modeled by bounded synchronized Petri nets.$$$In the first part of the paper, existing techniques for automata are adapted to this new setting.$$$Later on, new approaches, that exploit the net structure to efficiently compute synchronizing sequences without an exhaustive enumeration of the state space, are presented.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS/RESULTS
D01346,"In a voice-controlled smart-home, a controller must respond not only to user's requests but also according to the interaction context.$$$This paper describes Arcades, a system which uses deep reinforcement learning to extract context from a graphical representation of home automation system and to update continuously its behavior to the user's one.$$$This system is robust to changes in the environment (sensor breakdown or addition) through its graphical representation (scale well) and the reinforcement mechanism (adapt well).$$$The experiments on realistic data demonstrate that this method promises to reach long life context-aware control of smart-home.",BACKGROUND OBJECTIVES/METHODS METHODS/RESULTS CONCLUSIONS
D01321,"The digitalization of the legal domain has been ongoing for a couple of years.$$$In that process, the application of different machine learning (ML) techniques is crucial.$$$Tasks such as the classification of legal documents or contract clauses as well as the translation of those are highly relevant.$$$On the other side, digitized documents are barely accessible in this field, particularly in Germany.$$$Today, deep learning (DL) is one of the hot topics with many publications and various applications.$$$Sometimes it provides results outperforming the human level.$$$Hence this technique may be feasible for the legal domain as well.$$$However, DL requires thousands of samples to provide decent results.$$$A potential solution to this problem is multi-task DL to enable transfer learning.$$$This approach may be able to overcome the data scarcity problem in the legal domain, specifically for the German language.$$$We applied the state of the art multi-task model on three tasks: translation, summarization, and multi-label classification.$$$The experiments were conducted on legal document corpora utilizing several task combinations as well as various model parameters.$$$The goal was to find the optimal configuration for the tasks at hand within the legal domain.$$$The multi-task DL approach outperformed the state of the art results in all three tasks.$$$This opens a new direction to integrate DL technology more efficiently in the legal domain.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/METHODS OBJECTIVES/METHODS METHODS METHODS OBJECTIVES/METHODS RESULTS CONCLUSIONS
D05418,"Neural Machine Translation (NMT) can be improved by including document-level contextual information.$$$For this purpose, we propose a hierarchical attention model to capture the context in a structured and dynamic manner.$$$The model is integrated in the original NMT architecture as another level of abstraction, conditioning on the NMT model's own previous hidden states.$$$Experiments show that hierarchical attention significantly improves the BLEU score over a strong NMT baseline with the state-of-the-art in context-aware methods, and that both the encoder and decoder benefit from context in complementary ways.",BACKGROUND OBJECTIVES/METHODS METHODS RESULTS/CONCLUSIONS
D05873,"During imagery motor movements tasks, the so called mu and beta event related desynchronization (ERD) and synchronization (ERS) are taking place, allowing us to determine human patient imagery movement.$$$However, initial recordings of electroencephalography (EEG) signals contain system and environmental noise as well as interference that must be ejected in order to separate the ERS/ERD events from the rest of the signal.$$$This paper presents a new technique based on a reworked Second Order Blind Identification (SOBI) algorithm for noise removal while imagery movement classification is implemented using Support Vector Machine (SVM) technique.",BACKGROUND BACKGROUND BACKGROUND
D04002,"The new era of computing called Cloud Computing allows the user to access the cloud services dynamically over the Internet wherever and whenever needed.$$$Cloud consists of data and resources; and the cloud services include the delivery of software, infrastructure, applications, and storage over the Internet based on user demand through Internet.$$$In short, cloud computing is a business and economic model allowing the users to utilize high-end computing and storage virtually with minimal infrastructure on their end.$$$Cloud has three service models namely, Cloud Software-as-a-Service (SaaS), Cloud Platform-as-a-Service (PaaS), and Cloud Infrastructure-as-a-Service (IaaS).$$$This paper talks in depth of cloud infrastructure service management.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/CONCLUSIONS
D02168,"Motivated by value function estimation in reinforcement learning, we study statistical linear inverse problems, i.e., problems where the coefficients of a linear system to be solved are observed in noise.$$$We consider penalized estimators, where performance is evaluated using a matrix-weighted two-norm of the defect of the estimator measured with respect to the true, unknown coefficients.$$$Two objective functions are considered depending whether the error of the defect measured with respect to the noisy coefficients is squared or unsquared.$$$We propose simple, yet novel and theoretically well-founded data-dependent choices for the regularization parameters for both cases that avoid data-splitting.$$$A distinguishing feature of our analysis is that we derive deterministic error bounds in terms of the error of the coefficients, thus allowing the complete separation of the analysis of the stochastic properties of these errors.$$$We show that our results lead to new insights and bounds for linear value function estimation in reinforcement learning.",OBJECTIVES METHODS RESULTS RESULTS RESULTS RESULTS
D05364,"We propose fast probabilistic algorithms with low (i.e., sublinear in the input size) communication volume to check the correctness of operations in Big Data processing frameworks and distributed databases.$$$Our checkers cover many of the commonly used operations, including sum, average, median, and minimum aggregation, as well as sorting, union, merge, and zip.$$$An experimental evaluation of our implementation in Thrill (Bingmann et al., 2016) confirms the low overhead and high failure detection rate predicted by theoretical analysis.",RESULTS RESULTS RESULTS
D01658,"In this paper, we give algorithms and methods of construction of self-dual codes over finite fields using orthogonal matrices.$$$Randomization in the orthogonal group, and code extension are the main tools.$$$Some optimal, almost MDS, and MDS self-dual codes over both small and large prime fields are constructed.",OBJECTIVES METHODS RESULTS/CONCLUSIONS
D02894,"In this paper, we present FPT-algorithms for special cases of the shortest lattice vector, integer linear programming, and simplex width computation problems, when matrices included in the problems' formulations are near square.$$$The parameter is the maximum absolute value of rank minors of the corresponding matrices.$$$Additionally, we present FPT-algorithms with respect to the same parameter for the problems, when the matrices have no singular rank sub-matrices.",BACKGROUND/OBJECTIVES/RESULTS OBJECTIVES RESULTS
D01609,"Iterative Closest Point (ICP) is a widely used method for performing scan-matching and registration.$$$Being simple and robust method, it is still computationally expensive and may be challenging to use in real-time applications with limited resources on mobile platforms.$$$In this paper we propose novel effective method for acceleration of ICP which does not require substantial modifications to the existing code.$$$This method is based on an idea of Anderson acceleration which is an iterative procedure for finding a fixed point of contractive mapping.$$$The latter is often faster than a standard Picard iteration, usually used in ICP implementations.$$$We show that ICP, being a fixed point problem, can be significantly accelerated by this method enhanced by heuristics to improve overall robustness.$$$We implement proposed approach into Point Cloud Library (PCL) and make it available online.$$$Benchmarking on real-world data fully supports our claims.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS
D03239,"Cloud Computing is an emerging area for accessing computing resources.$$$In general, Cloud service providers offer services that can be clustered into three categories: SaaS, PaaS and IaaS.$$$This paper discusses the Cloud workload analysis.$$$The efficient Cloud workload resource mapping technique is proposed.$$$This paper aims to provide a means of understanding and investigating IaaS Cloud workloads and the resources.$$$In this paper, regression analysis is used to analyze the Cloud workloads and identifies the relationship between Cloud workloads and available resources.$$$The effective organization of dynamic nature resources can be done with the help of Cloud workloads.$$$Till Cloud workload is considered a vital talent, the Cloud resources cannot be consumed in an effective style.$$$The proposed technique has been validated by Z Formal specification language.$$$This approach is effective in minimizing the cost and submission burst time of Cloud workloads.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS/RESULTS METHODS/RESULTS CONCLUSIONS CONCLUSIONS RESULTS RESULTS/CONCLUSIONS
D03439,"We present monaa, a monitoring tool over a real-time property specified by either a timed automaton or a timed regular expression.$$$It implements a timed pattern matching algorithm that combines 1) features suited for online monitoring, and 2) acceleration by automata-based skipping.$$$Our experiments demonstrate monaa's performance advantage, especially in online usage.",OTHERS METHODS RESULTS
D00519,"Future machine to machine (M2M) communications need to support a massive number of devices communicating with each other with little or no human intervention.$$$Random access techniques were originally proposed to enable M2M multiple access, but suffer from severe congestion and access delay in an M2M system with a large number of devices.$$$In this paper, we propose a novel multiple access scheme for M2M communications based on the capacity-approaching analog fountain code to efficiently minimize the access delay and satisfy the delay requirement for each device.$$$This is achieved by allowing M2M devices to transmit at the same time on the same channel in an optimal probabilistic manner based on their individual delay requirements.$$$Simulation results show that the proposed scheme achieves a near optimal rate performance and at the same time guarantees the delay requirements of the devices.$$$We further propose a simple random access strategy and characterized the required overhead.$$$Simulation results show the proposed approach significantly outperforms the existing random access schemes currently used in long term evolution advanced (LTE-A) standard in terms of the access delay.",BACKGROUND BACKGROUND OBJECTIVES/METHODS METHODS RESULTS METHODS RESULTS
D00612,"In imperfect-information games, the optimal strategy in a subgame may depend on the strategy in other, unreached subgames.$$$Thus a subgame cannot be solved in isolation and must instead consider the strategy for the entire game as a whole, unlike perfect-information games.$$$Nevertheless, it is possible to first approximate a solution for the whole game and then improve it by solving individual subgames.$$$This is referred to as subgame solving.$$$We introduce subgame-solving techniques that outperform prior methods both in theory and practice.$$$We also show how to adapt them, and past subgame-solving techniques, to respond to opponent actions that are outside the original action abstraction; this significantly outperforms the prior state-of-the-art approach, action translation.$$$Finally, we show that subgame solving can be repeated as the game progresses down the game tree, leading to far lower exploitability.$$$These techniques were a key component of Libratus, the first AI to defeat top humans in heads-up no-limit Texas hold'em poker.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/RESULTS METHODS/RESULTS METHODS/RESULTS RESULTS/CONCLUSIONS
D02543,"In order to better manage the premiums and encourage safe driving, many commercial insurance companies (e.g., Geico, Progressive) are providing options for their customers to install sensors on their vehicles which collect individual vehicle's traveling data.$$$The driver's insurance is linked to his/her driving behavior.$$$At the other end, through analyzing the historical traveling data from a large number of vehicles, the insurance company could build a classifier to predict a new driver's driving style: aggressive or defensive.$$$However, collection of such vehicle traveling data explicitly breaches the drivers' personal privacy.$$$To tackle such privacy concerns, this paper presents a privacy-preserving driving style recognition technique to securely predict aggressive and defensive drivers for the insurance company without compromising the privacy of all the participating parties.$$$The insurance company cannot learn any private information from the vehicles, and vice-versa.$$$Finally, the effectiveness and efficiency of the privacy-preserving driving style recognition technique are validated with experimental results.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/METHODS OBJECTIVES/RESULTS/CONCLUSIONS RESULTS
D03236,"Control systems behavior can be analyzed taking into account a large number of parameters: performances, reliability, availability, security.$$$Each control system presents various security vulnerabilities that affect in lower or higher measure its functioning.$$$In this paper the authors present a method to assess the impact of security issues on the systems availability.$$$A fuzzy model for estimating the availability of the system based on the security level and achieved availability coefficient (depending on MTBF and MTR) is developed and described.$$$The results of the fuzzy inference system (FIS) are presented in the last section of the paper.",BACKGROUND BACKGROUND OBJECTIVES METHODS/RESULTS/CONCLUSIONS RESULTS
D03072,"Computer vision has made remarkable progress in recent years.$$$Deep neural network (DNN) models optimized to identify objects in images exhibit unprecedented task-trained accuracy and, remarkably, some generalization ability: new visual problems can now be solved more easily based on previous learning.$$$Biological vision (learned in life and through evolution) is also accurate and general-purpose.$$$Is it possible that these different learning regimes converge to similar problem-dependent optimal computations?$$$We therefore asked whether the human system-level computation of visual perception has DNN correlates and considered several anecdotal test cases.$$$We found that perceptual sensitivity to image changes has DNN mid-computation correlates, while sensitivity to segmentation, crowding and shape has DNN end-computation correlates.$$$Our results quantify the applicability of using DNN computation to estimate perceptual loss, and are consistent with the fascinating theoretical view that properties of human perception are a consequence of architecture-independent visual learning.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS CONCLUSIONS
D03435,"Recently, many approaches have been introduced by several researchers to identify plants.$$$Now, applications of texture, shape, color and vein features are common practices.$$$However, there are many possibilities of methods can be developed to improve the performance of such identification systems.$$$Therefore, several experiments had been conducted in this research.$$$As a result, a new novel approach by using combination of Gray-Level Co-occurrence Matrix, lacunarity and Shen features and a Bayesian classifier gives a better result compared to other plant identification systems.$$$For comparison, this research used two kinds of several datasets that were usually used for testing the performance of each plant identification system.$$$The results show that the system gives an accuracy rate of 97.19% when using the Flavia dataset and 95.00% when using the Foliage dataset and outperforms other approaches.",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES METHODS/RESULTS METHODS RESULTS/CONCLUSIONS
D00492,"In this paper we shall introduce a simple, effective numerical method for finding differential operators for scalar and vector-valued functions on surfaces.$$$The key idea of our algorithm is to develop an intrinsic and unified way to compute directly the partial derivatives of functions defined on triangular meshes which are the discretization of regular surfaces under consideration.$$$Most importantly, the divergence theorem and conservation laws on triangular meshes are fulfilled.",OBJECTIVES METHODS RESULTS/CONCLUSIONS
D06035,"Deep networks are successfully used as classification models yielding state-of-the-art results when trained on a large number of labeled samples.$$$These models, however, are usually much less suited for semi-supervised problems because of their tendency to overfit easily when trained on small amounts of data.$$$In this work we will explore a new training objective that is targeting a semi-supervised regime with only a small subset of labeled data.$$$This criterion is based on a deep metric embedding over distance relations within the set of labeled samples, together with constraints over the embeddings of the unlabeled set.$$$The final learned representations are discriminative in euclidean space, and hence can be used with subsequent nearest-neighbor classification using the labeled samples.",BACKGROUND BACKGROUND METHODS METHODS CONCLUSIONS
D00734,"This paper presents the ""Leipzig Corpus Miner"", a technical infrastructure for supporting qualitative and quantitative content analysis.$$$The infrastructure aims at the integration of 'close reading' procedures on individual documents with procedures of 'distant reading', e.g. lexical characteristics of large document collections.$$$Therefore information retrieval systems, lexicometric statistics and machine learning procedures are combined in a coherent framework which enables qualitative data analysts to make use of state-of-the-art Natural Language Processing techniques on very large document collections.$$$Applicability of the framework ranges from social sciences to media studies and market research.$$$As an example we introduce the usage of the framework in a political science study on post-democracy and neoliberalism.",BACKGROUND OBJECTIVES OBJECTIVES/METHODS BACKGROUND/OBJECTIVES RESULTS
D00324,"One major challenge in training Deep Neural Networks is preventing overfitting.$$$Many techniques such as data augmentation and novel regularizers such as Dropout have been proposed to prevent overfitting without requiring a massive amount of training data.$$$In this work, we propose a new regularizer called DeCov which leads to significantly reduced overfitting (as indicated by the difference between train and val performance), and better generalization.$$$Our regularizer encourages diverse or non-redundant representations in Deep Neural Networks by minimizing the cross-covariance of hidden activations.$$$This simple intuition has been explored in a number of past works but surprisingly has never been applied as a regularizer in supervised learning.$$$Experiments across a range of datasets and network architectures show that this loss always reduces overfitting while almost always maintaining or increasing generalization performance and often improving performance over Dropout.",BACKGROUND/OBJECTIVES BACKGROUND/OBJECTIVES METHODS/RESULTS METHODS/RESULTS BACKGROUND RESULTS/CONCLUSIONS
D06568,"We present Caffe con Troll (CcT), a fully compatible end-to-end version of the popular framework Caffe with rebuilt internals.$$$We built CcT to examine the performance characteristics of training and deploying general-purpose convolutional neural networks across different hardware architectures.$$$We find that, by employing standard batching optimizations for CPU training, we achieve a 4.5x throughput improvement over Caffe on popular networks like CaffeNet.$$$Moreover, with these improvements, the end-to-end training time for CNNs is directly proportional to the FLOPS delivered by the CPU, which enables us to efficiently train hybrid CPU-GPU systems for CNNs.",CONCLUSIONS OBJECTIVES METHODS/RESULTS RESULTS/CONCLUSIONS
D05485,"A heuristic procedure based on novel recursive formulation of sinusoid (RFS) and on regression with predictive least-squares (LS) enables to decompose both uniformly and nonuniformly sampled 1-d signals into a sparse set of sinusoids (SSS).$$$An optimal SSS is found by Levenberg-Marquardt (LM) optimization of RFS parameters of near-optimal sinusoids combined with common criteria for the estimation of the number of sinusoids embedded in noise.$$$The procedure estimates both the cardinality and the parameters of SSS.$$$The proposed algorithm enables to identify the RFS parameters of a sinusoid from a data sequence containing only a fraction of its cycle.$$$In extreme cases when the frequency of a sinusoid approaches zero the algorithm is able to detect a linear trend in data.$$$Also, an irregular sampling pattern enables the algorithm to correctly reconstruct the under-sampled sinusoid.$$$Parsimonious nature of the obtaining models opens the possibilities of using the proposed method in machine learning and in expert and intelligent systems needing analysis and simple representation of 1-d signals.$$$The properties of the proposed algorithm are evaluated on examples of irregularly sampled artificial signals in noise and are compared with high accuracy frequency estimation algorithms based on linear prediction (LP) approach, particularly with respect to Cramer-Rao Bound (CRB).",BACKGROUND/OBJECTIVES/METHODS BACKGROUND/METHODS RESULTS RESULTS RESULTS RESULTS OBJECTIVES/CONCLUSIONS BACKGROUND/METHODS
D04569,"Unmanned Aerial Vehicles (UAVs) have recently rapidly grown to facilitate a wide range of innovative applications that can fundamentally change the way cyber-physical systems (CPSs) are designed.$$$CPSs are a modern generation of systems with synergic cooperation between computational and physical potentials that can interact with humans through several new mechanisms.$$$The main advantages of using UAVs in CPS application is their exceptional features, including their mobility, dynamism, effortless deployment, adaptive altitude, agility, adjustability, and effective appraisal of real-world functions anytime and anywhere.$$$Furthermore, from the technology perspective, UAVs are predicted to be a vital element of the development of advanced CPSs.$$$Therefore, in this survey, we aim to pinpoint the most fundamental and important design challenges of multi-UAV systems for CPS applications.$$$We highlight key and versatile aspects that span the coverage and tracking of targets and infrastructure objects, energy-efficient navigation, and image analysis using machine learning for fine-grained CPS applications.$$$Key prototypes and testbeds are also investigated to show how these practical technologies can facilitate CPS applications.$$$We present and propose state-of-the-art algorithms to address design challenges with both quantitative and qualitative methods and map these challenges with important CPS applications to draw insightful conclusions on the challenges of each application.$$$Finally, we summarize potential new directions and ideas that could shape future research in these areas.",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES OBJECTIVES OBJECTIVES OBJECTIVES/METHODS/RESULTS METHODS/RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS
D01040,"Many deep models have been recently proposed for anomaly detection.$$$This paper presents comparison of selected generative deep models and classical anomaly detection methods on an extensive number of non--image benchmark datasets.$$$We provide statistical comparison of the selected models, in many configurations, architectures and hyperparamaters.$$$We arrive to conclusion that performance of the generative models is determined by the process of selection of their hyperparameters.$$$Specifically, performance of the deep generative models deteriorates with decreasing amount of anomalous samples used in hyperparameter selection.$$$In practical scenarios of anomaly detection, none of the deep generative models systematically outperforms the kNN.",BACKGROUND METHODS METHODS CONCLUSIONS CONCLUSIONS RESULTS
D03856,"In ontology-based data access (OBDA), users are provided with a conceptual view of a (relational) data source that abstracts away details about data storage.$$$This conceptual view is realized through an ontology that is connected to the data source through declarative mappings, and query answering is carried out by translating the user queries over the conceptual view into SQL queries over the data source.$$$Standard translation techniques in OBDA try to transform the user query into a union of conjunctive queries (UCQ), following the heuristic argument that UCQs can be efficiently evaluated by modern relational database engines.$$$In this work, we show that translating to UCQs is not always the best choice, and that, under certain conditions on the interplay between the ontology, the map- pings, and the statistics of the data, alternative translations can be evaluated much more efficiently.$$$To find the best translation, we devise a cost model together with a novel cardinality estimation that takes into account all such OBDA components.$$$Our experiments confirm that (i) alternatives to the UCQ translation might produce queries that are orders of magnitude more efficient, and (ii) the cost model we propose is faithful to the actual query evaluation cost, and hence is well suited to select the best translation.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS/RESULTS RESULTS/CONCLUSIONS
D05278,"Two channels are equivalent if their maximum likelihood (ML) decoders coincide for every code.$$$We show that this equivalence relation partitions the space of channels into a generalized hyperplane arrangement.$$$With this, we define a coding distance between channels in terms of their ML-decoders which is meaningful from the decoding point of view, in the sense that the closer two channels are, the larger is the probability of them sharing the same ML-decoder.$$$We give explicit formulas for these probabilities.",BACKGROUND OBJECTIVES/RESULTS OBJECTIVES/RESULTS OBJECTIVES/RESULTS
D06992,"In the present work, we use information theory to understand the empirical convergence rate of tractography, a widely-used approach to reconstruct anatomical fiber pathways in the living brain.$$$Based on diffusion MRI data, tractography is the starting point for many methods to study brain connectivity.$$$Of the available methods to perform tractography, most reconstruct a finite set of streamlines, or 3D curves, representing probable connections between anatomical regions, yet relatively little is known about how the sampling of this set of streamlines affects downstream results, and how exhaustive the sampling should be.$$$Here we provide a method to measure the information theoretic surprise (self-cross entropy) for tract sampling schema.$$$We then empirically assess four streamline methods.$$$We demonstrate that the relative information gain is very low after a moderate number of streamlines have been generated for each tested method.$$$The results give rise to several guidelines for optimal sampling in brain connectivity analyses.",OBJECTIVES BACKGROUND BACKGROUND METHODS METHODS RESULTS CONCLUSIONS
D00325,"Policy iteration (PI) is a recursive process of policy evaluation and improvement to solve an optimal decision-making, e.g., reinforcement learning (RL) or optimal control problem and has served as the fundamental to develop RL methods.$$$Motivated by integral PI (IPI) schemes in optimal control and RL methods in continuous time and space (CTS), this paper proposes on-policy IPI to solve the general RL problem in CTS, with its environment modeled by an ordinary differential equation (ODE).$$$In such continuous domain, we also propose four off-policy IPI methods---two are the ideal PI forms that use advantage and Q-functions, respectively, and the other two are natural extensions of the existing off-policy IPI schemes to our general RL framework.$$$Compared to the IPI methods in optimal control, the proposed IPI schemes can be applied to more general situations and do not require an initial stabilizing policy to run; they are also strongly relevant to the RL algorithms in CTS such as advantage updating, Q-learning, and value-gradient based (VGB) greedy policy improvement.$$$Our on-policy IPI is basically model-based but can be made partially model-free; each off-policy method is also either partially or completely model-free.$$$The mathematical properties of the IPI methods---admissibility, monotone improvement, and convergence towards the optimal solution---are all rigorously proven, together with the equivalence of on- and off-policy IPI.$$$Finally, the IPI methods are simulated with an inverted-pendulum model to support the theory and verify the performance.",BACKGROUND OBJECTIVES OBJECTIVES/METHODS RESULTS RESULTS RESULTS RESULTS
D02681,"Superpixels provide an efficient low/mid-level representation of image data, which greatly reduces the number of image primitives for subsequent vision tasks.$$$Existing superpixel algorithms are not differentiable, making them difficult to integrate into otherwise end-to-end trainable deep neural networks.$$$We develop a new differentiable model for superpixel sampling that leverages deep networks for learning superpixel segmentation.$$$The resulting ""Superpixel Sampling Network"" (SSN) is end-to-end trainable, which allows learning task-specific superpixels with flexible loss functions and has fast runtime.$$$Extensive experimental analysis indicates that SSNs not only outperform existing superpixel algorithms on traditional segmentation benchmarks, but can also learn superpixels for other tasks.$$$In addition, SSNs can be easily integrated into downstream deep networks resulting in performance improvements.",BACKGROUND BACKGROUND/OBJECTIVES OBJECTIVES/METHODS METHODS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS
D00587,"This study considers the 3D human pose estimation problem in a single RGB image by proposing a conditional random field (CRF) model over 2D poses, in which the 3D pose is obtained as a byproduct of the inference process.$$$The unary term of the proposed CRF model is defined based on a powerful heat-map regression network, which has been proposed for 2D human pose estimation.$$$This study also presents a regression network for lifting the 2D pose to 3D pose and proposes the prior term based on the consistency between the estimated 3D pose and the 2D pose.$$$To obtain the approximate solution of the proposed CRF model, the N-best strategy is adopted.$$$The proposed inference algorithm can be viewed as sequential processes of bottom-up generation of 2D and 3D pose proposals from the input 2D image based on deep networks and top-down verification of such proposals by checking their consistencies.$$$To evaluate the proposed method, we use two large-scale datasets: Human3.6M and HumanEva.$$$Experimental results show that the proposed method achieves the state-of-the-art 3D human pose estimation performance.",BACKGROUND/OBJECTIVES METHODS METHODS METHODS METHODS RESULTS RESULTS/CONCLUSIONS
D05126,"Model checking has been successfully used in many computer science fields, including artificial intelligence, theoretical computer science, and databases.$$$Most of the proposed solutions make use of classical, point-based temporal logics, while little work has been done in the interval temporal logic setting.$$$Recently, a non-elementary model checking algorithm for Halpern and Shoham's modal logic of time intervals HS over finite Kripke structures (under the homogeneity assumption) and an EXPSPACE model checking procedure for two meaningful fragments of it have been proposed.$$$In this paper, we show that more efficient model checking procedures can be developed for some expressive enough fragments of HS.",BACKGROUND OBJECTIVES BACKGROUND METHODS
D01510,"One way to analyze Cyber-Physical Systems is by modeling them as hybrid automata.$$$Since reachability analysis for hybrid nonlinear automata is a very challenging and computationally expensive problem, in practice, engineers try to solve the requirements falsification problem.$$$In one method, the falsification problem is solved by minimizing a robustness metric induced by the requirements.$$$This optimization problem is usually a non-convex non-smooth problem that requires heuristic and analytical guidance to be solved.$$$In this paper, functional gradient descent for hybrid systems is utilized for locally decreasing the robustness metric.$$$The local descent method is combined with Simulated Annealing as a global optimization method to search for unsafe behaviors.",BACKGROUND BACKGROUND/OBJECTIVES OBJECTIVES/METHODS BACKGROUND/OBJECTIVES/METHODS METHODS METHODS
D01993,"In this paper, optimal filter design for generalized frequency-division multiplexing (GFDM) is considered under two design criteria: rate maximization and out-of-band (OOB) emission minimization.$$$First, the problem of GFDM filter optimization for rate maximization is formulated by expressing the transmission rate of GFDM as a function of GFDM filter coefficients.$$$It is shown that Dirichlet filters are rate-optimal in additive white Gaussian noise (AWGN) channels with no carrier frequency offset (CFO) under linear zero-forcing (ZF) or minimum mean-square error (MMSE) receivers, but in general channels perturbed by CFO a properly designed nontrivial GFDM filter can yield better performance than Dirichlet filters by adjusting the subcarrier waveform to cope with the channel-induced CFO.$$$Next, the problem of GFDM filter design for OOB emission minimization is formulated by expressing the power spectral density (PSD) of the GFDM transmit signal as a function of GFDM filter coefficients, and it is shown that the OOB emission can be reduced significantly by designing the GFDM filter properly.$$$Finally, joint design of GFDM filter and window for the two design criteria is considered.",OBJECTIVES METHODS RESULTS RESULTS METHODS
D04756,"To face future reliability challenges, it is necessary to quantify the risk of error in any part of a computing system.$$$To this goal, the Architectural Vulnerability Factor (AVF) has long been used for chips.$$$However, this metric is used for offline characterisation, which is inappropriate for memory.$$$We survey the literature and formalise one of the metrics used, the Memory Vulnerability Factor, and extend it to take into account false errors.$$$These are reported errors which would have no impact on the program if they were ignored.$$$We measure the False Error Aware MVF (FEA) and related metrics precisely in a cycle-accurate simulator, and compare them with the effects of injecting faults in a program's data, in native parallel runs.$$$Our findings show that MVF and FEA are the only two metrics that are safe to use at runtime, as they both consistently give an upper bound on the probability of incorrect program outcome.$$$FEA gives a tighter bound than MVF, and is the metric that correlates best with the incorrect outcome probability of all considered metrics.",OBJECTIVES BACKGROUND OTHERS METHODS OTHERS METHODS RESULTS RESULTS
D05900,"Extraction of local feature descriptors is a vital stage in the solution pipelines for numerous computer vision tasks.$$$Learning-based approaches improve performance in certain tasks, but still cannot replace handcrafted features in general.$$$In this paper, we improve the learning of local feature descriptors by optimizing the performance of descriptor matching, which is a common stage that follows descriptor extraction in local feature based pipelines, and can be formulated as nearest neighbor retrieval.$$$Specifically, we directly optimize a ranking-based retrieval performance metric, Average Precision, using deep neural networks.$$$This general-purpose solution can also be viewed as a listwise learning to rank approach, which is advantageous compared to recent local ranking approaches.$$$On standard benchmarks, descriptors learned with our formulation achieve state-of-the-art results in patch verification, patch retrieval, and image matching.",BACKGROUND BACKGROUND OBJECTIVES METHODS OTHERS RESULTS
D01058,"Washing machine is of great domestic necessity as it frees us from the burden of washing our clothes and saves ample of our time.$$$This paper will cover the aspect of designing and developing of Fuzzy Logic based, Smart Washing Machine.$$$The regular washing machine (timer based) makes use of multi-turned timer based start-stop mechanism which is mechanical as is prone to breakage.$$$In addition to its starting and stopping issues, the mechanical timers are not efficient with respect of maintenance and electricity usage.$$$Recent developments have shown that merger of digital electronics in optimal functionality of this machine is possible and nowadays in practice.$$$A number of international renowned companies have developed the machine with the introduction of smart artificial intelligence.$$$Such a machine makes use of sensors and smartly calculates the amount of run-time (washing time) for the main machine motor.$$$Realtime calculations and processes are also catered in optimizing the run-time of the machine.$$$The obvious result is smart time management, better economy of electricity and efficiency of work.$$$This paper deals with the indigenization of FLC (Fuzzy Logic Controller) based Washing Machine, which is capable of automating the inputs and getting the desired output (wash-time).",BACKGROUND OBJECTIVES BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND RESULTS CONCLUSIONS
D01670,"Nowadays, deep learning has been widely used.$$$In natural language learning, the analysis of complex semantics has been achieved because of its high degree of flexibility.$$$The deceptive opinions detection is an important application area in deep learning model, and related mechanisms have been given attention and researched.$$$On-line opinions are quite short, varied types and content.$$$In order to effectively identify deceptive opinions, we need to comprehensively study the characteristics of deceptive opinions, and explore novel characteristics besides the textual semantics and emotional polarity that have been widely used in text analysis.$$$The detection mechanism based on deep learning has better self-adaptability and can effectively identify all kinds of deceptive opinions.$$$In this paper, we optimize the convolution neural network model by embedding the word order characteristics in its convolution layer and pooling layer, which makes convolution neural network more suitable for various text classification and deceptive opinions detection.$$$The TensorFlow-based experiments demonstrate that the detection mechanism proposed in this paper achieve more accurate deceptive opinion detection results.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND/OBJECTIVES OBJECTIVES OBJECTIVES/METHODS/RESULTS CONCLUSIONS
D03182,"Lane mark detection is an important element in the road scene analysis for Advanced Driver Assistant System (ADAS).$$$Limited by the onboard computing power, it is still a challenge to reduce system complexity and maintain high accuracy at the same time.$$$In this paper, we propose a Lane Marking Detector (LMD) using a deep convolutional neural network to extract robust lane marking features.$$$To improve its performance with a target of lower complexity, the dilated convolution is adopted.$$$A shallower and thinner structure is designed to decrease the computational cost.$$$Moreover, we also design post-processing algorithms to construct 3rd-order polynomial models to fit into the curved lanes.$$$Our system shows promising results on the captured road scenes.",BACKGROUND OBJECTIVES METHODS METHODS METHODS METHODS RESULTS/CONCLUSIONS
D03110,"This paper is to create a practical steganographic implementation for 4-bit images.The proposed technique converts 4 bit image into 4 shaded Gray Scale image.$$$This image will be act as reference image to hide the text.$$$Using this grey scale reference image any text can be hidden.$$$Single character of a text can be represented by 8-bit.$$$The 8-bit character can be split into 4X2 bit information.$$$If the reference image and the data file are transmitted through network separately, we can achieve the effect of Steganography.$$$Here the image is not at all distorted because said image is only used for referencing.$$$Any huge mount of text material can be hidden using a very small image.$$$Decipher the text is not possible intercepting the image or data file separately.$$$So, it is more secure.",OBJECTIVES BACKGROUND METHODS RESULTS OBJECTIVES BACKGROUND METHODS OBJECTIVES OBJECTIVES RESULTS
D06884,"This paper attempts to explain consequences of the relational calculus not allowing relations to be domains of relations, and to suggest a solution for the issue.$$$On the example of SQL we describe the consequent problem of the multitude of different representations for relations; analyze in detail the disadvantages of the notions ""TABLE"" and ""FOREIGN KEY""; and propose a complex solution which includes brand new data language, abandonment of tables as a representation for relations, and relatively small yet very significant alteration of the data storage concept, called ""multitable index"".",OBJECTIVES/METHODS METHODS/RESULTS
D05849,"While service-dominant logic proposes that all ""Goods are a distribution mechanism for service provision"" (FP3), there is a need to understand when and why a firm would utilise direct or indirect (goods) service provision, and the interactions between them, to co-create value with the customer.$$$Three longitudinal case studies in B2B equipment-based 'complex service' systems were analysed to gain an understanding of customers' co-creation activities to achieve outcomes.$$$We found the nature of value, degree of contextual variety and the firm's legacy viability to be viability threats.$$$To counter this, the firm uses (a) Direct Service Provision for Scalability and Replicability, (b) Indirect Service Provision for variety absorption and co-creating emotional value and customer experience and (c) designing direct and indirect provision for Scalability and Absorptive Resources of the customer.$$$The co-creation of complex multidimensional value could be delivered through different value propositions of the firm.$$$The research proposes a value-centric way of understanding the interactions between direct and indirect service provision in the design of the firm's value proposition and proposes a viable systems approach towards reorganising the firm.$$$The study provides a way for managers to understand the effectiveness (rather than efficiency) of the firm in co-creating value as a major issue in the design of complex socio-technical systems.$$$Goods are often designed within the domain of engineering and product design, often placing human activity as a supporting role to the equipment.$$$Through an SDLogic lens, this study considers the design of both equipment and human activity on an equal footing for value co-creation with the customer, and it yielded interesting results on when direct provisioning (goods) should be redesigned, considering all activities equally.",BACKGROUND OBJECTIVES METHODS METHODS METHODS METHODS METHODS RESULTS CONCLUSIONS
D03471,"Fundamental experimental measurements of quantities such as ignition delay times, laminar flame speeds, and species profiles (among others) serve important roles in understanding fuel chemistry and validating chemical kinetic models.$$$However, despite both the importance and abundance of such information in the literature, the community lacks a widely adopted standard format for this data.$$$This impedes both sharing and wide use by the community.$$$Here we introduce a new chemical kinetics experimental data format, ChemKED, and the related Python-based package for validating and working with ChemKED-formatted files called PyKED.$$$We also review past and related efforts, and motivate the need for a new solution.$$$ChemKED currently supports the representation of autoignition delay time measurements from shock tubes and rapid compression machines.$$$ChemKED-formatted files contain all of the information needed to simulate experimental data points, including the uncertainty of the data.$$$ChemKED is based on the YAML data serialization language, and is intended as a human- and machine-readable standard for easy creation and automated use.$$$Development of ChemKED and PyKED occurs openly on GitHub under the BSD 3-clause license, and contributions from the community are welcome.$$$Plans for future development include support for experimental data from laminar flame, jet stirred reactor, and speciation measurements.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES BACKGROUND METHODS METHODS METHODS METHODS CONCLUSIONS
D01841,"The numerical size of academic publications that are being published in recent years had grown rapidly.$$$Accessing and searching massive academic publications that are distributed over several locations need large amount of computing resources to increase the system performance.$$$Therefore, many grid-based search techniques were proposed to provide flexible methods for searching the distributed extensive data.$$$This paper proposes search technique that is capable of searching the extensive publications by utilizing grid computing technology.$$$The search technique is implemented as interconnected grid services to offer a mechanism to access different data locations.$$$The experimental result shows that the grid-based search technique has enhanced the performance of the search.",BACKGROUND BACKGROUND OTHERS OBJECTIVES METHODS RESULTS
D00089,"We present a proof procedure for univariate real polynomial problems in Isabelle/HOL.$$$The core mathematics of our procedure is based on univariate cylindrical algebraic decomposition.$$$We follow the approach of untrusted certificates, separating solving from verifying: efficient external tools perform expensive real algebraic computations, producing evidence that is formally checked within Isabelle's logic.$$$This allows us to exploit highly-tuned computer algebra systems like Mathematica to guide our procedure without impacting the correctness of its results.$$$We present experiments demonstrating the efficacy of this approach, in many cases yielding orders of magnitude improvements over previous methods.",OBJECTIVES BACKGROUND METHODS METHODS RESULTS/CONCLUSIONS
D06976,"The paper presents a technique to improve human detection in still images using deep learning.$$$Our novel method, ViS-HuD, computes visual saliency map from the image.$$$Then the input image is multiplied by the map and product is fed to the Convolutional Neural Network (CNN) which detects humans in the image.$$$A visual saliency map is generated using ML-Net and human detection is carried out using DetectNet.$$$ML-Net is pre-trained on SALICON while, DetectNet is pre-trained on ImageNet database for visual saliency detection and image classification respectively.$$$The CNNs of ViS-HuD were trained on two challenging databases - Penn Fudan and TUD-Brussels Benchmark.$$$Experimental results demonstrate that the proposed method achieves state-of-the-art performance on Penn Fudan Dataset with 91.4% human detection accuracy and it achieves average miss-rate of 53% on the TUDBrussels benchmark.",OBJECTIVES BACKGROUND/METHODS OBJECTIVES/METHODS BACKGROUND/METHODS BACKGROUND METHODS RESULTS/CONCLUSIONS
D02183,"Collaborative Filtering (CF) is one of the most commonly used recommendation methods.$$$CF consists in predicting whether, or how much, a user will like (or dislike) an item by leveraging the knowledge of the user's preferences as well as that of other users.$$$In practice, users interact and express their opinion on only a small subset of items, which makes the corresponding user-item rating matrix very sparse.$$$Such data sparsity yields two main problems for recommender systems: (1) the lack of data to effectively model users' preferences, and (2) the lack of data to effectively model item characteristics.$$$However, there are often many other data sources that are available to a recommender system provider, which can describe user interests and item characteristics (e.g., users' social network, tags associated to items, etc.).$$$These valuable data sources may supply useful information to enhance a recommendation system in modeling users' preferences and item characteristics more accurately and thus, hopefully, to make recommenders more precise.$$$For various reasons, these data sources may be managed by clusters of different data centers, thus requiring the development of distributed solutions.$$$In this paper, we propose a new distributed collaborative filtering algorithm, which exploits and combines multiple and diverse data sources to improve recommendation quality.$$$Our experimental evaluation using real datasets shows the effectiveness of our algorithm compared to state-of-the-art recommendation algorithms.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS
D01243,"We investigate the scenario that a robot needs to reach a designated goal after taking a sequence of appropriate actions in a non-static environment that is partially structured.$$$One application example is to control a marine vehicle to move in the ocean.$$$The ocean environment is dynamic and oftentimes the ocean waves result in strong disturbances that can disturb the vehicle's motion.$$$Modeling such dynamic environment is non-trivial, and integrating such model in the robotic motion control is particularly difficult.$$$Fortunately, the ocean currents usually form some local patterns (e.g. vortex) and thus the environment is partially structured.$$$The historically observed data can be used to train the robot to learn to interact with the ocean tidal disturbances.$$$In this paper we propose a method that applies the deep reinforcement learning framework to learn such partially structured complex disturbances.$$$Our results show that, by training the robot under artificial and real ocean disturbances, the robot is able to successfully act in complex and spatiotemporal environments.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS RESULTS/CONCLUSIONS
D06537,"Channel estimation at millimeter wave (mmWave) is challenging when large antenna arrays are used.$$$Prior work has leveraged the sparse nature of mmWave channels via compressed sensing based algorithms for channel estimation.$$$Most of these algorithms, though, assume perfect synchronization and are vulnerable to phase errors that arise due to carrier frequency offset (CFO) and phase noise.$$$Recently sparsity-aware, non-coherent beamforming algorithms that are robust to phase errors were proposed for narrowband phased array systems with full resolution analog-to-digital converters (ADCs).$$$Such energy based algorithms, however, are not robust to heavy quantization at the receiver.$$$In this paper, we develop a joint CFO and wideband channel estimation algorithm that is scalable across different mmWave architectures.$$$Our method exploits the sparsity of mmWave MIMO channel in the angle-delay domain, in addition to compressibility of the phase error vector.$$$We formulate the joint estimation as a sparse bilinear optimization problem and then use message passing for recovery.$$$We also give an efficient implementation of a generalized bilinear message passing algorithm for the joint estimation in mmWave systems with one-bit ADCs.$$$Simulation results show that our method is able to recover the CFO and the channel compressively, even in the presence of phase noise.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS RESULTS/CONCLUSIONS
D02629,"The increasing nature of World Wide Web has imposed great challenges for researchers in improving the search efficiency over the internet.$$$Now days web document clustering has become an important research topic to provide most relevant documents in huge volumes of results returned in response to a simple query.$$$In this paper, first we proposed a novel approach, to precisely define clusters based on maximal frequent item set (MFI) by Apriori algorithm.$$$Afterwards utilizing the same maximal frequent item set (MFI) based similarity measure for Hierarchical document clustering.$$$By considering maximal frequent item sets, the dimensionality of document set is decreased.$$$Secondly, providing privacy preserving of open web documents is to avoiding duplicate documents.$$$There by we can protect the privacy of individual copy rights of documents.$$$This can be achieved using equivalence relation.",BACKGROUND OBJECTIVES METHODS METHODS/RESULTS METHODS/RESULTS METHODS/RESULTS/CONCLUSIONS METHODS/RESULTS/CONCLUSIONS METHODS/RESULTS/CONCLUSIONS
D02950,"Deep learning techniques have recently demonstrated broad success in predicting complex dynamical systems ranging from turbulence to human speech, motivating broader questions about how neural networks encode and represent dynamical rules.$$$We explore this problem in the context of cellular automata (CA), simple dynamical systems that are intrinsically discrete and thus difficult to analyze using standard tools from dynamical systems theory.$$$We show that any CA may readily be represented using a convolutional neural network with a network-in-network architecture.$$$This motivates our development of a general convolutional multilayer perceptron architecture, which we find can learn the dynamical rules for arbitrary CA when given videos of the CA as training data.$$$In the limit of large network widths, we find that training dynamics are strongly stereotyped across replicates, and that common patterns emerge in the structure of networks trained on different CA rulesets.$$$We train ensembles of networks on randomly-sampled CA, and we probe how the trained networks internally represent the CA rules using an information-theoretic technique based on distributions of layer activation patterns.$$$We find that CA with simpler rule tables produce trained networks with hierarchical structure and layer specialization, while more complex CA tend to produce shallower representations---illustrating how the underlying complexity of the CA's rules influences the specificity of these internal representations.$$$Our results suggest how the entropy of a physical process can affect its representation when learned by neural networks.",BACKGROUND/OBJECTIVES METHODS RESULTS METHODS/RESULTS RESULTS METHODS RESULTS CONCLUSIONS
D00829,"A new algorithm to generate all Dyck words is presented, which is used in ranking and unranking Dyck words.$$$We emphasize the importance of using Dyck words in encoding objects related to Catalan numbers.$$$As a consequence of formulas used in the ranking algorithm we can obtain a recursive formula for the nth Catalan number.",OBJECTIVES RESULTS RESULTS
D06818,"Computer algebra systems are a great help for mathematical research but sometimes unexpected errors in the software can also badly affect it.$$$As an example, we show how we have detected an error of Mathematica computing determinants of matrices of integer numbers: not only it computes the determinants wrongly, but also it produces different results if one evaluates the same determinant twice.",BACKGROUND/CONCLUSIONS BACKGROUND/CONCLUSIONS
D01373,"Social network platforms can use the data produced by their users to serve them better.$$$One of the services these platforms provide is recommendation service.$$$Recommendation systems can predict the future preferences of users using their past preferences.$$$In the recommendation systems literature there are various techniques, such as neighborhood based methods, machine-learning based methods and matrix-factorization based methods.$$$In this work, a set of well known methods from natural language processing domain, namely Word2Vec, is applied to recommendation systems domain.$$$Unlike previous works that use Word2Vec for recommendation, this work uses non-textual features, the check-ins, and it recommends venues to visit/check-in to the target users.$$$For the experiments, a Foursquare check-in dataset is used.$$$The results show that use of continuous vector space representations of items modeled by techniques of Word2Vec is promising for making recommendations.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES/METHODS RESULTS RESULTS/CONCLUSIONS
D00199,"Deep spiking neural networks (SNNs) hold great potential for improving the latency and energy efficiency of deep neural networks through event-based computation.$$$However, training such networks is difficult due to the non-differentiable nature of asynchronous spike events.$$$In this paper, we introduce a novel technique, which treats the membrane potentials of spiking neurons as differentiable signals, where discontinuities at spike times are only considered as noise.$$$This enables an error backpropagation mechanism for deep SNNs, which works directly on spike signals and membrane potentials.$$$Thus, compared with previous methods relying on indirect training and conversion, our technique has the potential to capture the statics of spikes more precisely.$$$Our novel framework outperforms all previously reported results for SNNs on the permutation invariant MNIST benchmark, as well as the N-MNIST benchmark recorded with event-based vision sensors.",BACKGROUND OBJECTIVES METHODS METHODS OBJECTIVES RESULTS
D01913,"XML data warehouses form an interesting basis for decision-support applications that exploit heterogeneous data from multiple sources.$$$However, XML-native database systems currently suffer from limited performances in terms of manageable data volume and response time for complex analytical queries.$$$Fragmenting and distributing XML data warehouses (e.g., on data grids) allow to address both these issues.$$$In this paper, we work on XML warehouse fragmentation.$$$In relational data warehouses, several studies recommend the use of derived horizontal fragmentation.$$$Hence, we propose to adapt it to the XML context.$$$We particularly focus on the initial horizontal fragmentation of dimensions' XML documents and exploit two alternative algorithms.$$$We experimentally validate our proposal and compare these alternatives with respect to a unified XML warehouse model we advocate for.",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES BACKGROUND OBJECTIVES METHODS RESULTS
D06160,"Macro-management is an important problem in StarCraft, which has been studied for a long time.$$$Various datasets together with assorted methods have been proposed in the last few years.$$$But these datasets have some defects for boosting the academic and industrial research: 1) There're neither standard preprocessing, parsing and feature extraction procedures nor predefined training, validation and test set in some datasets.$$$2) Some datasets are only specified for certain tasks in macro-management.$$$3) Some datasets are either too small or don't have enough labeled data for modern machine learning algorithms such as deep neural networks.$$$So most previous methods are trained with various features, evaluated on different test sets from the same or different datasets, making it difficult to be compared directly.$$$To boost the research of macro-management in StarCraft, we release a new dataset MSC based on the platform SC2LE.$$$MSC consists of well-designed feature vectors, pre-defined high-level actions and final result of each match.$$$We also split MSC into training, validation and test set for the convenience of evaluation and comparison.$$$Besides the dataset, we propose a baseline model and present initial baseline results for global state evaluation and build order prediction, which are two of the key tasks in macro-management.$$$Various downstream tasks and analyses of the dataset are also described for the sake of research on macro-management in StarCraft II.$$$Homepage: https://github.com/wuhuikai/MSC.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS METHODS OTHERS
D01056,"We introduce Mix&Match (M&M) - a training framework designed to facilitate rapid and effective learning in RL agents, especially those that would be too slow or too challenging to train otherwise.$$$The key innovation is a procedure that allows us to automatically form a curriculum over agents.$$$Through such a curriculum we can progressively train more complex agents by, effectively, bootstrapping from solutions found by simpler agents.$$$In contradistinction to typical curriculum learning approaches, we do not gradually modify the tasks or environments presented, but instead use a process to gradually alter how the policy is represented internally.$$$We show the broad applicability of our method by demonstrating significant performance gains in three different experimental setups: (1) We train an agent able to control more than 700 actions in a challenging 3D first-person task; using our method to progress through an action-space curriculum we achieve both faster training and better final performance than one obtains using traditional methods.$$$(2) We further show that M&M can be used successfully to progress through a curriculum of architectural variants defining an agents internal state.$$$(3) Finally, we illustrate how a variant of our method can be used to improve agent performance in a multitask setting.",OBJECTIVES METHODS METHODS METHODS RESULTS RESULTS RESULTS
D00074,"In iterative supervised learning algorithms it is common to reach a point in the search where no further induction seems to be possible with the available data.$$$If the search is continued beyond this point, the risk of overfitting increases significantly.$$$Following the recent developments in inductive semantic stochastic methods, this paper studies the feasibility of using information gathered from the semantic neighborhood to decide when to stop the search.$$$Two semantic stopping criteria are proposed and experimentally assessed in Geometric Semantic Genetic Programming (GSGP) and in the Semantic Learning Machine (SLM) algorithm (the equivalent algorithm for neural networks).$$$The experiments are performed on real-world high-dimensional regression datasets.$$$The results show that the proposed semantic stopping criteria are able to detect stopping points that result in a competitive generalization for both GSGP and SLM.$$$This approach also yields computationally efficient algorithms as it allows the evolution of neural networks in less than 3 seconds on average, and of GP trees in at most 10 seconds.$$$The usage of the proposed semantic stopping criteria in conjunction with the computation of optimal mutation/learning steps also results in small trees and neural networks.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS
D05742,"Facial aging and facial rejuvenation analyze a given face photograph to predict a future look or estimate a past look of the person.$$$To achieve this, it is critical to preserve human identity and the corresponding aging progression and regression with high accuracy.$$$However, existing methods cannot simultaneously handle these two objectives well.$$$We propose a novel generative adversarial network based approach, named the Conditional Multi-Adversarial AutoEncoder with Ordinal Regression (CMAAE-OR).$$$It utilizes an age estimation technique to control the aging accuracy and takes a high-level feature representation to preserve personalized identity.$$$Specifically, the face is first mapped to a latent vector through a convolutional encoder.$$$The latent vector is then projected onto the face manifold conditional on the age through a deconvolutional generator.$$$The latent vector preserves personalized face features and the age controls facial aging and rejuvenation.$$$A discriminator and an ordinal regression are imposed on the encoder and the generator in tandem, making the generated face images to be more photorealistic while simultaneously exhibiting desirable aging effects.$$$Besides, a high-level feature representation is utilized to preserve personalized identity of the generated face.$$$Experiments on two benchmark datasets demonstrate appealing performance of the proposed method over the state-of-the-art.",BACKGROUND BACKGROUND/OBJECTIVES BACKGROUND METHODS METHODS METHODS METHODS METHODS METHODS METHODS RESULTS
D03478,"Unfortunately, the article ""A Comparative Study to Benchmark Cross-project Defect Prediction Approaches"" has a problem in the statistical analysis which was pointed out almost immediately after the pre-print of the article appeared online.$$$While the problem does not negate the contribution of the the article and all key findings remain the same, it does alter some rankings of approaches used in the study.$$$Within this correction, we will explain the problem, how we resolved it, and present the updated results.",BACKGROUND OBJECTIVES OBJECTIVES
D01196,"Observable operator models (OOMs) and related models are one of the most important and powerful tools for modeling and analyzing stochastic systems.$$$They exactly describe dynamics of finite-rank systems and can be efficiently and consistently estimated through spectral learning under the assumption of identically distributed data.$$$In this paper, we investigate the properties of spectral learning without this assumption due to the requirements of analyzing large-time scale systems, and show that the equilibrium dynamics of a system can be extracted from nonequilibrium observation data by imposing an equilibrium constraint.$$$In addition, we propose a binless extension of spectral learning for continuous data.$$$In comparison with the other continuous-valued spectral algorithms, the binless algorithm can achieve consistent estimation of equilibrium dynamics with only linear complexity.",BACKGROUND BACKGROUND OBJECTIVES/METHODS METHODS/RESULTS CONCLUSIONS
D06952,High-resolution regional hindcasting of ocean and sea ice plays an important role in the assessment of shipping and operational risks in the Arctic Ocean.$$$The ice-ocean model NEMO-LIM3 was modified to improve its simulation quality for appropriate spatio-temporal resolutions.$$$A multigrid model setup with connected coarse- (14 km) and fine-resolution (5 km) model configurations was devised.$$$These two configurations were implemented and run separately.$$$The resulting computational cost was lower when compared to that of the built-in AGRIF nesting system.$$$Ice and tracer boundary-condition schemes were modified to achieve the correct interaction between coarse- and fine grids through a long ice-covered open boundary.$$$An ice-restoring scheme was implemented to reduce spin-up time.$$$The NEMO-LIM3 configuration described in this article provides more flexible and customisable tools for high-resolution regional Arctic simulations.,BACKGROUND RESULTS METHODS METHODS RESULTS METHODS METHODS CONCLUSIONS
D05061,"The objective of Content-Based Image Retrieval (CBIR) methods is essentially to extract, from large (image) databases, a specified number of images similar in visual and semantic content to a so-called query image.$$$To bridge the semantic gap that exists between the representation of an image by low-level features (namely, colour, shape, texture) and its high-level semantic content as perceived by humans, CBIR systems typically make use of the relevance feedback (RF) mechanism.$$$RF iteratively incorporates user-given inputs regarding the relevance of retrieved images, to improve retrieval efficiency.$$$One approach is to vary the weights of the features dynamically via feature reweighting.$$$In this work, an attempt has been made to improve retrieval accuracy by enhancing a CBIR system based on color features alone, through implicit incorporation of shape information obtained through prior segmentation of the images.$$$Novel schemes for feature reweighting as well as for initialization of the relevant set for improved relevance feedback, have also been proposed for boosting performance of RF- based CBIR.$$$At the same time, new measures for evaluation of retrieval accuracy have been suggested, to overcome the limitations of existing measures in the RF context.$$$Results of extensive experiments have been presented to illustrate the effectiveness of the proposed approaches.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES METHODS METHODS RESULTS
D06872,"A number of methods have been proposed over the last decade for encoding information using deoxyribonucleic acid (DNA), giving rise to the emerging area of DNA data embedding.$$$Since a DNA sequence is conceptually equivalent to a sequence of quaternary symbols (bases), DNA data embedding (diversely called DNA watermarking or DNA steganography) can be seen as a digital communications problem where channel errors are tantamount to mutations of DNA bases.$$$Depending on the use of coding or noncoding DNA hosts, which, respectively, denote DNA segments that can or cannot be translated into proteins, DNA data embedding is essentially a problem of communications with or without side information at the encoder.$$$In this paper the Shannon capacity of DNA data embedding is obtained for the case in which DNA sequences are subject to substitution mutations modelled using the Kimura model from molecular evolution studies.$$$Inferences are also drawn with respect to the biological implications of some of the results presented.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/METHODS/RESULTS RESULTS
D02834,"Many popular form factors of digital assistant---such as Amazon Echo, Apple Homepod or Google Home---enable the user to hold a conversation with the assistant based only on the speech modality.$$$The lack of a screen from which the user can read text or watch supporting images or video presents unique challenges.$$$In order to satisfy the information need of a user, we believe that the presentation of the answer needs to be optimized for such voice-only interactions.$$$In this paper we propose a task of evaluating usefulness of prosody modifications for the purpose of voice-only question answering.$$$We describe a crowd-sourcing setup where we evaluate the quality of these modifications along multiple dimensions corresponding to the informativeness, naturalness, and ability of the user to identify the key part of the answer.$$$In addition, we propose a set of simple prosodic modifications that highlight important parts of the answer using various acoustic cues.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS
D03040,"Unlike many complex networks studied in the literature, social networks rarely exhibit unanimous behavior, or consensus.$$$This requires a development of mathematical models that are sufficiently simple to be examined and capture, at the same time, the complex behavior of real social groups, where opinions and actions related to them may form clusters of different size.$$$One such model, proposed by Friedkin and Johnsen, extends the idea of conventional consensus algorithm (also referred to as the iterative opinion pooling) to take into account the actors' prejudices, caused by some exogenous factors and leading to disagreement in the final opinions.$$$In this paper, we offer a novel multidimensional extension, describing the evolution of the agents' opinions on several topics.$$$Unlike the existing models, these topics are interdependent, and hence the opinions being formed on these topics are also mutually dependent.$$$We rigorous examine stability properties of the proposed model, in particular, convergence of the agents' opinions.$$$Although our model assumes synchronous communication among the agents, we show that the same final opinions may be reached ""on average"" via asynchronous gossip-based protocols.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES OBJECTIVES/METHODS RESULTS/CONCLUSIONS
D01047,"A deep learning network was used to predict future blood glucose levels, as this can permit diabetes patients to take action before imminent hyperglycaemia and hypoglycaemia.$$$A sequential model with one long-short-term memory (LSTM) layer, one bidirectional LSTM layer and several fully connected layers was used to predict blood glucose levels for different prediction horizons.$$$The method was trained and tested on 26 datasets from 20 real patients.$$$The proposed network outperforms the baseline methods in terms of all evaluation criteria.",METHODS METHODS METHODS RESULTS
